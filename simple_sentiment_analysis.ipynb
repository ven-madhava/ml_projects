{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Simple sentiment analyser\n",
    "-------------------------\n",
    "\n",
    "1. We will standard RNN cells - like GRU/LSTMs\n",
    "2. Using gensim word2vec for word embeddings\n",
    "3. Using a simple downloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkateshmadhava/Documents/pmate2/pmate2_env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# -------\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import csv\n",
    "import cv2\n",
    "import h5py\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from scipy.ndimage import label\n",
    "from skimage import measure\n",
    "\n",
    "# torch related imports\n",
    "# ---------------------\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# RNN related\n",
    "# -----------\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# local settings\n",
    "# --------------\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity function to find similarity distance between two words\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        formula -- cos thetha = dot_product_of(u,v)/l2_norm(u)*l2_norm(v)\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Compute the dot product between u and v (â‰ˆ1 line)\n",
    "    # -------------------------------------------------\n",
    "    dot = np.dot(u,v)\n",
    "    norm_u = np.sqrt(np.sum(np.square(u)))\n",
    "    norm_v = np.sqrt(np.sum(np.square(v)))\n",
    "    cosine_similarity = dot/(norm_u*norm_v)\n",
    "    \n",
    "    # final return\n",
    "    # -----------\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare user input dataset\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "def prepare_use_input(user_inlist, num_layers, num_directions, hidden_state_size):\n",
    "    \n",
    "    # 1. prepare user input\n",
    "    # ---------------------\n",
    "    print('1. preparing user input..')\n",
    "    userin_dict = {}\n",
    "    for i in range(len(user_inlist)):\n",
    "        temp_list = []\n",
    "        userin_dict[i] = {}\n",
    "        for each in user_inlist[i].split(' '):\n",
    "            temp_list.append(''.join(ch for ch in each.lower() if ch.isalnum()))\n",
    "        userin_dict[i]['words'] = temp_list\n",
    "        userin_dict[i]['label'] = -1\n",
    "        \n",
    "        \n",
    "    # creating input dataset\n",
    "    # ----------------------\n",
    "    print('2. Setting up datasets..')\n",
    "    xin_user,yin_user = create_rnn_input_from_indict(userin_dict)\n",
    "    print('##')\n",
    "    print(xin_user.size())\n",
    "    print(xin_user.type())\n",
    "    print(yin_user.size())\n",
    "    print(yin_user.type())\n",
    "    h_0_user = torch.zeros((num_layers*num_directions, xin_user.size()[1], hidden_state_size))\n",
    "    print(h_0_user.size())\n",
    "    \n",
    "    return xin_user, h_0_user\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a dataset from input text\n",
    "# -------------------------------------------\n",
    "\n",
    "def build_sentence_in_dataset(file_url):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1. takes in txt file with sentence_/t_label_/n format\n",
    "    2. outputs dict of format dict[index]['words'] = list of words in sentence, dict[index]['label'] = target\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    x_dict = {}\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    not_read = []\n",
    "\n",
    "    # 1. file open ops\n",
    "    # ----------------\n",
    "    file_in = open(file_url,'r')\n",
    "    file_in_list = file_in.read().split('\\n')\n",
    "\n",
    "    # 2. list processing into dict ops\n",
    "    # --------------------------------\n",
    "    for i in range(len(file_in_list)):\n",
    "\n",
    "        # flag update\n",
    "        # -----------\n",
    "        total += 1\n",
    "\n",
    "        # main statements\n",
    "        # ---------------\n",
    "        try:\n",
    "            curr_data = file_in_list[i]\n",
    "            curr_label = int(curr_data.split('\\t')[1])\n",
    "            curr_sentence = curr_data.split('\\t')[0].lower()\n",
    "            curr_sentence_words_alnum = []\n",
    "            \n",
    "            # keeping only alpha numeric chars\n",
    "            # --------------------------------\n",
    "            for each in curr_sentence.split(' '):\n",
    "                curr_sentence_words_alnum.append(''.join(ch for ch in each if ch.isalnum()))\n",
    "                \n",
    "\n",
    "            # saving to dict\n",
    "            # --------------\n",
    "            x_dict[i] = {}\n",
    "            x_dict[i]['words'] = curr_sentence_words_alnum\n",
    "            x_dict[i]['label'] = curr_label\n",
    "            counter += 1\n",
    "        except:\n",
    "            not_read.append(curr_data)\n",
    "\n",
    "\n",
    "    # 2.1 info print\n",
    "    # --------------\n",
    "    print(str(counter) + ' lines processed of a total of ' + str(total) )\n",
    "    \n",
    "    \n",
    "    # final return\n",
    "    # ------------\n",
    "    return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that builds a RNN input using input dict\n",
    "# --------------------------------------------------\n",
    "\n",
    "def create_rnn_input_from_indict(indict):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1. Takes input dict of format dict[index]['words'] = list of words in sentence, dict[index]['label'] = target\n",
    "    2. outputs RNN input of format (seq_len, batch, input_size)\n",
    "    3. That is (no_words_per_sentence, no_example_sentences, vector_len_of_each_word)\n",
    "    4. outputs target label (no_example_sentences) -- NOT ONE HOT\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    feature_vec_size = word2vec_dict['hi'].shape[0]\n",
    "    xout_list = []\n",
    "    target = np.zeros((0))\n",
    "    \n",
    "    \n",
    "    # 1. looping through dict\n",
    "    # -----------------------\n",
    "    print('1. Itering through in dict..')\n",
    "    for keys in indict:\n",
    "        \n",
    "        # 1.0 initialising temp seq for this current sentence\n",
    "        # ---------------------------------------------------\n",
    "        temp_seq = np.zeros((0,feature_vec_size))\n",
    "        \n",
    "        # looping through words\n",
    "        # ---------------------\n",
    "        for each in indict[keys]['words']:\n",
    "\n",
    "            # 1.1 checking if word2vec has the word\n",
    "            # -------------------------------------\n",
    "            try:\n",
    "                curr_wordvec = word2vec_dict[each].reshape(1,feature_vec_size)\n",
    "            except:\n",
    "                curr_wordvec = np.zeros((1,feature_vec_size))\n",
    "\n",
    "            # 1.2 building sequence\n",
    "            # ---------------------\n",
    "            temp_seq = np.concatenate((temp_seq,curr_wordvec), axis = 0)\n",
    "            \n",
    "        \n",
    "        # concatenating to output\n",
    "        # -----------------------\n",
    "        xout_list.append(torch.from_numpy(temp_seq))\n",
    "        target = np.concatenate((target,np.array([indict[keys]['label']])))\n",
    "    \n",
    "    \n",
    "    # 2. final padding before return\n",
    "    # ------------------------------\n",
    "    print('2. Padding list..')\n",
    "    xout = pad_sequence(xout_list)\n",
    "    target = torch.from_numpy(target)\n",
    "    target = target.view(-1,1)\n",
    "    \n",
    "    # final return\n",
    "    # ------------\n",
    "    return xout.float(), target.float()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 NN related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERIC - initialises weights for a NN\n",
    "# --------------------------------------\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "    #    print(classname)\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    #elif classname.find('Linear') != -1:\n",
    "    #    print(classname)\n",
    "    #    m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "        \n",
    "# GENERIC class that inherits nn module and makes a sequential object a model\n",
    "# ---------------------------------------------------------------------------\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,sequencelist):\n",
    "        super().__init__() # Initializing nn.Module construtors\n",
    "        self.forwardpass = sequencelist\n",
    "        \n",
    "    def forward(self,x):\n",
    "        xout = self.forwardpass(x)\n",
    "        return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a lineaf FC model\n",
    "# -----------------------------------\n",
    "\n",
    "def linear_fc(layers, nw_activations, target_activation, dropout_p):\n",
    "    \n",
    "    'The first value in the layers list is the input dimensions of the input'\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    seq_list = []\n",
    "    \n",
    "    # setting N/W activations\n",
    "    # -------------------\n",
    "    if nw_activations == 'relu':\n",
    "        nw_act = nn.ReLU()\n",
    "    elif nw_activations == 'lrelu':\n",
    "        nw_act = nn.LeakyReLU(0.2)\n",
    "    elif nw_activations == 'sigmoid':\n",
    "        nw_act = nn.Sigmoid()\n",
    "    elif nw_activations == 'tanh':\n",
    "        nw_act = nn.Tanh()\n",
    "    else:\n",
    "        nw_act = nn.ReLU()\n",
    "    \n",
    "    # setting target activations\n",
    "    # --------------------------\n",
    "    if target_activation == 'sigmoid':\n",
    "        target_act = nn.Sigmoid()\n",
    "    elif target_activation == 'softmax':\n",
    "        target_act = nn.Softmax()\n",
    "    else:\n",
    "        target_act = None\n",
    "    \n",
    "    # 1. building n/w's layer list\n",
    "    # ----------------------------\n",
    "    network = []\n",
    "    for k in range(len(layers)):\n",
    "        try:\n",
    "            network.append((layers[k],layers[k+1]))\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "    \n",
    "    # 2. constructing encoder n/w\n",
    "    # ----------------------------\n",
    "    for i in range(len(network)):\n",
    "        \n",
    "        # 2.1 adding linear layers to encoder\n",
    "        # ------------------------------------\n",
    "        curr_dims = network[i]\n",
    "        seq_mod = nn.Linear(curr_dims[0],curr_dims[1])\n",
    "        seq_list.append(seq_mod)\n",
    "        \n",
    "        # checking last layer or not\n",
    "        # --------------------------\n",
    "        if i+1 == len(network):\n",
    "            \n",
    "            # at last layer\n",
    "            # -------------\n",
    "            if target_act == None:\n",
    "                pass\n",
    "            else:\n",
    "                seq_list.append(target_act)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # batchnorm\n",
    "            # ---------\n",
    "            seq_list.append(nn.BatchNorm1d(curr_dims[1]))\n",
    "          \n",
    "            # non linear activation\n",
    "            # ---------------------\n",
    "            seq_list.append(nw_act)\n",
    "            \n",
    "            # dropout\n",
    "            # -------\n",
    "            seq_list.append(nn.Dropout(p = dropout_p))\n",
    "           \n",
    "            \n",
    "    \n",
    "    # 3. returning model\n",
    "    # ------------------\n",
    "    seq_list = nn.Sequential(*seq_list)\n",
    "    seq_list.apply(weights_init)\n",
    "\n",
    "    model = Net(seq_list)\n",
    "    model = model.train()\n",
    "    \n",
    "    return model\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to load a saved model\n",
    "# --------------------------------\n",
    "\n",
    "def load_saved_model_function(in_path, mode, use_cuda):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    path = /folder1/folder2/model_sentiment.tar format \n",
    "    \n",
    "    There will be two or more models associated with this depending on the task -\n",
    "    model_sentiment_rnn, model_sentiment_dense\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 0. Initialisation\n",
    "    # ------------------\n",
    "    models_out = {}\n",
    "    \n",
    "    if mode == 'rnn_dense':\n",
    "    \n",
    "        # 1. loading RNN model\n",
    "        # --------------------\n",
    "        rnn_path = in_path.replace('.tar','_RNN.tar')\n",
    "        model = torch.load(in_path.replace('.tar','_RNN_MODEL.tar'))\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()))\n",
    "\n",
    "        # Applying state dict\n",
    "        # ----------------------\n",
    "        if use_cuda == True:\n",
    "            \n",
    "            # loads to GPU\n",
    "            # ------------\n",
    "            checkpoint = torch.load(rnn_path)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # loads to CPU\n",
    "            # ------------\n",
    "            checkpoint = torch.load(rnn_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "        # loading sate dict values\n",
    "        # ---------------------------\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        \n",
    "        # building list\n",
    "        # -------------\n",
    "        models_out['rnn'] = [model, optimizer, epoch]\n",
    "        \n",
    "        \n",
    "        # 1. loading Dense model\n",
    "        # --------------------\n",
    "        dense_path = in_path.replace('.tar','_DENSE.tar')\n",
    "        model = torch.load(in_path.replace('.tar','_DENSE_MODEL.tar'))\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()))\n",
    "\n",
    "        # Applying state dict\n",
    "        # ----------------------\n",
    "        if use_cuda == True:\n",
    "            # loads to GPU\n",
    "            # ------------\n",
    "            checkpoint = torch.load(dense_path)\n",
    "        else:\n",
    "            # loads to CPU\n",
    "            # ------------\n",
    "            checkpoint = torch.load(dense_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "        # loading sate dict values\n",
    "        # ---------------------------\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        loss_mode = checkpoint['loss_mode']\n",
    "        \n",
    "        # building list\n",
    "        # -------------\n",
    "        models_out['dense'] = [model, optimizer, epoch, loss, loss_mode]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        assert 1 == 2, 'Other modes not built in yet.'\n",
    "    \n",
    "    \n",
    "    # final return\n",
    "    # ------------\n",
    "    return models_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERIC model function to train the networks\n",
    "# --------------------------------------------\n",
    "\n",
    "def model_rnn_train(xin,hin,yin,xval,hval,yval,load_mode,model_mode,model_dict,epochs,mbsize,loss_mode,use_cuda,save_state,path):\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    models_return_out = {}\n",
    "    \n",
    "\n",
    "    # 1. loading old model if required\n",
    "    # --------------------------------\n",
    "    if load_mode == 'from saved':\n",
    "        \n",
    "        # 0. loading saved models\n",
    "        # -----------------------\n",
    "        model_dict = load_saved_model_function(path,model_mode,use_cuda)\n",
    "        \n",
    "        # checking model modes and loading them\n",
    "        # --------------------------------------\n",
    "        if model_mode == 'rnn_dense':\n",
    "            \n",
    "\n",
    "            # 1. loading RNN from saved\n",
    "            # -------------------------\n",
    "            model_rnn = model_dict['rnn'][0]\n",
    "            optimizer_rnn = model_dict['rnn'][1]\n",
    "            saved_epoch_rnn = model_dict['rnn'][2]\n",
    "            \n",
    "            model_rnn = model_rnn.train()\n",
    "            print('Loading RNN model from saved state...')\n",
    "            print('Last saved epoch - ' + str(saved_epoch_rnn))\n",
    "           \n",
    "            \n",
    "            # 2. loading dense net from saved\n",
    "            # -------------------------------\n",
    "            model_dense = model_dict['dense'][0]\n",
    "            optimizer_dense = model_dict['dense'][1]\n",
    "            saved_epoch_dense = model_dict['dense'][2]\n",
    "            saved_loss_dense = model_dict['dense'][3]\n",
    "            saved_loss_mode_dense = model_dict['dense'][4]\n",
    "            \n",
    "            model_dense = model_dense.train()\n",
    "            dense_loss_mode = saved_loss_mode_dense\n",
    "            print('Loading Dense model from saved state...')\n",
    "            print('Last saved epoch - ' + str(saved_epoch_dense))\n",
    "            print('Last saved loss - ' + str(saved_loss_dense))\n",
    "            \n",
    "            \n",
    "            # 3. sanity assertion\n",
    "            # -------------------\n",
    "            assert int(saved_epoch_rnn) == int(saved_epoch_dense), 'FATAL ERROR: Rnn and densenet have different training epochs.'\n",
    "            epochs += int(saved_epoch_rnn)\n",
    "            start_epoch = int(saved_epoch_rnn)\n",
    "            \n",
    "            models_return_out['rnn'] = model_rnn\n",
    "            models_return_out['dense'] = model_dense\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            assert 1==2, 'Other modes not coded yet.'\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # building new models\n",
    "        # -------------------\n",
    "        start_epoch = 1\n",
    "        \n",
    "        # checkimg model mode\n",
    "        # -------------------\n",
    "        if model_mode == 'rnn_dense':\n",
    "            \n",
    "            # extracting models\n",
    "            # -----------------\n",
    "            model_rnn = model_dict['rnn']\n",
    "            model_rnn = model_rnn.train()\n",
    "            optimizer_rnn = torch.optim.Adam(filter(lambda p: p.requires_grad,model_rnn.parameters()))\n",
    "        \n",
    "            model_dense = model_dict['dense']\n",
    "            model_dense = model_dense.train()\n",
    "            optimizer_dense = torch.optim.Adam(filter(lambda p: p.requires_grad,model_dense.parameters()))\n",
    "            \n",
    "            models_return_out['rnn'] = model_rnn\n",
    "            models_return_out['dense'] = model_dense\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            assert 1==2, 'Other modes not coded yet.'\n",
    "            \n",
    "        \n",
    "    \n",
    "    # 2. setting loss criterion\n",
    "    # -------------------------\n",
    "    if loss_mode == 'MSE':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_mode == 'BCE':\n",
    "        criterion = nn.BCELoss()\n",
    "    elif loss_mode == 'NLL':\n",
    "        criterion = nn.NLLLoss()\n",
    "    elif loss_mode == 'crossentropy':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        yin = torch.max(yin.long(),1)[1]\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "    \n",
    "    # 3. Setting up minibatch features\n",
    "    # --------------------------------\n",
    "    m = xin.size()[1]\n",
    "    mb_list = []\n",
    "    mb_list = list(range(int(m/mbsize)))\n",
    "    if m % mbsize == 0: # if the minibatches can be split up perfectly.\n",
    "        'do nothing'\n",
    "    else:\n",
    "        mb_list.append(mb_list[len(mb_list)-1] + 1)\n",
    "    \n",
    "    \n",
    "    # 4. ACTUAL TRAINING ITERATIONS\n",
    "    # -----------------------------\n",
    "    if model_mode == 'rnn_dense':\n",
    "    \n",
    "        # 4.1 training iters\n",
    "        # ------------------\n",
    "        for i in range(start_epoch,epochs+1):\n",
    "\n",
    "            for p in mb_list:\n",
    "\n",
    "                # Mini batch operations\n",
    "                # ---------------------\n",
    "                start_index = p*mbsize\n",
    "                end_index = m if p == mb_list[len(mb_list)-1] else p*mbsize + mbsize\n",
    "                m_curr = end_index - start_index\n",
    "\n",
    "                Xin_mb = xin[:,start_index:end_index,:]\n",
    "                Hin_mb = hin[:,start_index:end_index,:]\n",
    "                Yin_mb = yin[start_index:end_index]\n",
    "\n",
    "                if use_cuda == True:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    model_rnn = model_rnn.cuda()\n",
    "                    model_dense = model_dense.cuda()\n",
    "                    Xin_mb = Xin_mb.cuda()\n",
    "                    Yin_mb = Yin_mb.cuda()\n",
    "                    Hin_mb = Hin_mb.cuda()\n",
    "\n",
    "                \n",
    "                # RNN network ops\n",
    "                # ---------------\n",
    "                _,model_rnn_out = model_rnn(Xin_mb,Hin_mb)\n",
    "                #print(model_rnn_out.size())\n",
    "                model_rnn_out = model_rnn_out.permute(1,0,2)\n",
    "                model_rnn_out = model_rnn_out.view(model_rnn_out.size()[0],-1)\n",
    "                #print(model_rnn_out.size())\n",
    "              \n",
    "                \n",
    "                # Dense NN network ops\n",
    "                # --------------------\n",
    "                model_out = model_dense(model_rnn_out)\n",
    "                #print(model_out.size())\n",
    "                #print(Yin_mb.size())\n",
    "                \n",
    "                # backward ops\n",
    "                # ------------\n",
    "                optimizer_rnn.zero_grad()\n",
    "                optimizer_dense.zero_grad()\n",
    "                loss = criterion(model_out.float(), Yin_mb.float()) # loss(output, target)\n",
    "                loss.backward()\n",
    "                optimizer_rnn.step()\n",
    "                optimizer_dense.step()\n",
    "                loss_train.append(loss.item())\n",
    "\n",
    "                # deleting curr variables\n",
    "                # -----------------------\n",
    "                if use_cuda == True:\n",
    "                    Xin_mb = Xin_mb.cpu()\n",
    "                    Yin_mb = Yin_mb.cpu()\n",
    "                    Hin_mb = Hin_mb.cpu()\n",
    "                    model_out = model_out.cpu()\n",
    "\n",
    "                    del Xin_mb\n",
    "                    del Yin_mb\n",
    "                    del Hin_mb\n",
    "                    del model_out\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                \n",
    "                # printing loss\n",
    "                # -------------\n",
    "                print('Epoch ' + str(i) + ', minibatch ' + str(p+1) + ' of '  +  str(len(mb_list)) + ' -- Model loss: ' + str(loss.item()))\n",
    "\n",
    "        \n",
    "        # 4.1.1 preparing function out dict\n",
    "        # ---------------------------------\n",
    "        models_return_out['rnn'] = model_rnn\n",
    "        models_return_out['dense'] = model_dense\n",
    "        \n",
    "        \n",
    "        # 4.2 outside training loop saving model states\n",
    "        # --------------------------------------------\n",
    "        if save_state == True and epochs+1 > start_epoch:\n",
    "\n",
    "            # 4.2.1 initialising rnn save dict\n",
    "            # -------------------------------\n",
    "            save_dict = {}\n",
    "            save_dict['epoch'] = str(i)\n",
    "            save_dict['model_state_dict'] = model_rnn.cpu().state_dict()\n",
    "            save_dict['optimizer_state_dict'] = optimizer_rnn.state_dict()\n",
    "            \n",
    "            # 4.2.2 saving\n",
    "            # ----------\n",
    "            rnn_path = path.replace('.tar','_RNN.tar')\n",
    "            torch.save(save_dict,rnn_path)\n",
    "\n",
    "            # saving full model to initialise a new model later on\n",
    "            # ----------------------------------------------------\n",
    "            torch.save(model_rnn.cpu(),rnn_path.replace('.tar','_MODEL.tar'))\n",
    "            print('Saved RNN net.')\n",
    "            \n",
    "            \n",
    "            # 4.2.3 saving dense NN state\n",
    "            # ---------------------------\n",
    "            save_dict = {}\n",
    "            save_dict['epoch'] = str(i)\n",
    "            save_dict['model_state_dict'] = model_dense.cpu().state_dict()\n",
    "            save_dict['optimizer_state_dict'] = optimizer_dense.state_dict()\n",
    "            save_dict['loss'] = str(loss.item())\n",
    "            save_dict['loss_mode'] = loss_mode\n",
    "            \n",
    "             \n",
    "            # 4.2.4 saving\n",
    "            # ----------\n",
    "            dense_path = path.replace('.tar','_DENSE.tar')\n",
    "            torch.save(save_dict,dense_path)\n",
    "\n",
    "            # saving full model to initialise a new model later on\n",
    "            # ----------------------------------------------------\n",
    "            torch.save(model_dense.cpu(),dense_path.replace('.tar','_MODEL.tar'))\n",
    "            \n",
    "            print('Saved dense net.')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        assert 1==2, 'Other modes not coded yet.'\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    # 4. return model in order to use elsewhere in the code\n",
    "    # -----------------------------------------------------\n",
    "    return models_return_out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference code\n",
    "# --------------\n",
    "\n",
    "def inference(xin,hin,mode,model_dict,use_cuda):\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    # NONE\n",
    "    \n",
    "    # 1. check mode and forward pass ops\n",
    "    # ----------------------------------\n",
    "    if mode == 'rnn_dense':\n",
    "        \n",
    "        # 1.0 setting up n/w\n",
    "        # ------------------\n",
    "        model_rnn = model_dict['rnn']\n",
    "        model_dense = model_dict['dense']\n",
    "        \n",
    "        model_rnn = model_rnn.eval()\n",
    "        model_dense = model_dense.eval()\n",
    "        \n",
    "        \n",
    "\n",
    "        # 1.1 Setting up minibatch features\n",
    "        # --------------------------------\n",
    "        m = xin.size()[1]\n",
    "        mbsize = 3\n",
    "        mb_list = []\n",
    "        mb_list = list(range(int(m/mbsize)))\n",
    "        if m % mbsize == 0: # if the minibatches can be split up perfectly.\n",
    "            'do nothing'\n",
    "        else:\n",
    "            mb_list.append(mb_list[len(mb_list)-1] + 1)\n",
    "            \n",
    "            \n",
    "        # 1.2 Mini batch ops\n",
    "        # ------------------\n",
    "        with tqdm(total=m) as pbar:\n",
    "            for p in mb_list:\n",
    "\n",
    "                # Mini batch operations\n",
    "                # ---------------------\n",
    "                start_index = p*mbsize\n",
    "                end_index = m if p == mb_list[len(mb_list)-1] else p*mbsize + mbsize\n",
    "                m_curr = end_index - start_index\n",
    "\n",
    "                Xin_mb = xin[:,start_index:end_index,:]\n",
    "                Hin_mb = hin[:,start_index:end_index,:]\n",
    "                Yin_mb = yin[start_index:end_index]\n",
    "\n",
    "                if use_cuda == True:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    model_rnn = model_rnn.cuda()\n",
    "                    model_dense = model_dense.cuda()\n",
    "                    Xin_mb = Xin_mb.cuda()\n",
    "                    Yin_mb = Yin_mb.cuda()\n",
    "                    Hin_mb = Hin_mb.cuda()\n",
    "\n",
    "\n",
    "                # RNN network ops\n",
    "                # ---------------\n",
    "                _,model_rnn_out = model_rnn(Xin_mb,Hin_mb)\n",
    "                model_rnn_out = model_rnn_out.permute(1,0,2)\n",
    "                model_rnn_out = model_rnn_out.view(model_rnn_out.size()[0],-1)\n",
    "\n",
    "                # Dense NN network ops\n",
    "                # --------------------\n",
    "                model_out = model_dense(model_rnn_out)\n",
    "\n",
    "                # concat opts\n",
    "                # -----------\n",
    "                try:\n",
    "                    final_out = torch.cat((final_out,model_out), 0)\n",
    "                except:\n",
    "                    final_out = model_out\n",
    "            \n",
    "                # pbar update\n",
    "                # -----------\n",
    "                pbar.update(mbsize)\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert 1==2, 'Other modes not coded yet.'\n",
    "        \n",
    "    \n",
    "    # 2. final return\n",
    "    # ---------------\n",
    "    return final_out.data.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 NN classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_NW(nn.Module):\n",
    "    \n",
    "    def __init__(self, mode, input_size, hidden_size, num_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 0. initialisationing n/w\n",
    "        # ------------------------\n",
    "        if mode == 'gru':\n",
    "            self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size*num_layers, num_layers=num_layers, bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size*num_layers, num_layers=num_layers, bidirectional=bidirectional)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        Outputs: output, h_n\n",
    "        --------------------\n",
    "        output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\n",
    "        Similarly, the directions can be separated in the packed case.\n",
    "        \n",
    "        output - the hidden state values for every word of every sentence example -- for each t\n",
    "        use this for word wise predictions\n",
    "        example - torch.Size([32, 1000, 128])\n",
    "\n",
    "        h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
    "        Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).\n",
    "        \n",
    "        hn - the hidden state values for every sentence example at last words onlye -- for time step t = seq_len\n",
    "        use this for sentence wise prediction\n",
    "        example - torch.Size([1, 1000, 128])\n",
    "        \n",
    "        '''\n",
    "\n",
    "        output, hidden = self.rnn.double()(input.double(), hidden.double())\n",
    "        return output.float(), hidden.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Running of codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 setting up env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/venkateshmadhava/Documents/pmate2/pmate2_env/models/\n"
     ]
    }
   ],
   "source": [
    "## SET UP CUDA OR NOT HERE + OTHER SET UPS\n",
    "##########################################\n",
    "\n",
    "dev_env = 'local' # 'gpu' or 'local'\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "\n",
    "# Setting CUDA\n",
    "# ------------\n",
    "if dev_env == 'gpu':\n",
    "    use_cuda = True\n",
    "else:\n",
    "    use_cuda = False\n",
    "if use_cuda == True:\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "\n",
    "# SET FILE SPECIFIC NAMES HERE\n",
    "# ----------------------------\n",
    "if dev_env == 'gpu':\n",
    "    save_path = '/home/venkateshmadhava/codes/pmate2_localgpuenv/models/'\n",
    "    parent_url = '/home/venkateshmadhava/datasets/images'\n",
    "else:\n",
    "    save_path = '/Users/venkateshmadhava/Documents/pmate2/pmate2_env/models/'\n",
    "    parent_url = '/Users/venkateshmadhava/Documents/projects/sequence/sentiment_labelled_sentences'\n",
    "\n",
    "\n",
    "# displaying model path\n",
    "# --------------------\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading word2vec model\n",
    "# ----------------------\n",
    "\n",
    "#word2vec_dict = gensim.models.KeyedVectors.load_word2vec_format(save_path + 'GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the vocab is 3000000\n",
      "The dimension of each vector is 300\n"
     ]
    }
   ],
   "source": [
    "# Showing some stats of the model\n",
    "# -------------------------------\n",
    "\n",
    "print('Total words in the vocab is ' + str(len(word2vec_dict.vocab)))\n",
    "print('The dimension of each vector is ' + str(word2vec_dict['father'].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between jupiter and earth is: 0.31955674\n",
      "Similarity between king and queen is: 0.65109557\n",
      "Similarity between inexpensive and cheap is: 0.70098954\n",
      "Similarity between mice and rat is: 0.49135464\n"
     ]
    }
   ],
   "source": [
    "# checking similarities\n",
    "# ---------------------\n",
    "\n",
    "word_pairs = [('jupiter','earth'),('king','queen'),('inexpensive','cheap'),('mice','rat')]\n",
    "\n",
    "for each in word_pairs:\n",
    "    print('Similarity between ' + each[0] + ' and ' + each[1] + ' is: ' + str(cosine_similarity(word2vec_dict[each[0]],word2vec_dict[each[1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 main calls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "setting up input with sentence of variable length\n",
    "-------------------------------------------------\n",
    "\n",
    "1. add word encodings to words in dict\n",
    "2. create a padded sequence\n",
    "\n",
    "Exmaple from docs\n",
    "-----------------\n",
    ">>> from torch.nn.utils.rnn import pad_sequence\n",
    ">>> a = torch.ones(25, 300)\n",
    ">>> b = torch.ones(22, 300)\n",
    ">>> c = torch.ones(15, 300)\n",
    ">>> pad_sequence([a, b, c]).size()\n",
    "\n",
    "torch.Size([25, 3, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 lines processed of a total of 1001\n"
     ]
    }
   ],
   "source": [
    "# importing dataset\n",
    "# -----------------\n",
    "\n",
    "fileurl = parent_url + '/yelp_labelled.txt'\n",
    "xdict = build_sentence_in_dataset(fileurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'words': ['what', 'should', 'have', 'been', 'a', 'hilarious', 'yummy', 'christmas', 'eve', 'dinner', 'to', 'remember', 'was', 'the', 'biggest', 'fail', 'of', 'the', 'entire', 'trip', 'for', 'us']}\n",
      "{'label': 1, 'words': ['the', 'black', 'eyed', 'peas', 'and', 'sweet', 'potatoes', 'unreal']}\n",
      "{'label': 1, 'words': ['cant', 'say', 'enough', 'good', 'things', 'about', 'this', 'place']}\n"
     ]
    }
   ],
   "source": [
    "# sanity viewing of data\n",
    "# ----------------------\n",
    "\n",
    "print(xdict[919])\n",
    "print(xdict[325])\n",
    "print(xdict[502])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Itering through in dict..\n",
      "2. Padding list..\n",
      "##\n",
      "torch.Size([32, 1000, 300])\n",
      "torch.FloatTensor\n",
      "torch.Size([1000, 1])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# creating input dataset\n",
    "# ----------------------\n",
    "\n",
    "xin,yin = create_rnn_input_from_indict(xdict)\n",
    "print('##')\n",
    "print(xin.size())\n",
    "print(xin.type())\n",
    "print(yin.size())\n",
    "print(yin.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 training model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Outputs: output, h_n\n",
    "--------------------\n",
    "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\n",
    "Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
    "Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up RNN hyper params\n",
    "# ---------------------------\n",
    "\n",
    "seq_vec_size = word2vec_dict['hi'].shape[0]\n",
    "hidden_state_size = 32\n",
    "num_layers = 1\n",
    "bidirectional = False\n",
    "rnn_mode = 'gru'\n",
    "\n",
    "# initialising h_0 with shape (num_layers * num_directions, batch, hidden_size)\n",
    "# -----------------------------------------------------------------------------\n",
    "if bidirectional == True:\n",
    "    num_directions = 2\n",
    "else:\n",
    "    num_directions = 1\n",
    "h_0 = torch.zeros((num_layers*num_directions, xin.size()[1], hidden_state_size))\n",
    "h_0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_NW(\n",
       "  (rnn): GRU(300, 32)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up RNN\n",
    "# -------------\n",
    "\n",
    "' SETTING UP THE RNN '\n",
    "\n",
    "model_rnn_senti = RNN_NW(rnn_mode,seq_vec_size,hidden_state_size,num_layers,bidirectional)\n",
    "model_rnn_senti.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (forwardpass): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up fully connected dense NN\n",
    "# -----------------------------------\n",
    "\n",
    "' SETTING UP THE DENSE NN '\n",
    "\n",
    "target_main_model_act = 'sigmoid'\n",
    "dense_layers = [h_0.size()[2],16,1]\n",
    "model_dense_senti = linear_fc(dense_layers, 'relu', target_main_model_act, 0.2)\n",
    "model_dense_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model dict\n",
    "# -------------------\n",
    "\n",
    "training_models_dict = {}\n",
    "training_models_dict['rnn'] = model_rnn_senti\n",
    "training_models_dict['dense'] = model_dense_senti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/venkateshmadhava/Documents/pmate2/pmate2_env/models/model_rnndense_sentiment.tar\n",
      "Epoch 1, minibatch 1 of 16 -- Model loss: 0.2479955554008484\n",
      "Epoch 1, minibatch 2 of 16 -- Model loss: 0.2546471357345581\n",
      "Epoch 1, minibatch 3 of 16 -- Model loss: 0.2504749596118927\n",
      "Epoch 1, minibatch 4 of 16 -- Model loss: 0.24182291328907013\n",
      "Epoch 1, minibatch 5 of 16 -- Model loss: 0.24123400449752808\n",
      "Epoch 1, minibatch 6 of 16 -- Model loss: 0.24330924451351166\n",
      "Epoch 1, minibatch 7 of 16 -- Model loss: 0.2597361207008362\n",
      "Epoch 1, minibatch 8 of 16 -- Model loss: 0.24123038351535797\n",
      "Epoch 1, minibatch 9 of 16 -- Model loss: 0.25934499502182007\n",
      "Epoch 1, minibatch 10 of 16 -- Model loss: 0.25601711869239807\n",
      "Epoch 1, minibatch 11 of 16 -- Model loss: 0.24160785973072052\n",
      "Epoch 1, minibatch 12 of 16 -- Model loss: 0.24975517392158508\n",
      "Epoch 1, minibatch 13 of 16 -- Model loss: 0.2624039053916931\n",
      "Epoch 1, minibatch 14 of 16 -- Model loss: 0.2433844953775406\n",
      "Epoch 1, minibatch 15 of 16 -- Model loss: 0.27386051416397095\n",
      "Epoch 1, minibatch 16 of 16 -- Model loss: 0.26749444007873535\n",
      "Epoch 2, minibatch 1 of 16 -- Model loss: 0.2394830882549286\n",
      "Epoch 2, minibatch 2 of 16 -- Model loss: 0.24864204227924347\n",
      "Epoch 2, minibatch 3 of 16 -- Model loss: 0.24320490658283234\n",
      "Epoch 2, minibatch 4 of 16 -- Model loss: 0.23888465762138367\n",
      "Epoch 2, minibatch 5 of 16 -- Model loss: 0.24109406769275665\n",
      "Epoch 2, minibatch 6 of 16 -- Model loss: 0.2442159503698349\n",
      "Epoch 2, minibatch 7 of 16 -- Model loss: 0.25296029448509216\n",
      "Epoch 2, minibatch 8 of 16 -- Model loss: 0.24120770394802094\n",
      "Epoch 2, minibatch 9 of 16 -- Model loss: 0.26071852445602417\n",
      "Epoch 2, minibatch 10 of 16 -- Model loss: 0.2499319314956665\n",
      "Epoch 2, minibatch 11 of 16 -- Model loss: 0.24163411557674408\n",
      "Epoch 2, minibatch 12 of 16 -- Model loss: 0.2467374950647354\n",
      "Epoch 2, minibatch 13 of 16 -- Model loss: 0.2556537985801697\n",
      "Epoch 2, minibatch 14 of 16 -- Model loss: 0.2408195286989212\n",
      "Epoch 2, minibatch 15 of 16 -- Model loss: 0.2667633295059204\n",
      "Epoch 2, minibatch 16 of 16 -- Model loss: 0.25395023822784424\n",
      "Epoch 3, minibatch 1 of 16 -- Model loss: 0.24021892249584198\n",
      "Epoch 3, minibatch 2 of 16 -- Model loss: 0.249581977725029\n",
      "Epoch 3, minibatch 3 of 16 -- Model loss: 0.24105100333690643\n",
      "Epoch 3, minibatch 4 of 16 -- Model loss: 0.24108368158340454\n",
      "Epoch 3, minibatch 5 of 16 -- Model loss: 0.23896123468875885\n",
      "Epoch 3, minibatch 6 of 16 -- Model loss: 0.24387019872665405\n",
      "Epoch 3, minibatch 7 of 16 -- Model loss: 0.2522008717060089\n",
      "Epoch 3, minibatch 8 of 16 -- Model loss: 0.24030454456806183\n",
      "Epoch 3, minibatch 9 of 16 -- Model loss: 0.2602561414241791\n",
      "Epoch 3, minibatch 10 of 16 -- Model loss: 0.24579954147338867\n",
      "Epoch 3, minibatch 11 of 16 -- Model loss: 0.2435392141342163\n",
      "Epoch 3, minibatch 12 of 16 -- Model loss: 0.24681535363197327\n",
      "Epoch 3, minibatch 13 of 16 -- Model loss: 0.257402628660202\n",
      "Epoch 3, minibatch 14 of 16 -- Model loss: 0.23928247392177582\n",
      "Epoch 3, minibatch 15 of 16 -- Model loss: 0.26250192523002625\n",
      "Epoch 3, minibatch 16 of 16 -- Model loss: 0.247150257229805\n",
      "Epoch 4, minibatch 1 of 16 -- Model loss: 0.2356080561876297\n",
      "Epoch 4, minibatch 2 of 16 -- Model loss: 0.24539542198181152\n",
      "Epoch 4, minibatch 3 of 16 -- Model loss: 0.23884865641593933\n",
      "Epoch 4, minibatch 4 of 16 -- Model loss: 0.23626111447811127\n",
      "Epoch 4, minibatch 5 of 16 -- Model loss: 0.23799970746040344\n",
      "Epoch 4, minibatch 6 of 16 -- Model loss: 0.24450162053108215\n",
      "Epoch 4, minibatch 7 of 16 -- Model loss: 0.2524526119232178\n",
      "Epoch 4, minibatch 8 of 16 -- Model loss: 0.2401612401008606\n",
      "Epoch 4, minibatch 9 of 16 -- Model loss: 0.2564953863620758\n",
      "Epoch 4, minibatch 10 of 16 -- Model loss: 0.24476255476474762\n",
      "Epoch 4, minibatch 11 of 16 -- Model loss: 0.24197201430797577\n",
      "Epoch 4, minibatch 12 of 16 -- Model loss: 0.24158363044261932\n",
      "Epoch 4, minibatch 13 of 16 -- Model loss: 0.24297434091567993\n",
      "Epoch 4, minibatch 14 of 16 -- Model loss: 0.23338159918785095\n",
      "Epoch 4, minibatch 15 of 16 -- Model loss: 0.25709131360054016\n",
      "Epoch 4, minibatch 16 of 16 -- Model loss: 0.2245776355266571\n",
      "Epoch 5, minibatch 1 of 16 -- Model loss: 0.23690524697303772\n",
      "Epoch 5, minibatch 2 of 16 -- Model loss: 0.2375836819410324\n",
      "Epoch 5, minibatch 3 of 16 -- Model loss: 0.2361036092042923\n",
      "Epoch 5, minibatch 4 of 16 -- Model loss: 0.23677794635295868\n",
      "Epoch 5, minibatch 5 of 16 -- Model loss: 0.2370021492242813\n",
      "Epoch 5, minibatch 6 of 16 -- Model loss: 0.24072624742984772\n",
      "Epoch 5, minibatch 7 of 16 -- Model loss: 0.24732664227485657\n",
      "Epoch 5, minibatch 8 of 16 -- Model loss: 0.24115747213363647\n",
      "Epoch 5, minibatch 9 of 16 -- Model loss: 0.24452029168605804\n",
      "Epoch 5, minibatch 10 of 16 -- Model loss: 0.24591737985610962\n",
      "Epoch 5, minibatch 11 of 16 -- Model loss: 0.2428530603647232\n",
      "Epoch 5, minibatch 12 of 16 -- Model loss: 0.2410717010498047\n",
      "Epoch 5, minibatch 13 of 16 -- Model loss: 0.24004104733467102\n",
      "Epoch 5, minibatch 14 of 16 -- Model loss: 0.23286910355091095\n",
      "Epoch 5, minibatch 15 of 16 -- Model loss: 0.2516612112522125\n",
      "Epoch 5, minibatch 16 of 16 -- Model loss: 0.2131020873785019\n",
      "Epoch 6, minibatch 1 of 16 -- Model loss: 0.2335415482521057\n",
      "Epoch 6, minibatch 2 of 16 -- Model loss: 0.22597761452198029\n",
      "Epoch 6, minibatch 3 of 16 -- Model loss: 0.23110167682170868\n",
      "Epoch 6, minibatch 4 of 16 -- Model loss: 0.2313070148229599\n",
      "Epoch 6, minibatch 5 of 16 -- Model loss: 0.23343220353126526\n",
      "Epoch 6, minibatch 6 of 16 -- Model loss: 0.2425709366798401\n",
      "Epoch 6, minibatch 7 of 16 -- Model loss: 0.24599039554595947\n",
      "Epoch 6, minibatch 8 of 16 -- Model loss: 0.24054750800132751\n",
      "Epoch 6, minibatch 9 of 16 -- Model loss: 0.2435639649629593\n",
      "Epoch 6, minibatch 10 of 16 -- Model loss: 0.2422892153263092\n",
      "Epoch 6, minibatch 11 of 16 -- Model loss: 0.24382874369621277\n",
      "Epoch 6, minibatch 12 of 16 -- Model loss: 0.23887567222118378\n",
      "Epoch 6, minibatch 13 of 16 -- Model loss: 0.2552669048309326\n",
      "Epoch 6, minibatch 14 of 16 -- Model loss: 0.22726194560527802\n",
      "Epoch 6, minibatch 15 of 16 -- Model loss: 0.2364356815814972\n",
      "Epoch 6, minibatch 16 of 16 -- Model loss: 0.21287576854228973\n",
      "Epoch 7, minibatch 1 of 16 -- Model loss: 0.2323066145181656\n",
      "Epoch 7, minibatch 2 of 16 -- Model loss: 0.22133217751979828\n",
      "Epoch 7, minibatch 3 of 16 -- Model loss: 0.2254575937986374\n",
      "Epoch 7, minibatch 4 of 16 -- Model loss: 0.22203011810779572\n",
      "Epoch 7, minibatch 5 of 16 -- Model loss: 0.22972597181797028\n",
      "Epoch 7, minibatch 6 of 16 -- Model loss: 0.24015982449054718\n",
      "Epoch 7, minibatch 7 of 16 -- Model loss: 0.24178732931613922\n",
      "Epoch 7, minibatch 8 of 16 -- Model loss: 0.23781996965408325\n",
      "Epoch 7, minibatch 9 of 16 -- Model loss: 0.23722146451473236\n",
      "Epoch 7, minibatch 10 of 16 -- Model loss: 0.2403130978345871\n",
      "Epoch 7, minibatch 11 of 16 -- Model loss: 0.24160856008529663\n",
      "Epoch 7, minibatch 12 of 16 -- Model loss: 0.23799876868724823\n",
      "Epoch 7, minibatch 13 of 16 -- Model loss: 0.2374083399772644\n",
      "Epoch 7, minibatch 14 of 16 -- Model loss: 0.2193242907524109\n",
      "Epoch 7, minibatch 15 of 16 -- Model loss: 0.23067663609981537\n",
      "Epoch 7, minibatch 16 of 16 -- Model loss: 0.19284175336360931\n",
      "Epoch 8, minibatch 1 of 16 -- Model loss: 0.22309598326683044\n",
      "Epoch 8, minibatch 2 of 16 -- Model loss: 0.22169235348701477\n",
      "Epoch 8, minibatch 3 of 16 -- Model loss: 0.22273574769496918\n",
      "Epoch 8, minibatch 4 of 16 -- Model loss: 0.21478044986724854\n",
      "Epoch 8, minibatch 5 of 16 -- Model loss: 0.226382777094841\n",
      "Epoch 8, minibatch 6 of 16 -- Model loss: 0.2229537069797516\n",
      "Epoch 8, minibatch 7 of 16 -- Model loss: 0.23063340783119202\n",
      "Epoch 8, minibatch 8 of 16 -- Model loss: 0.2332484871149063\n",
      "Epoch 8, minibatch 9 of 16 -- Model loss: 0.2382107973098755\n",
      "Epoch 8, minibatch 10 of 16 -- Model loss: 0.23223255574703217\n",
      "Epoch 8, minibatch 11 of 16 -- Model loss: 0.23603315651416779\n",
      "Epoch 8, minibatch 12 of 16 -- Model loss: 0.22457373142242432\n",
      "Epoch 8, minibatch 13 of 16 -- Model loss: 0.24603654444217682\n",
      "Epoch 8, minibatch 14 of 16 -- Model loss: 0.21878717839717865\n",
      "Epoch 8, minibatch 15 of 16 -- Model loss: 0.2134367823600769\n",
      "Epoch 8, minibatch 16 of 16 -- Model loss: 0.18924452364444733\n",
      "Epoch 9, minibatch 1 of 16 -- Model loss: 0.20357611775398254\n",
      "Epoch 9, minibatch 2 of 16 -- Model loss: 0.20535843074321747\n",
      "Epoch 9, minibatch 3 of 16 -- Model loss: 0.19950857758522034\n",
      "Epoch 9, minibatch 4 of 16 -- Model loss: 0.20214197039604187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, minibatch 5 of 16 -- Model loss: 0.2099061757326126\n",
      "Epoch 9, minibatch 6 of 16 -- Model loss: 0.1925184279680252\n",
      "Epoch 9, minibatch 7 of 16 -- Model loss: 0.19410356879234314\n",
      "Epoch 9, minibatch 8 of 16 -- Model loss: 0.21048276126384735\n",
      "Epoch 9, minibatch 9 of 16 -- Model loss: 0.21426492929458618\n",
      "Epoch 9, minibatch 10 of 16 -- Model loss: 0.22104546427726746\n",
      "Epoch 9, minibatch 11 of 16 -- Model loss: 0.18635447323322296\n",
      "Epoch 9, minibatch 12 of 16 -- Model loss: 0.20665013790130615\n",
      "Epoch 9, minibatch 13 of 16 -- Model loss: 0.18766018748283386\n",
      "Epoch 9, minibatch 14 of 16 -- Model loss: 0.1722354292869568\n",
      "Epoch 9, minibatch 15 of 16 -- Model loss: 0.20584087073802948\n",
      "Epoch 9, minibatch 16 of 16 -- Model loss: 0.23977099359035492\n",
      "Epoch 10, minibatch 1 of 16 -- Model loss: 0.18279124796390533\n",
      "Epoch 10, minibatch 2 of 16 -- Model loss: 0.16751131415367126\n",
      "Epoch 10, minibatch 3 of 16 -- Model loss: 0.15284226834774017\n",
      "Epoch 10, minibatch 4 of 16 -- Model loss: 0.14556549489498138\n",
      "Epoch 10, minibatch 5 of 16 -- Model loss: 0.1416385918855667\n",
      "Epoch 10, minibatch 6 of 16 -- Model loss: 0.13954229652881622\n",
      "Epoch 10, minibatch 7 of 16 -- Model loss: 0.16511012613773346\n",
      "Epoch 10, minibatch 8 of 16 -- Model loss: 0.1513545960187912\n",
      "Epoch 10, minibatch 9 of 16 -- Model loss: 0.1658775508403778\n",
      "Epoch 10, minibatch 10 of 16 -- Model loss: 0.17252019047737122\n",
      "Epoch 10, minibatch 11 of 16 -- Model loss: 0.14495542645454407\n",
      "Epoch 10, minibatch 12 of 16 -- Model loss: 0.18263116478919983\n",
      "Epoch 10, minibatch 13 of 16 -- Model loss: 0.12730517983436584\n",
      "Epoch 10, minibatch 14 of 16 -- Model loss: 0.13535702228546143\n",
      "Epoch 10, minibatch 15 of 16 -- Model loss: 0.19259211421012878\n",
      "Epoch 10, minibatch 16 of 16 -- Model loss: 0.24315245449543\n",
      "Epoch 11, minibatch 1 of 16 -- Model loss: 0.17434443533420563\n",
      "Epoch 11, minibatch 2 of 16 -- Model loss: 0.1547941267490387\n",
      "Epoch 11, minibatch 3 of 16 -- Model loss: 0.1383952796459198\n",
      "Epoch 11, minibatch 4 of 16 -- Model loss: 0.11899276077747345\n",
      "Epoch 11, minibatch 5 of 16 -- Model loss: 0.13780370354652405\n",
      "Epoch 11, minibatch 6 of 16 -- Model loss: 0.1348581612110138\n",
      "Epoch 11, minibatch 7 of 16 -- Model loss: 0.13570255041122437\n",
      "Epoch 11, minibatch 8 of 16 -- Model loss: 0.1285705268383026\n",
      "Epoch 11, minibatch 9 of 16 -- Model loss: 0.15140071511268616\n",
      "Epoch 11, minibatch 10 of 16 -- Model loss: 0.1446647197008133\n",
      "Epoch 11, minibatch 11 of 16 -- Model loss: 0.1375756710767746\n",
      "Epoch 11, minibatch 12 of 16 -- Model loss: 0.17000523209571838\n",
      "Epoch 11, minibatch 13 of 16 -- Model loss: 0.1309407502412796\n",
      "Epoch 11, minibatch 14 of 16 -- Model loss: 0.12020228803157806\n",
      "Epoch 11, minibatch 15 of 16 -- Model loss: 0.20441651344299316\n",
      "Epoch 11, minibatch 16 of 16 -- Model loss: 0.25904136896133423\n",
      "Epoch 12, minibatch 1 of 16 -- Model loss: 0.12481328845024109\n",
      "Epoch 12, minibatch 2 of 16 -- Model loss: 0.11687648296356201\n",
      "Epoch 12, minibatch 3 of 16 -- Model loss: 0.11543324589729309\n",
      "Epoch 12, minibatch 4 of 16 -- Model loss: 0.09541571140289307\n",
      "Epoch 12, minibatch 5 of 16 -- Model loss: 0.12626497447490692\n",
      "Epoch 12, minibatch 6 of 16 -- Model loss: 0.11883843690156937\n",
      "Epoch 12, minibatch 7 of 16 -- Model loss: 0.12037073075771332\n",
      "Epoch 12, minibatch 8 of 16 -- Model loss: 0.11358588933944702\n",
      "Epoch 12, minibatch 9 of 16 -- Model loss: 0.11236804723739624\n",
      "Epoch 12, minibatch 10 of 16 -- Model loss: 0.11844294518232346\n",
      "Epoch 12, minibatch 11 of 16 -- Model loss: 0.10367479175329208\n",
      "Epoch 12, minibatch 12 of 16 -- Model loss: 0.14408157765865326\n",
      "Epoch 12, minibatch 13 of 16 -- Model loss: 0.10350236296653748\n",
      "Epoch 12, minibatch 14 of 16 -- Model loss: 0.10397502779960632\n",
      "Epoch 12, minibatch 15 of 16 -- Model loss: 0.20824429392814636\n",
      "Epoch 12, minibatch 16 of 16 -- Model loss: 0.2612861692905426\n",
      "Epoch 13, minibatch 1 of 16 -- Model loss: 0.10750925540924072\n",
      "Epoch 13, minibatch 2 of 16 -- Model loss: 0.1089903712272644\n",
      "Epoch 13, minibatch 3 of 16 -- Model loss: 0.11384466290473938\n",
      "Epoch 13, minibatch 4 of 16 -- Model loss: 0.08753100037574768\n",
      "Epoch 13, minibatch 5 of 16 -- Model loss: 0.11779077351093292\n",
      "Epoch 13, minibatch 6 of 16 -- Model loss: 0.10022440552711487\n",
      "Epoch 13, minibatch 7 of 16 -- Model loss: 0.11477621644735336\n",
      "Epoch 13, minibatch 8 of 16 -- Model loss: 0.10448107123374939\n",
      "Epoch 13, minibatch 9 of 16 -- Model loss: 0.09456614404916763\n",
      "Epoch 13, minibatch 10 of 16 -- Model loss: 0.10322365164756775\n",
      "Epoch 13, minibatch 11 of 16 -- Model loss: 0.09125921130180359\n",
      "Epoch 13, minibatch 12 of 16 -- Model loss: 0.1298178881406784\n",
      "Epoch 13, minibatch 13 of 16 -- Model loss: 0.09508591145277023\n",
      "Epoch 13, minibatch 14 of 16 -- Model loss: 0.10175438225269318\n",
      "Epoch 13, minibatch 15 of 16 -- Model loss: 0.19985203444957733\n",
      "Epoch 13, minibatch 16 of 16 -- Model loss: 0.2543388605117798\n",
      "Epoch 14, minibatch 1 of 16 -- Model loss: 0.08702192455530167\n",
      "Epoch 14, minibatch 2 of 16 -- Model loss: 0.09416189044713974\n",
      "Epoch 14, minibatch 3 of 16 -- Model loss: 0.11098211258649826\n",
      "Epoch 14, minibatch 4 of 16 -- Model loss: 0.07252470403909683\n",
      "Epoch 14, minibatch 5 of 16 -- Model loss: 0.10521221160888672\n",
      "Epoch 14, minibatch 6 of 16 -- Model loss: 0.0873728096485138\n",
      "Epoch 14, minibatch 7 of 16 -- Model loss: 0.10616633296012878\n",
      "Epoch 14, minibatch 8 of 16 -- Model loss: 0.1034335047006607\n",
      "Epoch 14, minibatch 9 of 16 -- Model loss: 0.0981108546257019\n",
      "Epoch 14, minibatch 10 of 16 -- Model loss: 0.09468237310647964\n",
      "Epoch 14, minibatch 11 of 16 -- Model loss: 0.09025400131940842\n",
      "Epoch 14, minibatch 12 of 16 -- Model loss: 0.11788937449455261\n",
      "Epoch 14, minibatch 13 of 16 -- Model loss: 0.08981472253799438\n",
      "Epoch 14, minibatch 14 of 16 -- Model loss: 0.08112131059169769\n",
      "Epoch 14, minibatch 15 of 16 -- Model loss: 0.20076587796211243\n",
      "Epoch 14, minibatch 16 of 16 -- Model loss: 0.26305046677589417\n",
      "Epoch 15, minibatch 1 of 16 -- Model loss: 0.08734550327062607\n",
      "Epoch 15, minibatch 2 of 16 -- Model loss: 0.09174346923828125\n",
      "Epoch 15, minibatch 3 of 16 -- Model loss: 0.09876544773578644\n",
      "Epoch 15, minibatch 4 of 16 -- Model loss: 0.07663094997406006\n",
      "Epoch 15, minibatch 5 of 16 -- Model loss: 0.09394983947277069\n",
      "Epoch 15, minibatch 6 of 16 -- Model loss: 0.07844236493110657\n",
      "Epoch 15, minibatch 7 of 16 -- Model loss: 0.09215261787176132\n",
      "Epoch 15, minibatch 8 of 16 -- Model loss: 0.09026431292295456\n",
      "Epoch 15, minibatch 9 of 16 -- Model loss: 0.09163781255483627\n",
      "Epoch 15, minibatch 10 of 16 -- Model loss: 0.07577002048492432\n",
      "Epoch 15, minibatch 11 of 16 -- Model loss: 0.0887562558054924\n",
      "Epoch 15, minibatch 12 of 16 -- Model loss: 0.10149981081485748\n",
      "Epoch 15, minibatch 13 of 16 -- Model loss: 0.07787191867828369\n",
      "Epoch 15, minibatch 14 of 16 -- Model loss: 0.06757596135139465\n",
      "Epoch 15, minibatch 15 of 16 -- Model loss: 0.18235188722610474\n",
      "Epoch 15, minibatch 16 of 16 -- Model loss: 0.2522537112236023\n",
      "Epoch 16, minibatch 1 of 16 -- Model loss: 0.0807076245546341\n",
      "Epoch 16, minibatch 2 of 16 -- Model loss: 0.08197865635156631\n",
      "Epoch 16, minibatch 3 of 16 -- Model loss: 0.09049959480762482\n",
      "Epoch 16, minibatch 4 of 16 -- Model loss: 0.06387381255626678\n",
      "Epoch 16, minibatch 5 of 16 -- Model loss: 0.09301294386386871\n",
      "Epoch 16, minibatch 6 of 16 -- Model loss: 0.0727272480726242\n",
      "Epoch 16, minibatch 7 of 16 -- Model loss: 0.08967072516679764\n",
      "Epoch 16, minibatch 8 of 16 -- Model loss: 0.07966925203800201\n",
      "Epoch 16, minibatch 9 of 16 -- Model loss: 0.06844506412744522\n",
      "Epoch 16, minibatch 10 of 16 -- Model loss: 0.07219557464122772\n",
      "Epoch 16, minibatch 11 of 16 -- Model loss: 0.06910637766122818\n",
      "Epoch 16, minibatch 12 of 16 -- Model loss: 0.09301718324422836\n",
      "Epoch 16, minibatch 13 of 16 -- Model loss: 0.08487717807292938\n",
      "Epoch 16, minibatch 14 of 16 -- Model loss: 0.06917157024145126\n",
      "Epoch 16, minibatch 15 of 16 -- Model loss: 0.18808981776237488\n",
      "Epoch 16, minibatch 16 of 16 -- Model loss: 0.2552272379398346\n",
      "Epoch 17, minibatch 1 of 16 -- Model loss: 0.07287832349538803\n",
      "Epoch 17, minibatch 2 of 16 -- Model loss: 0.07782241702079773\n",
      "Epoch 17, minibatch 3 of 16 -- Model loss: 0.0906469002366066\n",
      "Epoch 17, minibatch 4 of 16 -- Model loss: 0.061489664018154144\n",
      "Epoch 17, minibatch 5 of 16 -- Model loss: 0.09309805929660797\n",
      "Epoch 17, minibatch 6 of 16 -- Model loss: 0.07552602887153625\n",
      "Epoch 17, minibatch 7 of 16 -- Model loss: 0.08625610917806625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, minibatch 8 of 16 -- Model loss: 0.079726941883564\n",
      "Epoch 17, minibatch 9 of 16 -- Model loss: 0.0605379194021225\n",
      "Epoch 17, minibatch 10 of 16 -- Model loss: 0.05978836491703987\n",
      "Epoch 17, minibatch 11 of 16 -- Model loss: 0.0632249042391777\n",
      "Epoch 17, minibatch 12 of 16 -- Model loss: 0.08553969860076904\n",
      "Epoch 17, minibatch 13 of 16 -- Model loss: 0.08979213237762451\n",
      "Epoch 17, minibatch 14 of 16 -- Model loss: 0.07081244140863419\n",
      "Epoch 17, minibatch 15 of 16 -- Model loss: 0.19838890433311462\n",
      "Epoch 17, minibatch 16 of 16 -- Model loss: 0.2599713206291199\n",
      "Epoch 18, minibatch 1 of 16 -- Model loss: 0.05536564812064171\n",
      "Epoch 18, minibatch 2 of 16 -- Model loss: 0.06470775604248047\n",
      "Epoch 18, minibatch 3 of 16 -- Model loss: 0.07366941124200821\n",
      "Epoch 18, minibatch 4 of 16 -- Model loss: 0.04658177122473717\n",
      "Epoch 18, minibatch 5 of 16 -- Model loss: 0.08584412187337875\n",
      "Epoch 18, minibatch 6 of 16 -- Model loss: 0.06748919188976288\n",
      "Epoch 18, minibatch 7 of 16 -- Model loss: 0.08483895659446716\n",
      "Epoch 18, minibatch 8 of 16 -- Model loss: 0.08887872844934464\n",
      "Epoch 18, minibatch 9 of 16 -- Model loss: 0.07162931561470032\n",
      "Epoch 18, minibatch 10 of 16 -- Model loss: 0.07123706489801407\n",
      "Epoch 18, minibatch 11 of 16 -- Model loss: 0.07849383354187012\n",
      "Epoch 18, minibatch 12 of 16 -- Model loss: 0.08254697918891907\n",
      "Epoch 18, minibatch 13 of 16 -- Model loss: 0.07308970391750336\n",
      "Epoch 18, minibatch 14 of 16 -- Model loss: 0.047655101865530014\n",
      "Epoch 18, minibatch 15 of 16 -- Model loss: 0.16613733768463135\n",
      "Epoch 18, minibatch 16 of 16 -- Model loss: 0.2445981353521347\n",
      "Epoch 19, minibatch 1 of 16 -- Model loss: 0.06252451986074448\n",
      "Epoch 19, minibatch 2 of 16 -- Model loss: 0.06259036064147949\n",
      "Epoch 19, minibatch 3 of 16 -- Model loss: 0.07191844284534454\n",
      "Epoch 19, minibatch 4 of 16 -- Model loss: 0.05889712646603584\n",
      "Epoch 19, minibatch 5 of 16 -- Model loss: 0.06600413471460342\n",
      "Epoch 19, minibatch 6 of 16 -- Model loss: 0.06291019171476364\n",
      "Epoch 19, minibatch 7 of 16 -- Model loss: 0.0688520222902298\n",
      "Epoch 19, minibatch 8 of 16 -- Model loss: 0.05487283319234848\n",
      "Epoch 19, minibatch 9 of 16 -- Model loss: 0.05084258317947388\n",
      "Epoch 19, minibatch 10 of 16 -- Model loss: 0.06753391027450562\n",
      "Epoch 19, minibatch 11 of 16 -- Model loss: 0.06884083151817322\n",
      "Epoch 19, minibatch 12 of 16 -- Model loss: 0.0750044584274292\n",
      "Epoch 19, minibatch 13 of 16 -- Model loss: 0.06683039665222168\n",
      "Epoch 19, minibatch 14 of 16 -- Model loss: 0.04273279756307602\n",
      "Epoch 19, minibatch 15 of 16 -- Model loss: 0.14428505301475525\n",
      "Epoch 19, minibatch 16 of 16 -- Model loss: 0.23715974390506744\n",
      "Epoch 20, minibatch 1 of 16 -- Model loss: 0.04940308257937431\n",
      "Epoch 20, minibatch 2 of 16 -- Model loss: 0.0597296766936779\n",
      "Epoch 20, minibatch 3 of 16 -- Model loss: 0.060435399413108826\n",
      "Epoch 20, minibatch 4 of 16 -- Model loss: 0.03548620641231537\n",
      "Epoch 20, minibatch 5 of 16 -- Model loss: 0.06322222948074341\n",
      "Epoch 20, minibatch 6 of 16 -- Model loss: 0.06345824152231216\n",
      "Epoch 20, minibatch 7 of 16 -- Model loss: 0.0636887401342392\n",
      "Epoch 20, minibatch 8 of 16 -- Model loss: 0.052432622760534286\n",
      "Epoch 20, minibatch 9 of 16 -- Model loss: 0.04618768021464348\n",
      "Epoch 20, minibatch 10 of 16 -- Model loss: 0.054203882813453674\n",
      "Epoch 20, minibatch 11 of 16 -- Model loss: 0.05310741439461708\n",
      "Epoch 20, minibatch 12 of 16 -- Model loss: 0.06310328096151352\n",
      "Epoch 20, minibatch 13 of 16 -- Model loss: 0.057700056582689285\n",
      "Epoch 20, minibatch 14 of 16 -- Model loss: 0.03541911765933037\n",
      "Epoch 20, minibatch 15 of 16 -- Model loss: 0.1413954794406891\n",
      "Epoch 20, minibatch 16 of 16 -- Model loss: 0.22654810547828674\n",
      "Epoch 21, minibatch 1 of 16 -- Model loss: 0.0465041808784008\n",
      "Epoch 21, minibatch 2 of 16 -- Model loss: 0.06301789730787277\n",
      "Epoch 21, minibatch 3 of 16 -- Model loss: 0.056475065648555756\n",
      "Epoch 21, minibatch 4 of 16 -- Model loss: 0.032515015453100204\n",
      "Epoch 21, minibatch 5 of 16 -- Model loss: 0.05690864101052284\n",
      "Epoch 21, minibatch 6 of 16 -- Model loss: 0.04976653307676315\n",
      "Epoch 21, minibatch 7 of 16 -- Model loss: 0.05701760575175285\n",
      "Epoch 21, minibatch 8 of 16 -- Model loss: 0.04440693184733391\n",
      "Epoch 21, minibatch 9 of 16 -- Model loss: 0.034825846552848816\n",
      "Epoch 21, minibatch 10 of 16 -- Model loss: 0.053630441427230835\n",
      "Epoch 21, minibatch 11 of 16 -- Model loss: 0.04730938747525215\n",
      "Epoch 21, minibatch 12 of 16 -- Model loss: 0.05668869987130165\n",
      "Epoch 21, minibatch 13 of 16 -- Model loss: 0.049263715744018555\n",
      "Epoch 21, minibatch 14 of 16 -- Model loss: 0.026710577309131622\n",
      "Epoch 21, minibatch 15 of 16 -- Model loss: 0.13752762973308563\n",
      "Epoch 21, minibatch 16 of 16 -- Model loss: 0.2275659590959549\n",
      "Epoch 22, minibatch 1 of 16 -- Model loss: 0.049257542937994\n",
      "Epoch 22, minibatch 2 of 16 -- Model loss: 0.057993195950984955\n",
      "Epoch 22, minibatch 3 of 16 -- Model loss: 0.0595560222864151\n",
      "Epoch 22, minibatch 4 of 16 -- Model loss: 0.031105659902095795\n",
      "Epoch 22, minibatch 5 of 16 -- Model loss: 0.06216851621866226\n",
      "Epoch 22, minibatch 6 of 16 -- Model loss: 0.04687252640724182\n",
      "Epoch 22, minibatch 7 of 16 -- Model loss: 0.06452999264001846\n",
      "Epoch 22, minibatch 8 of 16 -- Model loss: 0.038384102284908295\n",
      "Epoch 22, minibatch 9 of 16 -- Model loss: 0.031318653374910355\n",
      "Epoch 22, minibatch 10 of 16 -- Model loss: 0.05317658931016922\n",
      "Epoch 22, minibatch 11 of 16 -- Model loss: 0.036517854779958725\n",
      "Epoch 22, minibatch 12 of 16 -- Model loss: 0.05167286470532417\n",
      "Epoch 22, minibatch 13 of 16 -- Model loss: 0.053741391748189926\n",
      "Epoch 22, minibatch 14 of 16 -- Model loss: 0.031199708580970764\n",
      "Epoch 22, minibatch 15 of 16 -- Model loss: 0.15364286303520203\n",
      "Epoch 22, minibatch 16 of 16 -- Model loss: 0.23107460141181946\n",
      "Epoch 23, minibatch 1 of 16 -- Model loss: 0.034756481647491455\n",
      "Epoch 23, minibatch 2 of 16 -- Model loss: 0.0419662669301033\n",
      "Epoch 23, minibatch 3 of 16 -- Model loss: 0.04993777722120285\n",
      "Epoch 23, minibatch 4 of 16 -- Model loss: 0.029996806755661964\n",
      "Epoch 23, minibatch 5 of 16 -- Model loss: 0.06711797416210175\n",
      "Epoch 23, minibatch 6 of 16 -- Model loss: 0.05699942633509636\n",
      "Epoch 23, minibatch 7 of 16 -- Model loss: 0.06920354068279266\n",
      "Epoch 23, minibatch 8 of 16 -- Model loss: 0.05797811225056648\n",
      "Epoch 23, minibatch 9 of 16 -- Model loss: 0.03852847218513489\n",
      "Epoch 23, minibatch 10 of 16 -- Model loss: 0.05180162936449051\n",
      "Epoch 23, minibatch 11 of 16 -- Model loss: 0.043073564767837524\n",
      "Epoch 23, minibatch 12 of 16 -- Model loss: 0.04502591863274574\n",
      "Epoch 23, minibatch 13 of 16 -- Model loss: 0.06119043007493019\n",
      "Epoch 23, minibatch 14 of 16 -- Model loss: 0.033078547567129135\n",
      "Epoch 23, minibatch 15 of 16 -- Model loss: 0.1709858477115631\n",
      "Epoch 23, minibatch 16 of 16 -- Model loss: 0.27018028497695923\n",
      "Epoch 24, minibatch 1 of 16 -- Model loss: 0.03110954537987709\n",
      "Epoch 24, minibatch 2 of 16 -- Model loss: 0.03906909003853798\n",
      "Epoch 24, minibatch 3 of 16 -- Model loss: 0.033124372363090515\n",
      "Epoch 24, minibatch 4 of 16 -- Model loss: 0.025404363870620728\n",
      "Epoch 24, minibatch 5 of 16 -- Model loss: 0.04121807590126991\n",
      "Epoch 24, minibatch 6 of 16 -- Model loss: 0.04765946790575981\n",
      "Epoch 24, minibatch 7 of 16 -- Model loss: 0.054979708045721054\n",
      "Epoch 24, minibatch 8 of 16 -- Model loss: 0.04625100642442703\n",
      "Epoch 24, minibatch 9 of 16 -- Model loss: 0.04293059930205345\n",
      "Epoch 24, minibatch 10 of 16 -- Model loss: 0.04977777600288391\n",
      "Epoch 24, minibatch 11 of 16 -- Model loss: 0.06420926749706268\n",
      "Epoch 24, minibatch 12 of 16 -- Model loss: 0.05328601971268654\n",
      "Epoch 24, minibatch 13 of 16 -- Model loss: 0.0434950590133667\n",
      "Epoch 24, minibatch 14 of 16 -- Model loss: 0.022411443293094635\n",
      "Epoch 24, minibatch 15 of 16 -- Model loss: 0.1314091980457306\n",
      "Epoch 24, minibatch 16 of 16 -- Model loss: 0.2137453556060791\n",
      "Epoch 25, minibatch 1 of 16 -- Model loss: 0.030904173851013184\n",
      "Epoch 25, minibatch 2 of 16 -- Model loss: 0.040524084120988846\n",
      "Epoch 25, minibatch 3 of 16 -- Model loss: 0.03346211463212967\n",
      "Epoch 25, minibatch 4 of 16 -- Model loss: 0.05410250276327133\n",
      "Epoch 25, minibatch 5 of 16 -- Model loss: 0.03614235296845436\n",
      "Epoch 25, minibatch 6 of 16 -- Model loss: 0.0489712730050087\n",
      "Epoch 25, minibatch 7 of 16 -- Model loss: 0.06267248839139938\n",
      "Epoch 25, minibatch 8 of 16 -- Model loss: 0.028920765966176987\n",
      "Epoch 25, minibatch 9 of 16 -- Model loss: 0.031451329588890076\n",
      "Epoch 25, minibatch 10 of 16 -- Model loss: 0.04901626333594322\n",
      "Epoch 25, minibatch 11 of 16 -- Model loss: 0.03913423418998718\n",
      "Epoch 25, minibatch 12 of 16 -- Model loss: 0.0493110716342926\n",
      "Epoch 25, minibatch 13 of 16 -- Model loss: 0.04825815558433533\n",
      "Epoch 25, minibatch 14 of 16 -- Model loss: 0.026867875829339027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, minibatch 15 of 16 -- Model loss: 0.12214558571577072\n",
      "Epoch 25, minibatch 16 of 16 -- Model loss: 0.1916632056236267\n",
      "Epoch 26, minibatch 1 of 16 -- Model loss: 0.024723175913095474\n",
      "Epoch 26, minibatch 2 of 16 -- Model loss: 0.034868475049734116\n",
      "Epoch 26, minibatch 3 of 16 -- Model loss: 0.02544497139751911\n",
      "Epoch 26, minibatch 4 of 16 -- Model loss: 0.028505517169833183\n",
      "Epoch 26, minibatch 5 of 16 -- Model loss: 0.02903347834944725\n",
      "Epoch 26, minibatch 6 of 16 -- Model loss: 0.03954000025987625\n",
      "Epoch 26, minibatch 7 of 16 -- Model loss: 0.04748079925775528\n",
      "Epoch 26, minibatch 8 of 16 -- Model loss: 0.030903775244951248\n",
      "Epoch 26, minibatch 9 of 16 -- Model loss: 0.026273993775248528\n",
      "Epoch 26, minibatch 10 of 16 -- Model loss: 0.045531682670116425\n",
      "Epoch 26, minibatch 11 of 16 -- Model loss: 0.026711855083703995\n",
      "Epoch 26, minibatch 12 of 16 -- Model loss: 0.03980354592204094\n",
      "Epoch 26, minibatch 13 of 16 -- Model loss: 0.037662770599126816\n",
      "Epoch 26, minibatch 14 of 16 -- Model loss: 0.01908363401889801\n",
      "Epoch 26, minibatch 15 of 16 -- Model loss: 0.11885064840316772\n",
      "Epoch 26, minibatch 16 of 16 -- Model loss: 0.2315613329410553\n",
      "Epoch 27, minibatch 1 of 16 -- Model loss: 0.025925185531377792\n",
      "Epoch 27, minibatch 2 of 16 -- Model loss: 0.02782597951591015\n",
      "Epoch 27, minibatch 3 of 16 -- Model loss: 0.025373589247465134\n",
      "Epoch 27, minibatch 4 of 16 -- Model loss: 0.019875265657901764\n",
      "Epoch 27, minibatch 5 of 16 -- Model loss: 0.0452481247484684\n",
      "Epoch 27, minibatch 6 of 16 -- Model loss: 0.037291936576366425\n",
      "Epoch 27, minibatch 7 of 16 -- Model loss: 0.052125927060842514\n",
      "Epoch 27, minibatch 8 of 16 -- Model loss: 0.027363991364836693\n",
      "Epoch 27, minibatch 9 of 16 -- Model loss: 0.02854044921696186\n",
      "Epoch 27, minibatch 10 of 16 -- Model loss: 0.043794143944978714\n",
      "Epoch 27, minibatch 11 of 16 -- Model loss: 0.026985924690961838\n",
      "Epoch 27, minibatch 12 of 16 -- Model loss: 0.03653975948691368\n",
      "Epoch 27, minibatch 13 of 16 -- Model loss: 0.035393234342336655\n",
      "Epoch 27, minibatch 14 of 16 -- Model loss: 0.014335466548800468\n",
      "Epoch 27, minibatch 15 of 16 -- Model loss: 0.10967275500297546\n",
      "Epoch 27, minibatch 16 of 16 -- Model loss: 0.18419693410396576\n",
      "Epoch 28, minibatch 1 of 16 -- Model loss: 0.02043209783732891\n",
      "Epoch 28, minibatch 2 of 16 -- Model loss: 0.027486970648169518\n",
      "Epoch 28, minibatch 3 of 16 -- Model loss: 0.022666599601507187\n",
      "Epoch 28, minibatch 4 of 16 -- Model loss: 0.018805865198373795\n",
      "Epoch 28, minibatch 5 of 16 -- Model loss: 0.025399036705493927\n",
      "Epoch 28, minibatch 6 of 16 -- Model loss: 0.03289756551384926\n",
      "Epoch 28, minibatch 7 of 16 -- Model loss: 0.04779759794473648\n",
      "Epoch 28, minibatch 8 of 16 -- Model loss: 0.02331605553627014\n",
      "Epoch 28, minibatch 9 of 16 -- Model loss: 0.020837698131799698\n",
      "Epoch 28, minibatch 10 of 16 -- Model loss: 0.04003641754388809\n",
      "Epoch 28, minibatch 11 of 16 -- Model loss: 0.026298487558960915\n",
      "Epoch 28, minibatch 12 of 16 -- Model loss: 0.03613999858498573\n",
      "Epoch 28, minibatch 13 of 16 -- Model loss: 0.030680198222398758\n",
      "Epoch 28, minibatch 14 of 16 -- Model loss: 0.009937161579728127\n",
      "Epoch 28, minibatch 15 of 16 -- Model loss: 0.09955482184886932\n",
      "Epoch 28, minibatch 16 of 16 -- Model loss: 0.17151585221290588\n",
      "Epoch 29, minibatch 1 of 16 -- Model loss: 0.015610680915415287\n",
      "Epoch 29, minibatch 2 of 16 -- Model loss: 0.015839731320738792\n",
      "Epoch 29, minibatch 3 of 16 -- Model loss: 0.016978396102786064\n",
      "Epoch 29, minibatch 4 of 16 -- Model loss: 0.012270648032426834\n",
      "Epoch 29, minibatch 5 of 16 -- Model loss: 0.02176797389984131\n",
      "Epoch 29, minibatch 6 of 16 -- Model loss: 0.02785002626478672\n",
      "Epoch 29, minibatch 7 of 16 -- Model loss: 0.046314068138599396\n",
      "Epoch 29, minibatch 8 of 16 -- Model loss: 0.016616277396678925\n",
      "Epoch 29, minibatch 9 of 16 -- Model loss: 0.016017189249396324\n",
      "Epoch 29, minibatch 10 of 16 -- Model loss: 0.04248538985848427\n",
      "Epoch 29, minibatch 11 of 16 -- Model loss: 0.025139804929494858\n",
      "Epoch 29, minibatch 12 of 16 -- Model loss: 0.035199522972106934\n",
      "Epoch 29, minibatch 13 of 16 -- Model loss: 0.03172324225306511\n",
      "Epoch 29, minibatch 14 of 16 -- Model loss: 0.009295801632106304\n",
      "Epoch 29, minibatch 15 of 16 -- Model loss: 0.08595921844244003\n",
      "Epoch 29, minibatch 16 of 16 -- Model loss: 0.16569644212722778\n",
      "Epoch 30, minibatch 1 of 16 -- Model loss: 0.01454984862357378\n",
      "Epoch 30, minibatch 2 of 16 -- Model loss: 0.017056284472346306\n",
      "Epoch 30, minibatch 3 of 16 -- Model loss: 0.017290597781538963\n",
      "Epoch 30, minibatch 4 of 16 -- Model loss: 0.009195206686854362\n",
      "Epoch 30, minibatch 5 of 16 -- Model loss: 0.013210959732532501\n",
      "Epoch 30, minibatch 6 of 16 -- Model loss: 0.03377543017268181\n",
      "Epoch 30, minibatch 7 of 16 -- Model loss: 0.040172457695007324\n",
      "Epoch 30, minibatch 8 of 16 -- Model loss: 0.016981864348053932\n",
      "Epoch 30, minibatch 9 of 16 -- Model loss: 0.012707374058663845\n",
      "Epoch 30, minibatch 10 of 16 -- Model loss: 0.03330961987376213\n",
      "Epoch 30, minibatch 11 of 16 -- Model loss: 0.018876938149333\n",
      "Epoch 30, minibatch 12 of 16 -- Model loss: 0.03228675201535225\n",
      "Epoch 30, minibatch 13 of 16 -- Model loss: 0.02760935015976429\n",
      "Epoch 30, minibatch 14 of 16 -- Model loss: 0.010121744126081467\n",
      "Epoch 30, minibatch 15 of 16 -- Model loss: 0.07924730330705643\n",
      "Epoch 30, minibatch 16 of 16 -- Model loss: 0.163986936211586\n",
      "Epoch 31, minibatch 1 of 16 -- Model loss: 0.013828080147504807\n",
      "Epoch 31, minibatch 2 of 16 -- Model loss: 0.015563118271529675\n",
      "Epoch 31, minibatch 3 of 16 -- Model loss: 0.01813235878944397\n",
      "Epoch 31, minibatch 4 of 16 -- Model loss: 0.0116198705509305\n",
      "Epoch 31, minibatch 5 of 16 -- Model loss: 0.017952390015125275\n",
      "Epoch 31, minibatch 6 of 16 -- Model loss: 0.02989363856613636\n",
      "Epoch 31, minibatch 7 of 16 -- Model loss: 0.04120360314846039\n",
      "Epoch 31, minibatch 8 of 16 -- Model loss: 0.012515291571617126\n",
      "Epoch 31, minibatch 9 of 16 -- Model loss: 0.015438784845173359\n",
      "Epoch 31, minibatch 10 of 16 -- Model loss: 0.0338706336915493\n",
      "Epoch 31, minibatch 11 of 16 -- Model loss: 0.010715057142078876\n",
      "Epoch 31, minibatch 12 of 16 -- Model loss: 0.029563555493950844\n",
      "Epoch 31, minibatch 13 of 16 -- Model loss: 0.025183504447340965\n",
      "Epoch 31, minibatch 14 of 16 -- Model loss: 0.011905767023563385\n",
      "Epoch 31, minibatch 15 of 16 -- Model loss: 0.09445090591907501\n",
      "Epoch 31, minibatch 16 of 16 -- Model loss: 0.16636238992214203\n",
      "Epoch 32, minibatch 1 of 16 -- Model loss: 0.013103397563099861\n",
      "Epoch 32, minibatch 2 of 16 -- Model loss: 0.015111180953681469\n",
      "Epoch 32, minibatch 3 of 16 -- Model loss: 0.03776003047823906\n",
      "Epoch 32, minibatch 4 of 16 -- Model loss: 0.02092740498483181\n",
      "Epoch 32, minibatch 5 of 16 -- Model loss: 0.03119565173983574\n",
      "Epoch 32, minibatch 6 of 16 -- Model loss: 0.04165166988968849\n",
      "Epoch 32, minibatch 7 of 16 -- Model loss: 0.04550951346755028\n",
      "Epoch 32, minibatch 8 of 16 -- Model loss: 0.022247839719057083\n",
      "Epoch 32, minibatch 9 of 16 -- Model loss: 0.011849399656057358\n",
      "Epoch 32, minibatch 10 of 16 -- Model loss: 0.047801658511161804\n",
      "Epoch 32, minibatch 11 of 16 -- Model loss: 0.023132052272558212\n",
      "Epoch 32, minibatch 12 of 16 -- Model loss: 0.031863510608673096\n",
      "Epoch 32, minibatch 13 of 16 -- Model loss: 0.0249764584004879\n",
      "Epoch 32, minibatch 14 of 16 -- Model loss: 0.010680865496397018\n",
      "Epoch 32, minibatch 15 of 16 -- Model loss: 0.09302130341529846\n",
      "Epoch 32, minibatch 16 of 16 -- Model loss: 0.20046402513980865\n",
      "Epoch 33, minibatch 1 of 16 -- Model loss: 0.014555216766893864\n",
      "Epoch 33, minibatch 2 of 16 -- Model loss: 0.014132988639175892\n",
      "Epoch 33, minibatch 3 of 16 -- Model loss: 0.016063254326581955\n",
      "Epoch 33, minibatch 4 of 16 -- Model loss: 0.012889805249869823\n",
      "Epoch 33, minibatch 5 of 16 -- Model loss: 0.04272600635886192\n",
      "Epoch 33, minibatch 6 of 16 -- Model loss: 0.038217488676309586\n",
      "Epoch 33, minibatch 7 of 16 -- Model loss: 0.04111245647072792\n",
      "Epoch 33, minibatch 8 of 16 -- Model loss: 0.02690877765417099\n",
      "Epoch 33, minibatch 9 of 16 -- Model loss: 0.015612997114658356\n",
      "Epoch 33, minibatch 10 of 16 -- Model loss: 0.04175886511802673\n",
      "Epoch 33, minibatch 11 of 16 -- Model loss: 0.02675487846136093\n",
      "Epoch 33, minibatch 12 of 16 -- Model loss: 0.03445529565215111\n",
      "Epoch 33, minibatch 13 of 16 -- Model loss: 0.020652204751968384\n",
      "Epoch 33, minibatch 14 of 16 -- Model loss: 0.009059242904186249\n",
      "Epoch 33, minibatch 15 of 16 -- Model loss: 0.09168396145105362\n",
      "Epoch 33, minibatch 16 of 16 -- Model loss: 0.189224511384964\n",
      "Epoch 34, minibatch 1 of 16 -- Model loss: 0.012126038782298565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, minibatch 2 of 16 -- Model loss: 0.019932428374886513\n",
      "Epoch 34, minibatch 3 of 16 -- Model loss: 0.01464405469596386\n",
      "Epoch 34, minibatch 4 of 16 -- Model loss: 0.018249066546559334\n",
      "Epoch 34, minibatch 5 of 16 -- Model loss: 0.013890162110328674\n",
      "Epoch 34, minibatch 6 of 16 -- Model loss: 0.028423219919204712\n",
      "Epoch 34, minibatch 7 of 16 -- Model loss: 0.038935158401727676\n",
      "Epoch 34, minibatch 8 of 16 -- Model loss: 0.016780883073806763\n",
      "Epoch 34, minibatch 9 of 16 -- Model loss: 0.014514394104480743\n",
      "Epoch 34, minibatch 10 of 16 -- Model loss: 0.0384250208735466\n",
      "Epoch 34, minibatch 11 of 16 -- Model loss: 0.030875762924551964\n",
      "Epoch 34, minibatch 12 of 16 -- Model loss: 0.03314324468374252\n",
      "Epoch 34, minibatch 13 of 16 -- Model loss: 0.018022645264863968\n",
      "Epoch 34, minibatch 14 of 16 -- Model loss: 0.010915474034845829\n",
      "Epoch 34, minibatch 15 of 16 -- Model loss: 0.06956348568201065\n",
      "Epoch 34, minibatch 16 of 16 -- Model loss: 0.17803147435188293\n",
      "Epoch 35, minibatch 1 of 16 -- Model loss: 0.01572597585618496\n",
      "Epoch 35, minibatch 2 of 16 -- Model loss: 0.013523693196475506\n",
      "Epoch 35, minibatch 3 of 16 -- Model loss: 0.011247139424085617\n",
      "Epoch 35, minibatch 4 of 16 -- Model loss: 0.011409902945160866\n",
      "Epoch 35, minibatch 5 of 16 -- Model loss: 0.010412756353616714\n",
      "Epoch 35, minibatch 6 of 16 -- Model loss: 0.031084861606359482\n",
      "Epoch 35, minibatch 7 of 16 -- Model loss: 0.031636983156204224\n",
      "Epoch 35, minibatch 8 of 16 -- Model loss: 0.014832304790616035\n",
      "Epoch 35, minibatch 9 of 16 -- Model loss: 0.014647422358393669\n",
      "Epoch 35, minibatch 10 of 16 -- Model loss: 0.032311949878931046\n",
      "Epoch 35, minibatch 11 of 16 -- Model loss: 0.013942716643214226\n",
      "Epoch 35, minibatch 12 of 16 -- Model loss: 0.03350235894322395\n",
      "Epoch 35, minibatch 13 of 16 -- Model loss: 0.018798204138875008\n",
      "Epoch 35, minibatch 14 of 16 -- Model loss: 0.006398162338882685\n",
      "Epoch 35, minibatch 15 of 16 -- Model loss: 0.0664033517241478\n",
      "Epoch 35, minibatch 16 of 16 -- Model loss: 0.16866597533226013\n",
      "Epoch 36, minibatch 1 of 16 -- Model loss: 0.00935595203191042\n",
      "Epoch 36, minibatch 2 of 16 -- Model loss: 0.012654048390686512\n",
      "Epoch 36, minibatch 3 of 16 -- Model loss: 0.011491622775793076\n",
      "Epoch 36, minibatch 4 of 16 -- Model loss: 0.006099482532590628\n",
      "Epoch 36, minibatch 5 of 16 -- Model loss: 0.00869771745055914\n",
      "Epoch 36, minibatch 6 of 16 -- Model loss: 0.02690448984503746\n",
      "Epoch 36, minibatch 7 of 16 -- Model loss: 0.028562985360622406\n",
      "Epoch 36, minibatch 8 of 16 -- Model loss: 0.014571047388017178\n",
      "Epoch 36, minibatch 9 of 16 -- Model loss: 0.016124224290251732\n",
      "Epoch 36, minibatch 10 of 16 -- Model loss: 0.03891502320766449\n",
      "Epoch 36, minibatch 11 of 16 -- Model loss: 0.01847747527062893\n",
      "Epoch 36, minibatch 12 of 16 -- Model loss: 0.026945218443870544\n",
      "Epoch 36, minibatch 13 of 16 -- Model loss: 0.016489416360855103\n",
      "Epoch 36, minibatch 14 of 16 -- Model loss: 0.005508210510015488\n",
      "Epoch 36, minibatch 15 of 16 -- Model loss: 0.06565756350755692\n",
      "Epoch 36, minibatch 16 of 16 -- Model loss: 0.16395799815654755\n",
      "Epoch 37, minibatch 1 of 16 -- Model loss: 0.009000958874821663\n",
      "Epoch 37, minibatch 2 of 16 -- Model loss: 0.008459116332232952\n",
      "Epoch 37, minibatch 3 of 16 -- Model loss: 0.01032713521271944\n",
      "Epoch 37, minibatch 4 of 16 -- Model loss: 0.00693122623488307\n",
      "Epoch 37, minibatch 5 of 16 -- Model loss: 0.00889358390122652\n",
      "Epoch 37, minibatch 6 of 16 -- Model loss: 0.025651942938566208\n",
      "Epoch 37, minibatch 7 of 16 -- Model loss: 0.026751656085252762\n",
      "Epoch 37, minibatch 8 of 16 -- Model loss: 0.014347297139465809\n",
      "Epoch 37, minibatch 9 of 16 -- Model loss: 0.010165880434215069\n",
      "Epoch 37, minibatch 10 of 16 -- Model loss: 0.029475755989551544\n",
      "Epoch 37, minibatch 11 of 16 -- Model loss: 0.018493421375751495\n",
      "Epoch 37, minibatch 12 of 16 -- Model loss: 0.032567594200372696\n",
      "Epoch 37, minibatch 13 of 16 -- Model loss: 0.02085266448557377\n",
      "Epoch 37, minibatch 14 of 16 -- Model loss: 0.005264827981591225\n",
      "Epoch 37, minibatch 15 of 16 -- Model loss: 0.05982731655240059\n",
      "Epoch 37, minibatch 16 of 16 -- Model loss: 0.15685376524925232\n",
      "Epoch 38, minibatch 1 of 16 -- Model loss: 0.009152949787676334\n",
      "Epoch 38, minibatch 2 of 16 -- Model loss: 0.01044708676636219\n",
      "Epoch 38, minibatch 3 of 16 -- Model loss: 0.00814238004386425\n",
      "Epoch 38, minibatch 4 of 16 -- Model loss: 0.00871360581368208\n",
      "Epoch 38, minibatch 5 of 16 -- Model loss: 0.007507974281907082\n",
      "Epoch 38, minibatch 6 of 16 -- Model loss: 0.023326147347688675\n",
      "Epoch 38, minibatch 7 of 16 -- Model loss: 0.023054659366607666\n",
      "Epoch 38, minibatch 8 of 16 -- Model loss: 0.011268632486462593\n",
      "Epoch 38, minibatch 9 of 16 -- Model loss: 0.010294286534190178\n",
      "Epoch 38, minibatch 10 of 16 -- Model loss: 0.024543100968003273\n",
      "Epoch 38, minibatch 11 of 16 -- Model loss: 0.012916257604956627\n",
      "Epoch 38, minibatch 12 of 16 -- Model loss: 0.0307315606623888\n",
      "Epoch 38, minibatch 13 of 16 -- Model loss: 0.014391395263373852\n",
      "Epoch 38, minibatch 14 of 16 -- Model loss: 0.006793431006371975\n",
      "Epoch 38, minibatch 15 of 16 -- Model loss: 0.08825631439685822\n",
      "Epoch 38, minibatch 16 of 16 -- Model loss: 0.14294013381004333\n",
      "Epoch 39, minibatch 1 of 16 -- Model loss: 0.0067648617550730705\n",
      "Epoch 39, minibatch 2 of 16 -- Model loss: 0.009466995485126972\n",
      "Epoch 39, minibatch 3 of 16 -- Model loss: 0.012468243017792702\n",
      "Epoch 39, minibatch 4 of 16 -- Model loss: 0.009865166619420052\n",
      "Epoch 39, minibatch 5 of 16 -- Model loss: 0.009033996611833572\n",
      "Epoch 39, minibatch 6 of 16 -- Model loss: 0.027016662061214447\n",
      "Epoch 39, minibatch 7 of 16 -- Model loss: 0.020698703825473785\n",
      "Epoch 39, minibatch 8 of 16 -- Model loss: 0.015059053897857666\n",
      "Epoch 39, minibatch 9 of 16 -- Model loss: 0.0075202519074082375\n",
      "Epoch 39, minibatch 10 of 16 -- Model loss: 0.026090240105986595\n",
      "Epoch 39, minibatch 11 of 16 -- Model loss: 0.010190028697252274\n",
      "Epoch 39, minibatch 12 of 16 -- Model loss: 0.025965705513954163\n",
      "Epoch 39, minibatch 13 of 16 -- Model loss: 0.010876234620809555\n",
      "Epoch 39, minibatch 14 of 16 -- Model loss: 0.007502292282879353\n",
      "Epoch 39, minibatch 15 of 16 -- Model loss: 0.08321086317300797\n",
      "Epoch 39, minibatch 16 of 16 -- Model loss: 0.1618817299604416\n",
      "Epoch 40, minibatch 1 of 16 -- Model loss: 0.009754772298038006\n",
      "Epoch 40, minibatch 2 of 16 -- Model loss: 0.011488363146781921\n",
      "Epoch 40, minibatch 3 of 16 -- Model loss: 0.015435498207807541\n",
      "Epoch 40, minibatch 4 of 16 -- Model loss: 0.00980728305876255\n",
      "Epoch 40, minibatch 5 of 16 -- Model loss: 0.014433403499424458\n",
      "Epoch 40, minibatch 6 of 16 -- Model loss: 0.026814837008714676\n",
      "Epoch 40, minibatch 7 of 16 -- Model loss: 0.030132988467812538\n",
      "Epoch 40, minibatch 8 of 16 -- Model loss: 0.012544925324618816\n",
      "Epoch 40, minibatch 9 of 16 -- Model loss: 0.009650196880102158\n",
      "Epoch 40, minibatch 10 of 16 -- Model loss: 0.021864764392375946\n",
      "Epoch 40, minibatch 11 of 16 -- Model loss: 0.010901533998548985\n",
      "Epoch 40, minibatch 12 of 16 -- Model loss: 0.027705559507012367\n",
      "Epoch 40, minibatch 13 of 16 -- Model loss: 0.003112164558842778\n",
      "Epoch 40, minibatch 14 of 16 -- Model loss: 0.004613346885889769\n",
      "Epoch 40, minibatch 15 of 16 -- Model loss: 0.08362309634685516\n",
      "Epoch 40, minibatch 16 of 16 -- Model loss: 0.15585562586784363\n",
      "Epoch 41, minibatch 1 of 16 -- Model loss: 0.011230076663196087\n",
      "Epoch 41, minibatch 2 of 16 -- Model loss: 0.008989498019218445\n",
      "Epoch 41, minibatch 3 of 16 -- Model loss: 0.007876737974584103\n",
      "Epoch 41, minibatch 4 of 16 -- Model loss: 0.006771476473659277\n",
      "Epoch 41, minibatch 5 of 16 -- Model loss: 0.011483819223940372\n",
      "Epoch 41, minibatch 6 of 16 -- Model loss: 0.022701693698763847\n",
      "Epoch 41, minibatch 7 of 16 -- Model loss: 0.03072553686797619\n",
      "Epoch 41, minibatch 8 of 16 -- Model loss: 0.009763450361788273\n",
      "Epoch 41, minibatch 9 of 16 -- Model loss: 0.009133623912930489\n",
      "Epoch 41, minibatch 10 of 16 -- Model loss: 0.025174427777528763\n",
      "Epoch 41, minibatch 11 of 16 -- Model loss: 0.022310098633170128\n",
      "Epoch 41, minibatch 12 of 16 -- Model loss: 0.026434388011693954\n",
      "Epoch 41, minibatch 13 of 16 -- Model loss: 0.008440334349870682\n",
      "Epoch 41, minibatch 14 of 16 -- Model loss: 0.009328066371381283\n",
      "Epoch 41, minibatch 15 of 16 -- Model loss: 0.05895237997174263\n",
      "Epoch 41, minibatch 16 of 16 -- Model loss: 0.15998034179210663\n",
      "Epoch 42, minibatch 1 of 16 -- Model loss: 0.006004465743899345\n",
      "Epoch 42, minibatch 2 of 16 -- Model loss: 0.007977215573191643\n",
      "Epoch 42, minibatch 3 of 16 -- Model loss: 0.008546613156795502\n",
      "Epoch 42, minibatch 4 of 16 -- Model loss: 0.005050953943282366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, minibatch 5 of 16 -- Model loss: 0.010180752724409103\n",
      "Epoch 42, minibatch 6 of 16 -- Model loss: 0.025949135422706604\n",
      "Epoch 42, minibatch 7 of 16 -- Model loss: 0.022371040657162666\n",
      "Epoch 42, minibatch 8 of 16 -- Model loss: 0.01038768608123064\n",
      "Epoch 42, minibatch 9 of 16 -- Model loss: 0.008074741810560226\n",
      "Epoch 42, minibatch 10 of 16 -- Model loss: 0.024922708049416542\n",
      "Epoch 42, minibatch 11 of 16 -- Model loss: 0.020311886444687843\n",
      "Epoch 42, minibatch 12 of 16 -- Model loss: 0.02622726559638977\n",
      "Epoch 42, minibatch 13 of 16 -- Model loss: 0.00511676911264658\n",
      "Epoch 42, minibatch 14 of 16 -- Model loss: 0.006365691777318716\n",
      "Epoch 42, minibatch 15 of 16 -- Model loss: 0.0547504797577858\n",
      "Epoch 42, minibatch 16 of 16 -- Model loss: 0.14305762946605682\n",
      "Epoch 43, minibatch 1 of 16 -- Model loss: 0.007153680548071861\n",
      "Epoch 43, minibatch 2 of 16 -- Model loss: 0.006681537721306086\n",
      "Epoch 43, minibatch 3 of 16 -- Model loss: 0.009851892478764057\n",
      "Epoch 43, minibatch 4 of 16 -- Model loss: 0.007112237624824047\n",
      "Epoch 43, minibatch 5 of 16 -- Model loss: 0.005600735079497099\n",
      "Epoch 43, minibatch 6 of 16 -- Model loss: 0.02271462231874466\n",
      "Epoch 43, minibatch 7 of 16 -- Model loss: 0.01427225861698389\n",
      "Epoch 43, minibatch 8 of 16 -- Model loss: 0.006728679407387972\n",
      "Epoch 43, minibatch 9 of 16 -- Model loss: 0.006292783189564943\n",
      "Epoch 43, minibatch 10 of 16 -- Model loss: 0.03371095284819603\n",
      "Epoch 43, minibatch 11 of 16 -- Model loss: 0.008784026838839054\n",
      "Epoch 43, minibatch 12 of 16 -- Model loss: 0.025699766352772713\n",
      "Epoch 43, minibatch 13 of 16 -- Model loss: 0.004718361888080835\n",
      "Epoch 43, minibatch 14 of 16 -- Model loss: 0.006330729927867651\n",
      "Epoch 43, minibatch 15 of 16 -- Model loss: 0.06105482205748558\n",
      "Epoch 43, minibatch 16 of 16 -- Model loss: 0.14044158160686493\n",
      "Epoch 44, minibatch 1 of 16 -- Model loss: 0.006096635013818741\n",
      "Epoch 44, minibatch 2 of 16 -- Model loss: 0.0086364122107625\n",
      "Epoch 44, minibatch 3 of 16 -- Model loss: 0.007923366501927376\n",
      "Epoch 44, minibatch 4 of 16 -- Model loss: 0.0051490627229213715\n",
      "Epoch 44, minibatch 5 of 16 -- Model loss: 0.009208887815475464\n",
      "Epoch 44, minibatch 6 of 16 -- Model loss: 0.025971010327339172\n",
      "Epoch 44, minibatch 7 of 16 -- Model loss: 0.023974142968654633\n",
      "Epoch 44, minibatch 8 of 16 -- Model loss: 0.007410199847072363\n",
      "Epoch 44, minibatch 9 of 16 -- Model loss: 0.009469754062592983\n",
      "Epoch 44, minibatch 10 of 16 -- Model loss: 0.02046755887567997\n",
      "Epoch 44, minibatch 11 of 16 -- Model loss: 0.013472729362547398\n",
      "Epoch 44, minibatch 12 of 16 -- Model loss: 0.02294347807765007\n",
      "Epoch 44, minibatch 13 of 16 -- Model loss: 0.006660765502601862\n",
      "Epoch 44, minibatch 14 of 16 -- Model loss: 0.0030071616638451815\n",
      "Epoch 44, minibatch 15 of 16 -- Model loss: 0.058745045214891434\n",
      "Epoch 44, minibatch 16 of 16 -- Model loss: 0.1382262110710144\n",
      "Epoch 45, minibatch 1 of 16 -- Model loss: 0.005523365456610918\n",
      "Epoch 45, minibatch 2 of 16 -- Model loss: 0.007921705953776836\n",
      "Epoch 45, minibatch 3 of 16 -- Model loss: 0.006201190408319235\n",
      "Epoch 45, minibatch 4 of 16 -- Model loss: 0.005642560310661793\n",
      "Epoch 45, minibatch 5 of 16 -- Model loss: 0.005766856949776411\n",
      "Epoch 45, minibatch 6 of 16 -- Model loss: 0.021243857219815254\n",
      "Epoch 45, minibatch 7 of 16 -- Model loss: 0.010323788039386272\n",
      "Epoch 45, minibatch 8 of 16 -- Model loss: 0.0068917605094611645\n",
      "Epoch 45, minibatch 9 of 16 -- Model loss: 0.005364410113543272\n",
      "Epoch 45, minibatch 10 of 16 -- Model loss: 0.024997057393193245\n",
      "Epoch 45, minibatch 11 of 16 -- Model loss: 0.009397377260029316\n",
      "Epoch 45, minibatch 12 of 16 -- Model loss: 0.02161143533885479\n",
      "Epoch 45, minibatch 13 of 16 -- Model loss: 0.00438751932233572\n",
      "Epoch 45, minibatch 14 of 16 -- Model loss: 0.0041634635999798775\n",
      "Epoch 45, minibatch 15 of 16 -- Model loss: 0.05405808985233307\n",
      "Epoch 45, minibatch 16 of 16 -- Model loss: 0.13595335185527802\n",
      "Epoch 46, minibatch 1 of 16 -- Model loss: 0.009843297302722931\n",
      "Epoch 46, minibatch 2 of 16 -- Model loss: 0.006496144458651543\n",
      "Epoch 46, minibatch 3 of 16 -- Model loss: 0.0081801638007164\n",
      "Epoch 46, minibatch 4 of 16 -- Model loss: 0.002988050691783428\n",
      "Epoch 46, minibatch 5 of 16 -- Model loss: 0.010305379517376423\n",
      "Epoch 46, minibatch 6 of 16 -- Model loss: 0.02360597252845764\n",
      "Epoch 46, minibatch 7 of 16 -- Model loss: 0.021975554525852203\n",
      "Epoch 46, minibatch 8 of 16 -- Model loss: 0.0042239404283463955\n",
      "Epoch 46, minibatch 9 of 16 -- Model loss: 0.007388588506728411\n",
      "Epoch 46, minibatch 10 of 16 -- Model loss: 0.019637491554021835\n",
      "Epoch 46, minibatch 11 of 16 -- Model loss: 0.007706047967076302\n",
      "Epoch 46, minibatch 12 of 16 -- Model loss: 0.021807510405778885\n",
      "Epoch 46, minibatch 13 of 16 -- Model loss: 0.005508389323949814\n",
      "Epoch 46, minibatch 14 of 16 -- Model loss: 0.002781120827421546\n",
      "Epoch 46, minibatch 15 of 16 -- Model loss: 0.0544988214969635\n",
      "Epoch 46, minibatch 16 of 16 -- Model loss: 0.13351626694202423\n",
      "Epoch 47, minibatch 1 of 16 -- Model loss: 0.007928299717605114\n",
      "Epoch 47, minibatch 2 of 16 -- Model loss: 0.0038685386534780264\n",
      "Epoch 47, minibatch 3 of 16 -- Model loss: 0.004517487715929747\n",
      "Epoch 47, minibatch 4 of 16 -- Model loss: 0.003188583068549633\n",
      "Epoch 47, minibatch 5 of 16 -- Model loss: 0.0038339190650731325\n",
      "Epoch 47, minibatch 6 of 16 -- Model loss: 0.018246958032250404\n",
      "Epoch 47, minibatch 7 of 16 -- Model loss: 0.017264846712350845\n",
      "Epoch 47, minibatch 8 of 16 -- Model loss: 0.0039533996023237705\n",
      "Epoch 47, minibatch 9 of 16 -- Model loss: 0.004601642955094576\n",
      "Epoch 47, minibatch 10 of 16 -- Model loss: 0.017458463087677956\n",
      "Epoch 47, minibatch 11 of 16 -- Model loss: 0.00784214399755001\n",
      "Epoch 47, minibatch 12 of 16 -- Model loss: 0.021038470789790154\n",
      "Epoch 47, minibatch 13 of 16 -- Model loss: 0.0028588452842086554\n",
      "Epoch 47, minibatch 14 of 16 -- Model loss: 0.004235574044287205\n",
      "Epoch 47, minibatch 15 of 16 -- Model loss: 0.053125523030757904\n",
      "Epoch 47, minibatch 16 of 16 -- Model loss: 0.13240410387516022\n",
      "Epoch 48, minibatch 1 of 16 -- Model loss: 0.0026034917682409286\n",
      "Epoch 48, minibatch 2 of 16 -- Model loss: 0.007710609585046768\n",
      "Epoch 48, minibatch 3 of 16 -- Model loss: 0.005598704796284437\n",
      "Epoch 48, minibatch 4 of 16 -- Model loss: 0.0024421322159469128\n",
      "Epoch 48, minibatch 5 of 16 -- Model loss: 0.002975837327539921\n",
      "Epoch 48, minibatch 6 of 16 -- Model loss: 0.020068123936653137\n",
      "Epoch 48, minibatch 7 of 16 -- Model loss: 0.002727653831243515\n",
      "Epoch 48, minibatch 8 of 16 -- Model loss: 0.007131882943212986\n",
      "Epoch 48, minibatch 9 of 16 -- Model loss: 0.004041595850139856\n",
      "Epoch 48, minibatch 10 of 16 -- Model loss: 0.019852692261338234\n",
      "Epoch 48, minibatch 11 of 16 -- Model loss: 0.005391236394643784\n",
      "Epoch 48, minibatch 12 of 16 -- Model loss: 0.023074526339769363\n",
      "Epoch 48, minibatch 13 of 16 -- Model loss: 0.0027074250392615795\n",
      "Epoch 48, minibatch 14 of 16 -- Model loss: 0.0036995282862335443\n",
      "Epoch 48, minibatch 15 of 16 -- Model loss: 0.0519818477332592\n",
      "Epoch 48, minibatch 16 of 16 -- Model loss: 0.12933123111724854\n",
      "Epoch 49, minibatch 1 of 16 -- Model loss: 0.004919035825878382\n",
      "Epoch 49, minibatch 2 of 16 -- Model loss: 0.003972959704697132\n",
      "Epoch 49, minibatch 3 of 16 -- Model loss: 0.004403690807521343\n",
      "Epoch 49, minibatch 4 of 16 -- Model loss: 0.0027094651013612747\n",
      "Epoch 49, minibatch 5 of 16 -- Model loss: 0.0027519005816429853\n",
      "Epoch 49, minibatch 6 of 16 -- Model loss: 0.01937679946422577\n",
      "Epoch 49, minibatch 7 of 16 -- Model loss: 0.0033963352907449007\n",
      "Epoch 49, minibatch 8 of 16 -- Model loss: 0.004026397131383419\n",
      "Epoch 49, minibatch 9 of 16 -- Model loss: 0.0032095930073410273\n",
      "Epoch 49, minibatch 10 of 16 -- Model loss: 0.017765242606401443\n",
      "Epoch 49, minibatch 11 of 16 -- Model loss: 0.0036683811340481043\n",
      "Epoch 49, minibatch 12 of 16 -- Model loss: 0.02151934616267681\n",
      "Epoch 49, minibatch 13 of 16 -- Model loss: 0.002590799704194069\n",
      "Epoch 49, minibatch 14 of 16 -- Model loss: 0.0018008674960583448\n",
      "Epoch 49, minibatch 15 of 16 -- Model loss: 0.050384312868118286\n",
      "Epoch 49, minibatch 16 of 16 -- Model loss: 0.13304995000362396\n",
      "Epoch 50, minibatch 1 of 16 -- Model loss: 0.0028170584701001644\n",
      "Epoch 50, minibatch 2 of 16 -- Model loss: 0.0055617718026041985\n",
      "Epoch 50, minibatch 3 of 16 -- Model loss: 0.0035758723970502615\n",
      "Epoch 50, minibatch 4 of 16 -- Model loss: 0.003551485948264599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, minibatch 5 of 16 -- Model loss: 0.001579396310262382\n",
      "Epoch 50, minibatch 6 of 16 -- Model loss: 0.0202022772282362\n",
      "Epoch 50, minibatch 7 of 16 -- Model loss: 0.003990373574197292\n",
      "Epoch 50, minibatch 8 of 16 -- Model loss: 0.0033045730087906122\n",
      "Epoch 50, minibatch 9 of 16 -- Model loss: 0.0037733090575784445\n",
      "Epoch 50, minibatch 10 of 16 -- Model loss: 0.01773349568247795\n",
      "Epoch 50, minibatch 11 of 16 -- Model loss: 0.005761397536844015\n",
      "Epoch 50, minibatch 12 of 16 -- Model loss: 0.022831402719020844\n",
      "Epoch 50, minibatch 13 of 16 -- Model loss: 0.004126668442040682\n",
      "Epoch 50, minibatch 14 of 16 -- Model loss: 0.0017098068492487073\n",
      "Epoch 50, minibatch 15 of 16 -- Model loss: 0.051737524569034576\n",
      "Epoch 50, minibatch 16 of 16 -- Model loss: 0.13132378458976746\n",
      "Saved RNN net.\n",
      "Saved dense net.\n"
     ]
    }
   ],
   "source": [
    "# training the models\n",
    "# ------------------\n",
    "\n",
    "''' USE -1 AS EPOCHS TO LOAD SAVED MODEL WITHOUT TRAINING '''\n",
    "\n",
    "# model_rnn_train(xin,yin,xval,yval,load_mode,model_mode,model_dict,epochs,mbsize,loss_mode,use_cuda,save_state,path)\n",
    "\n",
    "cn_file_name = 'model_rnndense_sentiment.tar'\n",
    "cn_save_path = save_path + cn_file_name\n",
    "print(cn_save_path)\n",
    "\n",
    "\n",
    "md = model_rnn_train(xin,h_0,yin,None,None,None,'new','rnn_dense',training_models_dict,50,64,'MSE',use_cuda,True,cn_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 predicting user input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter sentences below:\n",
    "#######################\n",
    "\n",
    "user_inlist = [\n",
    "    \n",
    "    'we would rather have stayed home',\n",
    "    'honestly, dont go',\n",
    "    'this place is amazing'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. preparing user input..\n",
      "2. Setting up datasets..\n",
      "1. Itering through in dict..\n",
      "2. Padding list..\n",
      "##\n",
      "torch.Size([6, 3, 300])\n",
      "torch.FloatTensor\n",
      "torch.Size([3, 1])\n",
      "torch.FloatTensor\n",
      "torch.Size([1, 3, 32])\n"
     ]
    }
   ],
   "source": [
    "# preparing user input dataset\n",
    "# ----------------------------\n",
    "\n",
    "xu,hu = prepare_use_input(user_inlist, num_layers, num_directions, hidden_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1042.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inference ops\n",
    "# -------------\n",
    "\n",
    "pred = inference(xu,hu,'rnn_dense',md,use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: we would rather have stayed home\n",
      "Prediction: 1.41 % positive.\n",
      "***\n",
      "\n",
      "Text: honestly, dont go\n",
      "Prediction: 1.57 % positive.\n",
      "***\n",
      "\n",
      "Text: this place is amazing\n",
      "Prediction: 56.19 % positive.\n",
      "***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# friendly display\n",
    "# ----------------\n",
    "\n",
    "for i in range(len(user_inlist)):\n",
    "    print('Text: ' + user_inlist[i])\n",
    "    print('Prediction: ' + str(round(pred[i,0]*100,2)) + ' % positive.')\n",
    "    print('***\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#############################################################################\n",
    "## DO NOT DELETE - Interdependency checking -- capturing joint probability ##\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "# dataset preparation\n",
    "# -------------------\n",
    "\n",
    "\n",
    "#x = np.array(([1,1,0],[0,1,1],[1,0,1]))\n",
    "#y = np.array(([1,0,0],[0,0,1],[0,1,0]))\n",
    "\n",
    "#x = np.array(([0.48,0.21],[0.19,0.52],[0.51,0.22],[0.19,0.50],[0.49,0.23],[0.21,0.49]))\n",
    "#y = np.array(([1,0],[0,1],[1,0],[0,1],[1,0],[0,1]))\n",
    "\n",
    "#x = np.array(([0.48,0.21],[0.19,0.52],[0.51,0.22],[0.19,0.50],[0.49,0.23],[0.21,0.49]))\n",
    "#y = np.array(([1],[0],[1],[0],[1],[0]))\n",
    "\n",
    "x = np.array(([0.3,0.3,0.6],[0.6,0.6,0.6],[0.6,0.6,0.3],[0.3,0.3,0.3]))\n",
    "y = np.array(([1],[0],[1],[0]))\n",
    "\n",
    "\n",
    "#x = np.array(([1,1,0],[0,1,1]))\n",
    "#y = np.array(([1,0],[0,1]))\n",
    "\n",
    "xt = Variable(torch.from_numpy(x)).float()\n",
    "yt = Variable(torch.from_numpy(y)).float()\n",
    "\n",
    "x.shape, y.shape, xt.size(), yt.size()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# model set up\n",
    "# ------------\n",
    "\n",
    "target_main_model_act = 'sigmoid'\n",
    "dense_layers = [3,4,1]\n",
    "model = linear_fc(dense_layers, 'relu', target_main_model_act, 0.0)\n",
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# simple model training loop\n",
    "# --------------------------\n",
    "\n",
    "epoch = 2500\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    mout = model(xt)\n",
    "    loss = criterion(mout,yt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Loss at epoch ' + str(i) + ' is ' + str(loss.item()))\n",
    " \n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.forwardpass[0].weight"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.forwardpass[0:4](xt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.forwardpass[4].weight"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.forwardpass[4:6](model.forwardpass[0:4](xt))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmate2_env",
   "language": "python",
   "name": "pmate2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
