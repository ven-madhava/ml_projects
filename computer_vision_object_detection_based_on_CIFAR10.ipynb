{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One shot multiple object detection using deep FCN and CIFAR 10 \n",
    "--------------------------------------------------------------\n",
    "\n",
    "1. We will use channel wise object prediction - train a dedicated channel for each object\n",
    "2. We will try to predict objects of arbitrary size by expanding weights of the first convolutional layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# -------\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import csv\n",
    "import cv2\n",
    "import h5py\n",
    "import time\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from scipy.ndimage import label\n",
    "from skimage import measure\n",
    "from scipy import interpolate\n",
    "import json\n",
    "import pickle\n",
    "from skimage.transform import resize\n",
    "\n",
    "# torch related imports\n",
    "# ---------------------\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "\n",
    "# local settings\n",
    "# --------------\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    \n",
    "# storage related imports\n",
    "# -----------------------\n",
    "import tempfile\n",
    "from tempfile import TemporaryFile\n",
    "from google.cloud import storage\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/home/venkateshmadhava/datasets/ven-ml-project-387fdf3f596f.json'\n",
    "\n",
    "%matplotlib inline\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check accuracy on regression model\n",
    "# ----------------------------------------------\n",
    "\n",
    "def check_accuracy(x_in,y_in,model,mode):\n",
    "    \n",
    "    \n",
    "    # 0. sanity stuff\n",
    "    # ---------------\n",
    "    model = model.eval()\n",
    "    m = x_in.shape[0]\n",
    "    no_classes = y_in.shape[3]\n",
    "    x_intrch = setup_image_tensor(x_in)\n",
    "    \n",
    "    # 0. predicting output\n",
    "    # --------------------\n",
    "    out = chunk_pass(x_intrch,model,False,use_cuda,1)\n",
    "    out = out.view(out.size()[0],-1).cpu().data.numpy()\n",
    "    \n",
    "    # 1. accuracy ops\n",
    "    # ---------------\n",
    "    if mode == 'sigmoid':\n",
    "        out[out >= 0.5] = 1\n",
    "        out[out < 0.5] = 0\n",
    "    else:\n",
    "        out = softmax_to_onehot(out)\n",
    "    \n",
    "    sum_sim = np.sum((out == y_in.reshape(m,no_classes)), axis = 1)\n",
    "    total_correct = np.sum((sum_sim == no_classes).astype('int'))  \n",
    "    print('Total correct is : ' + str(total_correct) + ' (' + str(round((total_correct/m)*100,2))  + '%)')\n",
    "\n",
    "\n",
    "    \n",
    "#### simple function to convert softmax to one hot\n",
    "# -------------------------------------------------\n",
    "def softmax_to_onehot(p):\n",
    "    \n",
    "    # 0. converting p to numpy\n",
    "    # ------------------------\n",
    "    xout = np.zeros(p.shape)\n",
    "    \n",
    "    # 1. unfortuately havint to loop through\n",
    "    # --------------------------------------\n",
    "    inds = p.argmax(axis = 1)\n",
    "    for i in range(inds.shape[0]):\n",
    "        xout[i,inds[i]] = 1\n",
    "    \n",
    "    # 2. setting output to pytorch\n",
    "    # ----------------------------\n",
    "    return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting file from google cloud storage\n",
    "# ---------------------------------------\n",
    "\n",
    "def get_file_from_google_storage(file_name):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1. takes an input google cloud storage file name\n",
    "    2. downloads to temp file\n",
    "    3. returns local temp file path\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 0. initialising bucket\n",
    "    # ----------------------\n",
    "    print('0. initialising bucket..')\n",
    "    bucket_name = 'gpu_datatset_bucket'\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    \n",
    "    # 1. retrieveing blob\n",
    "    # -------------------\n",
    "    print('1. retrieving blob..')\n",
    "    blob = bucket.blob(file_name)\n",
    "    \n",
    "    # 2. downloading blob to temp file\n",
    "    # --------------------------------\n",
    "    print('2. downloading blob to temp file, this may take a while..')\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as gcs_tempfile:\n",
    "        blob.download_to_filename(gcs_tempfile.name)\n",
    "        \n",
    "    # 3. final return\n",
    "    # ---------------\n",
    "    print('Done.')\n",
    "    return gcs_tempfile.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to take in a folder and save images in a dict\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_dataset_from_folder_to_dict(infolder):\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    d = {}\n",
    "    image_list = [f for f in listdir(infolder) if isfile(join(infolder, f)) and '.jpg' in f.lower()]\n",
    "    assert len(image_list) > 0, 'No images found in the folder'\n",
    "    assert len(image_list) <= 100, 'More than 100 images found. Cannot proceed further using for loop.'\n",
    "    \n",
    "    # 1. simple iteration\n",
    "    # -------------------\n",
    "    for i in range(len(image_list)):\n",
    "        print('At image ' + str(i+1) + ' of ' + str(len(image_list)) + ' images..', end = '\\r')\n",
    "        img = cv2.imread(join(infolder, image_list[i]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        d[i] = img\n",
    "    \n",
    "    # 2. final return\n",
    "    # ---------------\n",
    "    print('\\ndone.')\n",
    "    return d\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pool function to create a dataset from input folder\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def create_dataset_from_folder_all(infolder,n_h,n_w):\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    image_list = [f for f in listdir(infolder) if isfile(join(infolder, f)) and '.jpg' in f.lower()]\n",
    "    assert len(image_list) > 0, 'No images found in the folder'\n",
    "    xout = np.zeros((len(image_list),n_h,n_w,3), dtype='uint8')\n",
    "    \n",
    "    # 1. building args\n",
    "    # ----------------\n",
    "    all_args = []\n",
    "    for i in range(xout.shape[0]):\n",
    "        all_args.append((i,xout,infolder,image_list,n_h,n_w))\n",
    "    \n",
    "    # 2. calling resize function across multiprocessing pool\n",
    "    # ------------------------------------------------------\n",
    "    pool = ThreadPool(5)\n",
    "    pool.starmap(create_dataset_from_folder_single, all_args)\n",
    "    print('Done creating a dataset with ' + str(xout.shape[0]) + ' images.')\n",
    "    \n",
    "    return xout\n",
    "\n",
    "    \n",
    "# FUNCTION 2\n",
    "# GENERIC FUNCTION - to resize a single image\n",
    "# ------------------------------------------\n",
    "def create_dataset_from_folder_single(i,xout,infolder,image_list,n_h,n_w):\n",
    "    \n",
    "    # snippet\n",
    "    # -------\n",
    "    name = image_list[i]\n",
    "    img = cv2.imread(join(infolder, name))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (n_w,n_h))\n",
    "    xout[i] = img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pool function to resize images\n",
    "# ------------------------------\n",
    "\n",
    "def resize_all(ximgs,new_h,new_w,nearest_interpolate=False):\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    c = ximgs.shape[3]\n",
    "    x_images_resized = np.zeros((ximgs.shape[0],new_h,new_w,c), dtype='uint8')\n",
    "    \n",
    "    # 1. building args\n",
    "    # ----------------\n",
    "    all_args = []\n",
    "    for i in range(ximgs.shape[0]):\n",
    "        all_args.append((i,ximgs[i],x_images_resized,new_h,new_w,nearest_interpolate))\n",
    "    \n",
    "    # 2. calling resize function across multiprocessing pool\n",
    "    # ------------------------------------------------------\n",
    "    pool = ThreadPool(5)\n",
    "    pool.starmap(resize_image_single, all_args)\n",
    "    print('Done resizing ' + str(ximgs.shape[0]) + ' images.')\n",
    "    \n",
    "    return x_images_resized\n",
    "\n",
    "    \n",
    "# FUNCTION 2\n",
    "# GENERIC FUNCTION - to resize a single image\n",
    "# ------------------------------------------\n",
    "def resize_image_single(i,x_in,x_out,new_h,new_w,nearest_interpolate_mode):\n",
    "    \n",
    "    # simple code\n",
    "    # -----------\n",
    "    img = x_in\n",
    "    if nearest_interpolate_mode == True:\n",
    "        img = cv2.resize(img, (new_w,new_h), interpolation=cv2.INTER_NEAREST)\n",
    "    else:\n",
    "        img = cv2.resize(img, (new_w,new_h))\n",
    "    x_out[i] = img\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE FUNTION TO CONVERT RGB TO GRAYSCALE\n",
    "# -------------------------------------------\n",
    "def rgb2gray(x):\n",
    "    \n",
    "\n",
    "    x[:,:,:,0] = x[:,:,:,0] * 0.2989\n",
    "    x[:,:,:,1] = x[:,:,:,0] * 0.5870\n",
    "    x[:,:,:,2] = x[:,:,:,0] * 0.1140\n",
    "    xout = np.sum(x,axis = 3)\n",
    "    \n",
    "\n",
    "    #r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    #gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpicle cifar 10\n",
    "# ----------------\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple snippet to find out the distribution of class values\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def return_class_distribution(yin):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1. returns a simple dict with number of examples in each class in y\n",
    "    2. input must be of shape (m,c)\n",
    "    3. returns d[class_number] = num_examples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    m,c = yin.shape\n",
    "    d = {}\n",
    "    \n",
    "    # 1. looping through classes\n",
    "    # --------------------------\n",
    "    for curr_c in range(c):\n",
    "        d[curr_c] = np.argwhere(yin[:,curr_c] == 1).shape[0]\n",
    "    \n",
    "    # 2. printing results\n",
    "    # -------------------\n",
    "    for keys in d:\n",
    "        print('class ' + str(keys) + ': ' + str(d[keys]))\n",
    "    \n",
    "    # 3. final return\n",
    "    # ---------------\n",
    "    return d\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to load cifar 10 batches into numpy arrays, threaded function\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_cifar10(b_url):\n",
    "    \n",
    "    ''' loads a batch of cifar 10 images, returns x,y '''\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    global b_dict\n",
    "    b_dict = unpickle(b_url)\n",
    "    m = b_dict[b'data'].shape[0]\n",
    "    no_classes = 10\n",
    "    h,w,c = 32,32,3\n",
    "    \n",
    "    # 0.1 some global initialisations as well\n",
    "    # ---------------------------------------\n",
    "    global x\n",
    "    x = np.zeros((m,h,w,c), dtype = 'uint8')\n",
    "    global y\n",
    "    y = np.zeros((m,no_classes))\n",
    "    \n",
    "    # 1. threaded function\n",
    "    # --------------------\n",
    "    pool = ThreadPool(5)\n",
    "    pool.map(load_cifar10_single, list(range(m)))\n",
    "    print('Done with ' + str(m) + ' images. Access at global x and global y.')\n",
    "    \n",
    "    \n",
    "    \n",
    "# cifar 10 single function\n",
    "# ------------------------\n",
    "def load_cifar10_single(i):\n",
    "    \n",
    "    \n",
    "    # 0. getting in global variables\n",
    "    # ------------------------------\n",
    "    global b_dict\n",
    "    global x\n",
    "    global y\n",
    "    \n",
    "    # 1. setting indices\n",
    "    # ------------------\n",
    "    len_r = 1024\n",
    "    r_end = len_r\n",
    "    g_str = r_end\n",
    "    g_end = g_str + len_r\n",
    "    b_str = g_end\n",
    "    b_end = b_str + len_r\n",
    "    r_end,g_str,g_end,b_str,b_end\n",
    "    \n",
    "    # 2. setting channel values\n",
    "    # -------------------------\n",
    "    img_row = b_dict[b'data'][i]\n",
    "    r_channel = img_row[0:r_end].reshape(32,32,1)\n",
    "    g_channel = img_row[g_str:g_end].reshape(32,32,1)\n",
    "    b_channel = img_row[b_str:b_end].reshape(32,32,1)\n",
    "    im = np.concatenate((r_channel,g_channel,b_channel), axis = 2)\n",
    "    \n",
    "    # 3. final ops\n",
    "    # ------------\n",
    "    x[i] = im\n",
    "    y[i,b_dict[b'labels'][i]] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN related code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple function to generate output from latent\n",
    "# ------------------------------------------------\n",
    "\n",
    "def generate_output(xin,model,start_ind,end_ind,print_images,use_cuda):\n",
    "    \n",
    "    \n",
    "    # 1. generating output\n",
    "    # --------------------\n",
    "    xout = chunk_pass(xin[start_ind:end_ind],model.eval(),False,use_cuda,1)\n",
    "    \n",
    "    # images dataset\n",
    "    # --------------\n",
    "    xout_gen = to_numpy_image(xout.cpu().data)#.astype('uint8')\n",
    "    xout_orig = to_numpy_image(xin[start_ind:end_ind].cpu().data).astype('uint8')\n",
    "        \n",
    " \n",
    "    # 4. priniting images\n",
    "    # -------------------\n",
    "    if print_images == True:\n",
    "        for i in range(xout_orig.shape[0]):\n",
    "            print('Example ' + str(i) + '..')\n",
    "            print('----------------------')\n",
    "            print('Original - ')\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(xout_orig[i])\n",
    "            plt.show()\n",
    "            print('Generated - ')\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(xout_gen[i])\n",
    "            plt.show()\n",
    "            print('\\n----------------\\n')\n",
    "    \n",
    "    # returns\n",
    "    # -------\n",
    "    return xout_orig, xout_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super helpful chunker function that returns seq chunks correctly sized even at ends\n",
    "# -----------------------------------------------------------------------------------\n",
    "# GENERIC function to calculate conv outsize\n",
    "# ------------------------------------------\n",
    "def outsize_conv(n_H,n_W,f,s,pad):\n",
    "    \n",
    "    h = ((n_H - f + (2*pad))/s) + 1\n",
    "    w = ((n_W - f + (2*pad))/s) + 1\n",
    "    return h,w\n",
    "    \n",
    "    \n",
    "# GENERIC function to calculate upconv outsize\n",
    "# --------------------------------------------    \n",
    "def outsize_upconv(h,w,f,s,p):\n",
    "    hout = (h-1)*s - 2*p + f\n",
    "    wout = (w-1)*s - 2*p + f\n",
    "    return hout, wout\n",
    "\n",
    "\n",
    "\n",
    "def chunker(seq, size):\n",
    "    \n",
    "    # from http://stackoverflow.com/a/434328\n",
    "    # not touch this code\n",
    "    # -------------------\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "\n",
    "\n",
    "# GENERIC - initialises weights for a NN\n",
    "# --------------------------------------\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "    #    print(classname)\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    #elif classname.find('Linear') != -1:\n",
    "    #    print(classname)\n",
    "    #    m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "        \n",
    "# GENERIC - change an torch image to numoy image\n",
    "# ----------------------------------------------\n",
    "def to_numpy_image(xin):\n",
    "    \n",
    "    try:\n",
    "        xin = xin.data.numpy()\n",
    "    except:\n",
    "        xin = xin.numpy()\n",
    "    \n",
    "    \n",
    "    xout = np.swapaxes(xin,1,2)\n",
    "    xout = np.swapaxes(xout,2,3)\n",
    "    \n",
    "    # returns axes swapped numpy images\n",
    "    # ---------------------------------\n",
    "    return xout       \n",
    "\n",
    "\n",
    "\n",
    "# GENERIC - converts numpy images to torch tensors for training\n",
    "# -------------------------------------------------------------\n",
    "def setup_image_tensor(xin):\n",
    "    xout = np.swapaxes(xin,1,3)\n",
    "    xout = np.swapaxes(xout,2,3)\n",
    "    \n",
    "    # returns axes swapped torch tensor\n",
    "    # ---------------------------------\n",
    "    xout = torch.from_numpy(xout)\n",
    "    return xout.float()\n",
    "\n",
    "\n",
    "# A functino to get linemarkings\n",
    "# -------------------------------\n",
    "\n",
    "def get_ae_output_image(x,net,use_cuda):\n",
    "    \n",
    "    # 0. Setting up input as torch\n",
    "    # ----------------------------\n",
    "    x_t = Variable(setup_image_tensor(x)).float()\n",
    "        \n",
    "        \n",
    "    # 1. Using chunk pass to get linemarkings\n",
    "    # ----------------------------------------\n",
    "    xout = chunk_pass(x_t,net.eval(),False,use_cuda,1)\n",
    "    xout = to_numpy_image(xout.cpu().data)\n",
    "    xout = (xout * 255).astype('uint8')\n",
    "    \n",
    "    # 2. final return\n",
    "    # ---------------\n",
    "    return xout\n",
    "    \n",
    "    \n",
    "# GENERIC class that inherits nn module and makes a sequential object a model\n",
    "# ---------------------------------------------------------------------------\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,sequencelist):\n",
    "        super().__init__() # Initializing nn.Module construtors\n",
    "        self.forwardpass = sequencelist\n",
    "        \n",
    "    def forward(self,x):\n",
    "        xout = self.forwardpass(x)\n",
    "        return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a lineaf FC model\n",
    "# -----------------------------------\n",
    "\n",
    "def linear_fc(layers, nw_activations, target_activation, dropout_p):\n",
    "    \n",
    "    'The first value in the layers list is the input dimensions of the input'\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    seq_list = []\n",
    "    \n",
    "    # setting N/W activations\n",
    "    # -------------------\n",
    "    if nw_activations == 'relu':\n",
    "        nw_act = nn.ReLU()\n",
    "    elif nw_activations == 'lrelu':\n",
    "        nw_act = nn.LeakyReLU(0.2)\n",
    "    elif nw_activations == 'sigmoid':\n",
    "        nw_act = nn.Sigmoid()\n",
    "    elif nw_activations == 'tanh':\n",
    "        nw_act = nn.Tanh()\n",
    "    else:\n",
    "        nw_act = nn.ReLU()\n",
    "    \n",
    "    # setting target activations\n",
    "    # --------------------------\n",
    "    if target_activation == 'sigmoid':\n",
    "        target_act = nn.Sigmoid()\n",
    "    elif target_activation == 'softmax':\n",
    "        target_act = nn.Softmax()\n",
    "    else:\n",
    "        target_act = None\n",
    "    \n",
    "    # 1. building n/w's layer list\n",
    "    # ----------------------------\n",
    "    network = []\n",
    "    for k in range(len(layers)):\n",
    "        try:\n",
    "            network.append((layers[k],layers[k+1]))\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "    \n",
    "    # 2. constructing encoder n/w\n",
    "    # ----------------------------\n",
    "    for i in range(len(network)):\n",
    "        \n",
    "        # 2.1 adding linear layers to encoder\n",
    "        # ------------------------------------\n",
    "        curr_dims = network[i]\n",
    "        seq_mod = nn.Linear(curr_dims[0],curr_dims[1])\n",
    "        seq_list.append(seq_mod)\n",
    "        \n",
    "        # checking last layer or not\n",
    "        # --------------------------\n",
    "        if i+1 == len(network):\n",
    "            \n",
    "            # at last layer\n",
    "            # -------------\n",
    "            if target_act == None:\n",
    "                pass\n",
    "            else:\n",
    "                seq_list.append(target_act)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # batchnorm\n",
    "            # ---------\n",
    "            seq_list.append(nn.BatchNorm1d(curr_dims[1]))\n",
    "          \n",
    "            # non linear activation\n",
    "            # ---------------------\n",
    "            seq_list.append(nw_act)\n",
    "            \n",
    "            # dropout\n",
    "            # -------\n",
    "            seq_list.append(nn.Dropout(p = dropout_p))\n",
    "           \n",
    "            \n",
    "    \n",
    "    # 3. returning model\n",
    "    # ------------------\n",
    "    seq_list = nn.Sequential(*seq_list)\n",
    "    seq_list.apply(weights_init)\n",
    "\n",
    "    model = Net(seq_list)\n",
    "    model = model.train()\n",
    "    \n",
    "    return model\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERIC model function to train the networks\n",
    "# --------------------------------------------\n",
    "\n",
    "def model_train(xin,yin,xval,yval,load_mode,model,epochs,mbsize,loss_mode,use_cuda,save_state,path):\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    norm_flag = 0\n",
    "\n",
    "    \n",
    "\n",
    "    # normalising input to 0-1 while making sure this is an image\n",
    "    # -----------------------------------------------------------\n",
    "    if len(xin.size()) > 3:\n",
    "            \n",
    "        # if the input and output are images - try will go through with the statement\n",
    "        # ----------------------------------------------------------------------------\n",
    "        if len(xin.size()) > 3 and len(yin.size()) > 3:\n",
    "\n",
    "            assert torch.mean(xin).item() > 1 and torch.mean(yin).item() > 1, 'Input data is already in range 0-1. Not consistent with flow.'\n",
    "            \n",
    "\n",
    "            # both need to be normalised\n",
    "            # --------------------------\n",
    "            #xin = xin/255\n",
    "            #yin = yin/255\n",
    "            norm_flag = 1\n",
    "            print('Input and Output dataset will be normalised to 0-1')\n",
    "\n",
    "        \n",
    "        # incase input and output both are NOT images\n",
    "        # -------------------------------------------\n",
    "        else:\n",
    "\n",
    "            assert torch.mean(xin).item() > 1, 'Input data is already in range 0-1. Not consistent with flow.'\n",
    "\n",
    "            # normalising input\n",
    "            # -----------------\n",
    "            #xin = xin/255\n",
    "            norm_flag = 2\n",
    "            print('Input dataset will be normalised to 0-1')\n",
    "            \n",
    "            \n",
    "    \n",
    "    # ensuring xval and yval are None\n",
    "    # -------------------------------\n",
    "    assert xval == None and yval == None, 'xval and yval provided, but there is no code to normalise'\n",
    "    \n",
    "    \n",
    "    if load_mode == 'from saved':\n",
    "        \n",
    "        # loading from saved\n",
    "        # ------------------\n",
    "        model,optimizer,saved_epoch,saved_loss,saved_loss_mode = load_saved_model_function(path,use_cuda)\n",
    "        model = model.train()\n",
    "        loss_mode = saved_loss_mode\n",
    "        print('Loading model from saved state...')\n",
    "        print('Last saved loss - ' + str(saved_loss))\n",
    "        print('Last saved epoch - ' + str(saved_epoch))\n",
    "        epochs += int(saved_epoch)\n",
    "        start_epoch = int(saved_epoch)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # building new\n",
    "        # ------------\n",
    "        start_epoch = 1\n",
    "        model = model.train()\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()))\n",
    "        #optimizer = torch.optim.Adadelta(filter(lambda p: p.requires_grad,model.parameters()))\n",
    "        #optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad,model.parameters()))\n",
    "        #optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad,model.parameters()), lr=0.1, momentum=0.9)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # model set up as per cuda\n",
    "    # ------------------------\n",
    "    if use_cuda == True:\n",
    "        torch.cuda.empty_cache()\n",
    "        model = model.cuda()        \n",
    "    \n",
    "    \n",
    "    # setting loss criterion\n",
    "    # ----------------------\n",
    "    if loss_mode == 'MSE':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_mode == 'BCE':\n",
    "        criterion = nn.BCELoss()\n",
    "    elif loss_mode == 'NLL':\n",
    "        criterion = nn.NLLLoss()\n",
    "    elif loss_mode == 'crossentropy':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        yin = torch.max(yin.long(),1)[1]\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "    \n",
    "    # 1. Setting up minibatch features\n",
    "    # --------------------------------\n",
    "    m = xin.size()[0]\n",
    "    mb_list = []\n",
    "    mb_list = list(range(int(m/mbsize)))\n",
    "    if m % mbsize == 0: # if the minibatches can be split up perfectly.\n",
    "        'do nothing'\n",
    "    else:\n",
    "        mb_list.append(mb_list[len(mb_list)-1] + 1)\n",
    "        \n",
    "    # 2. Actual iters\n",
    "    # ----------------\n",
    "    for i in range(start_epoch,epochs+1):\n",
    "            \n",
    "        for p in mb_list:\n",
    "            \n",
    "            # Mini batch operations\n",
    "            # ---------------------\n",
    "            start_index = p*mbsize\n",
    "            end_index = m if p == mb_list[len(mb_list)-1] else p*mbsize + mbsize\n",
    "            m_curr = end_index - start_index\n",
    "            \n",
    "            Xin_mb = xin[start_index:end_index]\n",
    "            Yin_mb = yin[start_index:end_index]\n",
    "            \n",
    "            if use_cuda == True:\n",
    "                Xin_mb = Xin_mb.cuda()\n",
    "                Yin_mb = Yin_mb.cuda()\n",
    "                \n",
    "            # normalising ops\n",
    "            # --------------\n",
    "            if norm_flag == 1:\n",
    "                \n",
    "                # normalise both input and target\n",
    "                # -------------------------------\n",
    "                Xin_mb = copy.deepcopy(Xin_mb)/255\n",
    "                Yin_mb = copy.deepcopy(Yin_mb)/255\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # normalise only input\n",
    "                # --------------------\n",
    "                Xin_mb = copy.deepcopy(Xin_mb)/255\n",
    "                \n",
    "                \n",
    "            # Network ops\n",
    "            # -----------\n",
    "            model_out = model(Xin_mb)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model_out, Yin_mb) # loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train.append(loss.item())\n",
    "            \n",
    "            # deleting curr variables\n",
    "            # -----------------------\n",
    "            if use_cuda == True:\n",
    "                Xin_mb = Xin_mb.cpu()\n",
    "                Yin_mb = Yin_mb.cpu()\n",
    "                model_out = model_out.cpu()\n",
    "                \n",
    "                del Xin_mb\n",
    "                del Yin_mb\n",
    "                del model_out\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # printing loss\n",
    "            # -------------\n",
    "            print('Epoch ' + str(i) + ', minibatch ' + str(p+1) + ' of '  +  str(len(mb_list)) + ' -- Model loss: ' + str(loss.item()))\n",
    "            \n",
    "\n",
    "    # 3. outside for loop saving model state\n",
    "    # --------------------------------------\n",
    "    if save_state == True and epochs+1 > start_epoch:\n",
    "        \n",
    "        # 3.1 initialising save dict\n",
    "        # --------------------------\n",
    "        save_dict = {}\n",
    "        save_dict['epoch'] = str(i)\n",
    "        save_dict['model_state_dict'] = model.cpu().state_dict()\n",
    "        save_dict['optimizer_state_dict'] = optimizer.state_dict()\n",
    "        save_dict['loss'] = str(loss.cpu().item())\n",
    "        save_dict['loss_mode'] = loss_mode\n",
    "        \n",
    "        \n",
    "        # 3.2 saving\n",
    "        # ----------\n",
    "        torch.save(save_dict,path)\n",
    "        \n",
    "        # saving full model to initialise a new model later on\n",
    "        # ----------------------------------------------------\n",
    "        torch.save(model.cpu(),path.replace('.tar','_MODEL.tar'))\n",
    "        \n",
    "        print('Saved.')\n",
    "        \n",
    "    \n",
    "    # 4. return model in order to use elsewhere in the code\n",
    "    # -----------------------------------------------------\n",
    "    return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to load a saved model\n",
    "# --------------------------------\n",
    "\n",
    "def load_saved_model_function(path, use_cuda):\n",
    "    \n",
    "    \n",
    "    ''' path = /folder1/folder2/model_ae.tar format'''\n",
    "    \n",
    "    # 1. loading full model\n",
    "    # ---------------------\n",
    "    model = torch.load(path.replace('.tar','_MODEL.tar'))\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()))\n",
    "    \n",
    "    # 2. Applying state dict\n",
    "    # ----------------------\n",
    "    if use_cuda == True:\n",
    "        \n",
    "        # loads to GPU\n",
    "        # ------------\n",
    "        checkpoint = torch.load(path)\n",
    "        \n",
    "    else:\n",
    "        # loads to CPU\n",
    "        # ------------\n",
    "        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        \n",
    "        \n",
    "    # loading checkpoint\n",
    "    # -------------------\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # loading optimizer\n",
    "    # -----------------\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if use_cuda == True:\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.cuda()\n",
    "            \n",
    "            \n",
    "            \n",
    "    # loading other stuff\n",
    "    # -------------------\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    loss_mode = checkpoint['loss_mode']\n",
    "    \n",
    "    return model, optimizer, epoch, loss, loss_mode\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to do a forward pass by chunks\n",
    "# ----------------------------------------------\n",
    "\n",
    "def chunk_pass(xin,model,latent,use_cuda,chunksize):\n",
    "    \n",
    "    # 0. some initialisations\n",
    "    # -----------------------\n",
    "    model = model.eval()\n",
    "    if use_cuda == True:\n",
    "        torch.cuda.empty_cache()\n",
    "        model = model.cuda()\n",
    "        \n",
    "    \n",
    "    # sanity assertion\n",
    "    # ----------------\n",
    "    if len(xin.size()) > 3:\n",
    "        \n",
    "        # normalising data\n",
    "        # ----------------\n",
    "        assert torch.mean(xin).item() > 1, 'Input data is already in range 0-1. Not consistent with flow.'\n",
    "        xin = copy.deepcopy(xin)/255\n",
    "        print(\"Normalised data to 0-1\")\n",
    "       \n",
    "\n",
    "    # 1. chuck loop\n",
    "    # -------------\n",
    "    with tqdm(total=xin.size()[0]) as pbar:\n",
    "        for i,chunk_data in enumerate(chunker(xin, chunksize)):\n",
    "            \n",
    "            # forward pass ops\n",
    "            # ----------------\n",
    "            try:\n",
    "                chunk_data = Variable(chunk_data.data, volatile=True)\n",
    "            except:\n",
    "                chunk_data = Variable(chunk_data, volatile=True)\n",
    "                \n",
    "            if use_cuda == True:\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "               \n",
    "                if latent == True:\n",
    "                    try:\n",
    "                        curr_forwardpass = model.latent(chunk_data.cuda().detach())\n",
    "                    except:\n",
    "                        curr_forwardpass = model.latent(chunk_data.cuda())\n",
    "                else:\n",
    "                    try:\n",
    "                        curr_forwardpass = model(chunk_data.cuda().detach())\n",
    "                    except:\n",
    "                        curr_forwardpass = model(chunk_data.cuda())\n",
    "            else:\n",
    "                \n",
    "                if latent == True:\n",
    "                    try:\n",
    "                        curr_forwardpass = model.latent(chunk_data.cpu().detach())\n",
    "                    except:\n",
    "                        curr_forwardpass = model.latent(chunk_data.cpu())\n",
    "                else:\n",
    "                    try:\n",
    "                        curr_forwardpass = model(chunk_data.cpu().detach())\n",
    "                    except:\n",
    "                        curr_forwardpass = model(chunk_data.cpu())\n",
    "                \n",
    "            # concat ops\n",
    "            # ----------\n",
    "            try:\n",
    "                xout = torch.cat((xout,curr_forwardpass), 0)\n",
    "            except:\n",
    "                xout= curr_forwardpass\n",
    "                \n",
    "            # for memory purpose\n",
    "            # ------------------\n",
    "            if use_cuda == True:\n",
    "                curr_forwardpass = curr_forwardpass.cpu()\n",
    "                chunk_data = chunk_data.cpu()\n",
    "                del curr_forwardpass\n",
    "                del chunk_data\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            pbar.update(chunksize)\n",
    "        \n",
    "    # 2. return\n",
    "    # ---------\n",
    "    xout = Variable(xout.data, volatile=False).cpu()\n",
    "    \n",
    "\n",
    "    return xout\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_ae_4_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2) # nn.Tanh() nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        added_act = nn.Tanh()\n",
    "        dropout_prob = 0.1\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        \n",
    "        #  what worked - 3,64,128,256,128,64,3\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 32\n",
    "        ct1 = nn.Conv2d(3,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 64\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 128\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nn.Softmax2d() #nw_activation_conv\n",
    "        cl3 = [ct3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3)\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        #conv4 = 256\n",
    "        #ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        #cb4 = nn.BatchNorm2d(conv4)\n",
    "        #ca4 = nn.Softmax2d() #nw_activation_conv\n",
    "        #cl4 = [ct4,ca4,dropout_node]\n",
    "        #self.convl4 = nn.Sequential(*cl4) # size 6 x 4\n",
    "        \n",
    "        \n",
    "        # Pooling layer\n",
    "        mxpl =  [nn.MaxPool2d((3,3), stride=3)]\n",
    "        #avpl =  [nn.AvgPool2d((6,4), stride=1)]\n",
    "        self.pool_net = nn.Sequential(*mxpl)\n",
    "        \n",
    "        # Adding a fully connected linear layer\n",
    "        #\n",
    "        \n",
    "        # Upconv layer 0\n",
    "        ###\n",
    "        #t0 = nn.ConvTranspose2d(conv4,conv4,2,stride = 2)\n",
    "        #b0 = nn.BatchNorm2d(conv4)\n",
    "        #a0 = nw_activation_conv\n",
    "        #l0 = [t0,b0,a0]\n",
    "        #self.upcl0 = nn.Sequential(*l0)\n",
    "        \n",
    "        # Upconv layer 1\n",
    "        ###\n",
    "        up_conv1 = 128\n",
    "        t1 = nn.ConvTranspose2d(conv3,up_conv1,3,stride = 3)\n",
    "        b1 = nn.BatchNorm2d(up_conv1)\n",
    "        a1 = nw_activation_conv\n",
    "        l1 = [t1,b1,a1,dropout_node]\n",
    "        self.upcl1 = nn.Sequential(*l1)\n",
    "        \n",
    "        # Upconv layer 2\n",
    "        ###\n",
    "        up_conv2 = 64\n",
    "        t2 = nn.ConvTranspose2d(up_conv1,up_conv2,f,stride = s)\n",
    "        b2 = nn.BatchNorm2d(up_conv2)\n",
    "        a2 = nw_activation_conv\n",
    "        l2 = [t2,b2,a2,dropout_node]\n",
    "        self.upcl2 = nn.Sequential(*l2)\n",
    "        \n",
    "        # Upconv layer 3\n",
    "        ###\n",
    "        up_conv3 = 32\n",
    "        t3 = nn.ConvTranspose2d(up_conv2,up_conv3,f,stride = s)\n",
    "        b3 = nn.BatchNorm2d(up_conv3)\n",
    "        a3 = nw_activation_conv\n",
    "        l3 = [t3,b3,a3,dropout_node]\n",
    "        self.upcl3 = nn.Sequential(*l3)\n",
    "        \n",
    "        # Upconv layer 4\n",
    "        ###\n",
    "        t4 = nn.ConvTranspose2d(up_conv3,3,f,stride = s)\n",
    "        a4 = nn.Sigmoid()\n",
    "        l4 = [t4,a4]\n",
    "        self.upcl4 = nn.Sequential(*l4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Generation\n",
    "        # ----------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        #c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.pool_net(c3_out)\n",
    "        \n",
    "        #f1_out = self.upcl0(c5_out)\n",
    "        f2_out = self.upcl1(c5_out)\n",
    "        f3_out = self.upcl2(f2_out)\n",
    "        f4_out = self.upcl3(f3_out)\n",
    "        f5_out = self.upcl4(f4_out)\n",
    "        \n",
    "        return f5_out\n",
    "\n",
    "    \n",
    "    def latent(self, x):\n",
    "        \n",
    "        \n",
    "        # 0. forward prop\n",
    "        # ---------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        \n",
    "\n",
    "        # 1. Working out layer number\n",
    "        # ---------------------------\n",
    "        if self.layer_mode == 'deep':\n",
    "            forward_out = self.convl4(c3_out)\n",
    "        \n",
    "        elif self.layer_mode == 'deep_minus_2':\n",
    "            forward_out = c2_out\n",
    "        \n",
    "        else:\n",
    "            forward_out = c3_out\n",
    "\n",
    "\n",
    "        # 2. Including a pool layer - setting dims\n",
    "        # ----------------------------------------\n",
    "        if self.layer_dims_set == True:\n",
    "            \n",
    "            ih, iw, pool_stride = self.layer_f, self.layer_f, self.layer_s\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            ih,iw = forward_out.size()[2],forward_out.size()[3]\n",
    "            pool_stride = 1\n",
    "        \n",
    "        if self.pool_mode == 'avg':\n",
    "\n",
    "            # avg pool - comment/uncomment\n",
    "            # ----------------------------\n",
    "            avpl =  nn.Sequential(*[nn.AvgPool2d((ih,iw), stride=pool_stride)])\n",
    "            latent_out = avpl(forward_out)\n",
    "        \n",
    "        elif self.pool_mode == 'max':\n",
    "            \n",
    "            # maxpool - comment/uncomment\n",
    "            # ---------------------------\n",
    "            mxpl =  nn.Sequential(*[nn.MaxPool2d((ih,iw), stride=pool_stride)])\n",
    "            latent_out = mxpl(forward_out)\n",
    "            \n",
    "        elif self.pool_mode == 'both':\n",
    "            \n",
    "            # avg pool\n",
    "            # --------\n",
    "            avpl =  nn.Sequential(*[nn.AvgPool2d((ih,iw), stride=pool_stride)])\n",
    "            latent_out_avg = avpl(forward_out)\n",
    "            latent_out_avg = latent_out_avg.view(latent_out_avg.size()[0],-1)\n",
    "            \n",
    "            # maxpool\n",
    "            # -------\n",
    "            mxpl =  nn.Sequential(*[nn.MaxPool2d((ih,iw), stride=pool_stride)])\n",
    "            latent_out_max = mxpl(forward_out)\n",
    "            latent_out_max = latent_out_max.view(latent_out_max.size()[0],-1)\n",
    "            \n",
    "            # final concat\n",
    "            # ------------\n",
    "            latent_out = torch.cat((latent_out_avg, latent_out_max), 1)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # no pooling\n",
    "            # ----------\n",
    "            latent_out = forward_out\n",
    "            \n",
    "        \n",
    "        return latent_out.view(latent_out.size()[0],-1)\n",
    "        \n",
    "    \n",
    "     \n",
    "    def set_pool_mode(self, pool_mode, layer_mode, layer_dims_set, layer_f, layer_s):\n",
    "        \n",
    "        # setting pool mode\n",
    "        # -----------------\n",
    "        self.pool_mode = pool_mode\n",
    "        self.layer_mode = layer_mode\n",
    "        self.layer_dims_set = layer_dims_set\n",
    "        self.layer_f = layer_f\n",
    "        self.layer_s = layer_s\n",
    "        \n",
    "        print('Modes set.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_ae_3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2) # nn.Tanh() nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        added_act = nn.Tanh()\n",
    "        dropout_prob = 0.1\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        \n",
    "        #  what worked - 3,64,128,256,128,64,3\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 64\n",
    "        ct1 = nn.Conv2d(3,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 128\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 256\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nn.Softmax2d() #nn.Softmax2d() #nw_activation_conv\n",
    "        cl3 = [ct3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3)\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        #conv4 = 256\n",
    "        #ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        #cb4 = nn.BatchNorm2d(conv4)\n",
    "        #ca4 = nn.Softmax2d() #nw_activation_conv\n",
    "        #cl4 = [ct4,ca4,dropout_node]\n",
    "        #self.convl4 = nn.Sequential(*cl4) # size 6 x 4\n",
    "        \n",
    "        \n",
    "        # Pooling layer\n",
    "        #mxpl =  [nn.MaxPool2d((2,2), stride=2)]\n",
    "        #avpl =  [nn.AvgPool2d((6,4), stride=1)]\n",
    "        #self.pool_net = nn.Sequential(*mxpl)\n",
    "        \n",
    "        # Adding a fully connected linear layer\n",
    "        #\n",
    "        \n",
    "        # Upconv layer 0\n",
    "        ###\n",
    "        #t0 = nn.ConvTranspose2d(conv4,conv4,2,stride = 2)\n",
    "        #b0 = nn.BatchNorm2d(conv4)\n",
    "        #a0 = nw_activation_conv\n",
    "        #l0 = [t0,b0,a0]\n",
    "        #self.upcl0 = nn.Sequential(*l0)\n",
    "        \n",
    "        # Upconv layer 1\n",
    "        ###\n",
    "        #up_conv1 = 128\n",
    "        #t1 = nn.ConvTranspose2d(conv4,up_conv1,f,stride = s)\n",
    "        #b1 = nn.BatchNorm2d(up_conv1)\n",
    "        #a1 = nw_activation_conv\n",
    "        #l1 = [t1,b1,a1,dropout_node]\n",
    "        #self.upcl1 = nn.Sequential(*l1)\n",
    "        \n",
    "        # Upconv layer 2\n",
    "        ###\n",
    "        up_conv2 = 128\n",
    "        t2 = nn.ConvTranspose2d(conv3,up_conv2,f,stride = s)\n",
    "        b2 = nn.BatchNorm2d(up_conv2)\n",
    "        a2 = nw_activation_conv\n",
    "        l2 = [t2,b2,a2,dropout_node]\n",
    "        self.upcl2 = nn.Sequential(*l2)\n",
    "        \n",
    "        # Upconv layer 3\n",
    "        ###\n",
    "        up_conv3 = 64\n",
    "        t3 = nn.ConvTranspose2d(up_conv2,up_conv3,f,stride = s)\n",
    "        b3 = nn.BatchNorm2d(up_conv3)\n",
    "        a3 = nw_activation_conv\n",
    "        l3 = [t3,b3,a3,dropout_node]\n",
    "        self.upcl3 = nn.Sequential(*l3)\n",
    "        \n",
    "        # Upconv layer 4\n",
    "        ###\n",
    "        t4 = nn.ConvTranspose2d(up_conv3,3,f,stride = s)\n",
    "        a4 = nn.Sigmoid()\n",
    "        l4 = [t4,a4]\n",
    "        self.upcl4 = nn.Sequential(*l4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Generation\n",
    "        # ----------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        #c4_out = self.convl4(c3_out)\n",
    "        #c5_out = self.pool_net(c3_out)\n",
    "        \n",
    "        #f1_out = self.upcl0(c5_out)\n",
    "        #f2_out = self.upcl1(c4_out)\n",
    "        f3_out = self.upcl2(c3_out)\n",
    "        f4_out = self.upcl3(f3_out)\n",
    "        f5_out = self.upcl4(f4_out)\n",
    "        \n",
    "        return f5_out\n",
    "\n",
    "    \n",
    "    def latent(self, x):\n",
    "        \n",
    "        \n",
    "        # 0. forward prop\n",
    "        # ---------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        \n",
    "        # 1. Working out layer number\n",
    "        # ---------------------------\n",
    "        if self.layer_mode == 'deep_minus_1':\n",
    "            forward_out = c2_out\n",
    "        \n",
    "        else:\n",
    "            forward_out = self.convl3(c2_out)\n",
    "        \n",
    "\n",
    "        # 2. Including a pool layer - setting dims\n",
    "        # ----------------------------------------\n",
    "        if self.layer_dims_set == True:\n",
    "            \n",
    "            ih, iw, pool_stride = self.layer_f, self.layer_f, self.layer_s\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            ih,iw = forward_out.size()[2],forward_out.size()[3]\n",
    "            pool_stride = 1\n",
    "        \n",
    "        if self.pool_mode == 'avg':\n",
    "\n",
    "            # avg pool - comment/uncomment\n",
    "            # ----------------------------\n",
    "            avpl =  nn.Sequential(*[nn.AvgPool2d((ih,iw), stride=pool_stride)])\n",
    "            latent_out = avpl(forward_out)\n",
    "        \n",
    "        elif self.pool_mode == 'max':\n",
    "            \n",
    "            # maxpool - comment/uncomment\n",
    "            # ---------------------------\n",
    "            mxpl =  nn.Sequential(*[nn.MaxPool2d((ih,iw), stride=pool_stride)])\n",
    "            latent_out = mxpl(forward_out)\n",
    "            \n",
    "        elif self.pool_mode == 'both':\n",
    "            \n",
    "            # avg pool\n",
    "            # --------\n",
    "            avpl =  nn.Sequential(*[nn.AvgPool2d((ih,iw), stride=pool_stride)])\n",
    "            latent_out_avg = avpl(forward_out)\n",
    "            latent_out_avg = latent_out_avg.view(latent_out_avg.size()[0],-1)\n",
    "            \n",
    "            # maxpool\n",
    "            # -------\n",
    "            mxpl =  nn.Sequential(*[nn.MaxPool2d((ih,iw), stride=pool_stride)])\n",
    "            latent_out_max = mxpl(forward_out)\n",
    "            latent_out_max = latent_out_max.view(latent_out_max.size()[0],-1)\n",
    "            \n",
    "            # final concat\n",
    "            # ------------\n",
    "            latent_out = torch.cat((latent_out_avg, latent_out_max), 1)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # no pooling\n",
    "            # ----------\n",
    "            latent_out = forward_out\n",
    "            \n",
    "        \n",
    "        return latent_out.view(latent_out.size()[0],-1)\n",
    "        \n",
    "    \n",
    "     \n",
    "    def set_pool_mode(self, pool_mode, layer_mode, layer_dims_set, layer_f, layer_s):\n",
    "        \n",
    "        # setting pool mode\n",
    "        # -----------------\n",
    "        self.pool_mode = pool_mode\n",
    "        self.layer_mode = layer_mode\n",
    "        self.layer_dims_set = layer_dims_set\n",
    "        self.layer_f = layer_f\n",
    "        self.layer_s = layer_s\n",
    "        \n",
    "        print('Modes set.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_ae_deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2) # nn.Tanh() nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        added_act = nn.Tanh()\n",
    "        dropout_prob = 0.2\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        \n",
    "        #  what worked - 3,64,128,256,128,64,3\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 32\n",
    "        ct1 = nn.Conv2d(3,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 64\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 128\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv #nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3)\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 256\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) \n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 512\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nn.Softmax2d()\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) \n",
    "        \n",
    "        \n",
    "        # Pooling layer\n",
    "        #mxpl =  [nn.MaxPool2d((2,2), stride=2)]\n",
    "        #avpl =  [nn.AvgPool2d((6,4), stride=1)]\n",
    "        #self.pool_net = nn.Sequential(*mxpl)\n",
    "        \n",
    "        # Adding a fully connected linear layer\n",
    "        #\n",
    "        \n",
    "        # Upconv layer 0\n",
    "        ###\n",
    "        up_conv0 = 256\n",
    "        t0 = nn.ConvTranspose2d(conv5,up_conv0,f,stride = s)\n",
    "        b0 = nn.BatchNorm2d(up_conv0)\n",
    "        a0 = nw_activation_conv\n",
    "        l0 = [t0,b0,a0,dropout_node]\n",
    "        self.upcl0 = nn.Sequential(*l0)\n",
    "        \n",
    "        # Upconv layer 1\n",
    "        ###\n",
    "        up_conv1 = 128\n",
    "        t1 = nn.ConvTranspose2d(up_conv0,up_conv1,f,stride = s)\n",
    "        b1 = nn.BatchNorm2d(up_conv1)\n",
    "        a1 = nw_activation_conv\n",
    "        l1 = [t1,b1,a1,dropout_node]\n",
    "        self.upcl1 = nn.Sequential(*l1)\n",
    "        \n",
    "        # Upconv layer 2\n",
    "        ###\n",
    "        up_conv2 = 64\n",
    "        t2 = nn.ConvTranspose2d(up_conv1,up_conv2,f,stride = s)\n",
    "        b2 = nn.BatchNorm2d(up_conv2)\n",
    "        a2 = nw_activation_conv\n",
    "        l2 = [t2,b2,a2,dropout_node]\n",
    "        self.upcl2 = nn.Sequential(*l2)\n",
    "        \n",
    "        # Upconv layer 3\n",
    "        ###\n",
    "        up_conv3 = 32\n",
    "        t3 = nn.ConvTranspose2d(up_conv2,up_conv3,f,stride = s)\n",
    "        b3 = nn.BatchNorm2d(up_conv3)\n",
    "        a3 = nw_activation_conv\n",
    "        l3 = [t3,b3,a3,dropout_node]\n",
    "        self.upcl3 = nn.Sequential(*l3)\n",
    "        \n",
    "        # Upconv layer 4\n",
    "        ###\n",
    "        t4 = nn.ConvTranspose2d(up_conv3,3,f,stride = s)\n",
    "        a4 = nn.Sigmoid()\n",
    "        l4 = [t4,a4]\n",
    "        self.upcl4 = nn.Sequential(*l4)\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        # forward pass\n",
    "        # ------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        \n",
    "        f1_out = self.upcl0(c5_out)\n",
    "        f2_out = self.upcl1(f1_out)\n",
    "        f3_out = self.upcl2(f2_out)\n",
    "        f4_out = self.upcl3(f3_out)\n",
    "        f5_out = self.upcl4(f4_out)\n",
    "        \n",
    "        return f5_out\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def latent(self, x):\n",
    "        \n",
    "        \n",
    "        # 0. forward prop\n",
    "        # ---------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "\n",
    "        # 1. Working out layer number\n",
    "        # ---------------------------\n",
    "        if self.layer_mode == 'deep':\n",
    "            forward_out = self.convl5(c4_out)\n",
    "        \n",
    "        elif self.layer_mode == 'deep_minus_2':\n",
    "            forward_out = c3_out\n",
    "        \n",
    "        else:\n",
    "            forward_out = c4_out\n",
    "\n",
    "\n",
    "        # 2. Including a pool layer \n",
    "        # -------------------------\n",
    "        ih,iw = forward_out.size()[2],forward_out.size()[3]\n",
    "        \n",
    "        if self.pool_mode == 'avg':\n",
    "\n",
    "            # avg pool - comment/uncomment\n",
    "            # ----------------------------\n",
    "            avpl =  nn.Sequential(*[nn.AvgPool2d((ih,iw), stride=1)])\n",
    "            latent_out = avpl(forward_out)\n",
    "        \n",
    "        elif self.pool_mode == 'max':\n",
    "            \n",
    "            # maxpool - comment/uncomment\n",
    "            # ---------------------------\n",
    "            mxpl =  nn.Sequential(*[nn.MaxPool2d((ih,iw), stride=1)])\n",
    "            latent_out = mxpl(forward_out)\n",
    "            \n",
    "        elif self.pool_mode == 'both':\n",
    "            \n",
    "            # avg pool\n",
    "            # --------\n",
    "            avpl =  nn.Sequential(*[nn.AvgPool2d((ih,iw), stride=1)])\n",
    "            latent_out_avg = avpl(forward_out)\n",
    "            latent_out_avg = latent_out_avg.view(latent_out_avg.size()[0],-1)\n",
    "            \n",
    "            # maxpool\n",
    "            # -------\n",
    "            mxpl =  nn.Sequential(*[nn.MaxPool2d((ih,iw), stride=1)])\n",
    "            latent_out_max = mxpl(forward_out)\n",
    "            latent_out_max = latent_out_max.view(latent_out_max.size()[0],-1)\n",
    "            \n",
    "            # final concat\n",
    "            # ------------\n",
    "            latent_out = torch.cat((latent_out_avg, latent_out_max), 1)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # no pooling\n",
    "            # ----------\n",
    "            latent_out = forward_out\n",
    "            \n",
    "        \n",
    "        return latent_out.view(latent_out.size()[0],-1)\n",
    "        \n",
    "    \n",
    "     \n",
    "    def set_pool_mode(self, pool_mode, layer_mode):\n",
    "        \n",
    "        # setting pool mode\n",
    "        # -----------------\n",
    "        self.pool_mode = pool_mode\n",
    "        self.layer_mode = layer_mode\n",
    "        print('Modes set.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_ae_6_layer_WNET(nn.Module):\n",
    "    def __init__(self, in_channels, latent_softmax):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is WNET model\n",
    "        # ------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (191,191) -- Insize\n",
    "        \n",
    "        # @conv1 - (95,95)\n",
    "        # @conv2 - (47, 47)\n",
    "        # @conv3 - (23, 23)\n",
    "        # @conv4 - (11, 11)\n",
    "        # @conv5 - (5, 5)\n",
    "        # @conv6 - (2,2)\n",
    "        # Followed by a an avg pool (2,2) to make this 1,1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2) # nn.Tanh() nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        dropout_prob = 0.2\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 16\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3)\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 128\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) \n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 256\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) \n",
    "        \n",
    "        # Conv 6\n",
    "        ###\n",
    "        conv6 = 512\n",
    "        ct6 = nn.Conv2d(conv5,conv6,f,stride = s)\n",
    "        cb6 = nn.BatchNorm2d(conv6)\n",
    "        ca6 = nw_activation_conv\n",
    "        cl6 = [ct6,cb6,ca6,dropout_node]\n",
    "        self.convl6 = nn.Sequential(*cl6) \n",
    "        \n",
    "\n",
    "        # Pooling layer + softmax activation\n",
    "        # ----------------------------------\n",
    "        if latent_softmax == True:\n",
    "            avpl =  [nn.AvgPool2d((2,2), stride=1), nn.Softmax2d()]\n",
    "        else:\n",
    "            avpl =  [nn.AvgPool2d((2,2), stride=1)]\n",
    "        self.pool_net = nn.Sequential(*avpl)\n",
    "        \n",
    "      \n",
    "        # Transconv layers\n",
    "        # ----------------\n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # Incoming input is 1 x 1 x C\n",
    "        # (5, 5)\n",
    "        # (11, 11)\n",
    "        # (23, 23)\n",
    "        # (47, 47)\n",
    "        # (95, 95)\n",
    "        # (191, 191)\n",
    "        \n",
    "        # Upconv layer 0\n",
    "        ###\n",
    "        up_conv0 = conv6\n",
    "        t0 = nn.ConvTranspose2d(conv6,up_conv0,2,stride = 1)\n",
    "        b0 = nn.BatchNorm2d(up_conv0)\n",
    "        a0 = nw_activation_conv\n",
    "        l0 = [t0,b0,a0,dropout_node]\n",
    "        self.upcl0 = nn.Sequential(*l0) # 2x2\n",
    "        \n",
    "        # Upconv layer 1\n",
    "        # concat layer\n",
    "        ###\n",
    "        up_conv1 = 256\n",
    "        t1 = nn.ConvTranspose2d(up_conv0 + conv6,up_conv1,f,stride = s)\n",
    "        b1 = nn.BatchNorm2d(up_conv1)\n",
    "        a1 = nw_activation_conv\n",
    "        l1 = [t1,b1,a1,dropout_node]\n",
    "        self.upcl1 = nn.Sequential(*l1) # 5x5\n",
    "        \n",
    "        # Upconv layer 2\n",
    "        # concat layer\n",
    "        ###\n",
    "        up_conv2 = 128\n",
    "        t2 = nn.ConvTranspose2d(up_conv1 + conv5,up_conv2,f,stride = s)\n",
    "        b2 = nn.BatchNorm2d(up_conv2)\n",
    "        a2 = nw_activation_conv\n",
    "        l2 = [t2,b2,a2,dropout_node]\n",
    "        self.upcl2 = nn.Sequential(*l2)\n",
    "        \n",
    "        # Upconv layer 3\n",
    "        # concat layer\n",
    "        ###\n",
    "        up_conv3 = 64\n",
    "        t3 = nn.ConvTranspose2d(up_conv2 + conv4,up_conv3,f,stride = s)\n",
    "        b3 = nn.BatchNorm2d(up_conv3)\n",
    "        a3 = nw_activation_conv\n",
    "        l3 = [t3,b3,a3,dropout_node]\n",
    "        self.upcl3 = nn.Sequential(*l3)\n",
    "        \n",
    "        # Upconv layer 4\n",
    "        # concat layer\n",
    "        ###\n",
    "        up_conv4 = 32\n",
    "        t4 = nn.ConvTranspose2d(up_conv3 + conv3,up_conv4,f,stride = s)\n",
    "        b4 = nn.BatchNorm2d(up_conv4)\n",
    "        a4 = nw_activation_conv\n",
    "        l4 = [t4,b4,a4,dropout_node]\n",
    "        self.upcl4 = nn.Sequential(*l4)\n",
    "        \n",
    "        # Upconv layer 5\n",
    "        # concat layer\n",
    "        ###\n",
    "        up_conv5 = 16\n",
    "        t5 = nn.ConvTranspose2d(up_conv4 + conv2,up_conv5,f,stride = s)\n",
    "        b5 = nn.BatchNorm2d(up_conv5)\n",
    "        a5 = nw_activation_conv\n",
    "        l5 = [t5,b5,a5,dropout_node]\n",
    "        self.upcl5 = nn.Sequential(*l5)\n",
    "    \n",
    "    \n",
    "        # Upconv layer 6\n",
    "        # concat layer - FINAL LAYER\n",
    "        ###\n",
    "        t6 = nn.ConvTranspose2d(up_conv5 + conv1,3,f,stride = s)\n",
    "        a6 = nn.Sigmoid()\n",
    "        l6 = [t6,a6]\n",
    "        self.upcl6 = nn.Sequential(*l6)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Conv pass\n",
    "        # ---------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        c6_out = self.convl6(c5_out)\n",
    "        \n",
    "        # pooling\n",
    "        # -------\n",
    "        latent_out = self.pool_net(c6_out)\n",
    "        \n",
    "        # Transconv pass\n",
    "        # --------------\n",
    "        f1_out = self.upcl0(latent_out)\n",
    "        f2_out = self.upcl1(torch.cat((f1_out,c6_out), 1))\n",
    "        f3_out = self.upcl2(torch.cat((f2_out,c5_out), 1))\n",
    "        f4_out = self.upcl3(torch.cat((f3_out,c4_out), 1))\n",
    "        f5_out = self.upcl4(torch.cat((f4_out,c3_out), 1))\n",
    "        f6_out = self.upcl5(torch.cat((f5_out,c2_out), 1))\n",
    "        f7_out = self.upcl6(torch.cat((f6_out,c1_out), 1))\n",
    "        \n",
    "        return f7_out\n",
    "\n",
    "    \n",
    "    def latent(self, x):\n",
    "        \n",
    "        \n",
    "        # Conv pass\n",
    "        # ---------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        c6_out = self.convl6(c5_out)\n",
    "        \n",
    "        # pooling\n",
    "        # -------\n",
    "        latent_out = self.pool_net(c6_out)\n",
    "\n",
    "        return latent_out.view(latent_out.size()[0],-1)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class standard_cnn_6_layer(nn.Module):\n",
    "    def __init__(self, in_channels, latent_softmax):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is WNET model\n",
    "        # ------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (191,191) -- Insize\n",
    "        \n",
    "        # @conv1 - (95,95)\n",
    "        # @conv2 - (47, 47)\n",
    "        # @conv3 - (23, 23)\n",
    "        # @conv4 - (11, 11)\n",
    "        # @conv5 - (5, 5)\n",
    "        # @conv6 - (2,2)\n",
    "        # Followed by a an avg pool (2,2) to make this 1,1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2) # nn.Tanh() nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        dropout_prob = 0.2\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 16\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3)\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 128\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) \n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 256\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) \n",
    "        \n",
    "        # Conv 6\n",
    "        ###\n",
    "        conv6 = 512\n",
    "        ct6 = nn.Conv2d(conv5,conv6,f,stride = s)\n",
    "        cb6 = nn.BatchNorm2d(conv6)\n",
    "        ca6 = nw_activation_conv\n",
    "        cl6 = [ct6,cb6,ca6,dropout_node]\n",
    "        self.convl6 = nn.Sequential(*cl6) \n",
    "        \n",
    "\n",
    "        # Pooling layer + softmax activation\n",
    "        # ----------------------------------\n",
    "        if latent_softmax == True:\n",
    "            avpl =  [nn.AvgPool2d((2,2), stride=1), nn.Softmax2d()]\n",
    "        else:\n",
    "            avpl =  [nn.AvgPool2d((2,2), stride=1)]\n",
    "        self.pool_net = nn.Sequential(*avpl)\n",
    "        \n",
    "        \n",
    "        # Adding linear layers\n",
    "        # -------------------\n",
    "        lnt1 = nn.Linear(conv6,256)\n",
    "        lnb1 = nn.BatchNorm1d(256)\n",
    "        lna1 = nw_activation_conv\n",
    "        ln1 = [lnt1,lnb1,lna1,dropout_node]\n",
    "        self.linear1 = nn.Sequential(*ln1) \n",
    "      \n",
    "        lnt2 = nn.Linear(256,128)\n",
    "        lnb2 = nn.BatchNorm1d(128)\n",
    "        lna2 = nw_activation_conv\n",
    "        ln2 = [lnt2,lnb2,lna2,dropout_node]\n",
    "        self.linear2 = nn.Sequential(*ln2)\n",
    "        \n",
    "        lnt3 = nn.Linear(128,64)\n",
    "        lnb3 = nn.BatchNorm1d(64)\n",
    "        lna3 = nw_activation_conv\n",
    "        ln3 = [lnt3,lnb3,lna3,dropout_node]\n",
    "        self.linear3 = nn.Sequential(*ln3)\n",
    "        \n",
    "        ln4 = [nn.Linear(64,1)]\n",
    "        self.linear4 = nn.Sequential(*ln4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Conv pass\n",
    "        # ---------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        c6_out = self.convl6(c5_out)\n",
    "        \n",
    "        # pooling\n",
    "        # -------\n",
    "        latent_out = self.pool_net(c6_out)\n",
    "        \n",
    "        # linear out\n",
    "        # ----------\n",
    "        linear1_out = self.linear1(latent_out.view(latent_out.size()[0],-1))\n",
    "        linear2_out = self.linear2(linear1_out)\n",
    "        linear3_out = self.linear3(linear2_out)\n",
    "        linear4_out = self.linear4(linear3_out)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return linear4_out\n",
    "\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class standard_cnn_4_layer(nn.Module):\n",
    "    def __init__(self, in_channels, latent_softmax):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is WNET model\n",
    "        # ------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (191,191) -- Insize\n",
    "        \n",
    "        # @conv1 - (95,95)\n",
    "        # @conv2 - (47, 47)\n",
    "        # @conv3 - (23, 23)\n",
    "        # @conv4 - (11, 11)\n",
    "        # @conv5 - (5, 5)\n",
    "        # @conv6 - (2,2)\n",
    "        # Followed by a an avg pool (2,2) to make this 1,1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2) # nn.Tanh() nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        dropout_prob = 0.2\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 64\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 128\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 256\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3)\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 512\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) \n",
    "    \n",
    "        \n",
    "        # Adding linear layers\n",
    "        # -------------------\n",
    "        lnt1 = nn.Linear(conv4*11*11,1024)\n",
    "        lnb1 = nn.BatchNorm1d(1024)\n",
    "        lna1 = nw_activation_conv\n",
    "        ln1 = [lnt1,lnb1,lna1,dropout_node]\n",
    "        self.linear1 = nn.Sequential(*ln1) \n",
    "      \n",
    "        lnt2 = nn.Linear(1024,512)\n",
    "        lnb2 = nn.BatchNorm1d(512)\n",
    "        lna2 = nw_activation_conv\n",
    "        ln2 = [lnt2,lnb2,lna2,dropout_node]\n",
    "        self.linear2 = nn.Sequential(*ln2)\n",
    "        \n",
    "        lnt3 = nn.Linear(512,256)\n",
    "        lnb3 = nn.BatchNorm1d(256)\n",
    "        lna3 = nw_activation_conv\n",
    "        ln3 = [lnt3,lnb3,lna3,dropout_node]\n",
    "        self.linear3 = nn.Sequential(*ln3)\n",
    "        \n",
    "        ln4 = [nn.Linear(256,1)]\n",
    "        self.linear4 = nn.Sequential(*ln4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Conv pass\n",
    "        # ---------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        \n",
    "        # linear out\n",
    "        # ----------\n",
    "        linear1_out = self.linear1(c4_out.view(c4_out.size()[0],-1))\n",
    "        linear2_out = self.linear2(linear1_out)\n",
    "        linear3_out = self.linear3(linear2_out)\n",
    "        linear4_out = self.linear4(linear3_out)\n",
    "        \n",
    "        return linear4_out\n",
    "\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_ae_6_layer_UNET_multiple_outchannels(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is an UNET model\n",
    "        # ---------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (32,32) -- Insize\n",
    "        \n",
    "        # @conv1    -   (30,30)\n",
    "        # @conv2    -   (28,28)\n",
    "        # @maxpool1 -   (14,14)\n",
    "        \n",
    "        # @conv3    -   (12,12)\n",
    "        # @conv4    -   (10,10)\n",
    "        # @maxpool2 -   (5,5)\n",
    "        \n",
    "        # @conv5    -   (3,3)\n",
    "        # @conv6    -   (1,1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2), nn.Tanh(), nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 1\n",
    "        dropout_prob = 0.2\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        pool_dims = (2,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 16\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1) # 30x30\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2) # 28x28\n",
    "        \n",
    "        # Maxpool 1\n",
    "        ###\n",
    "        mxpool_1 =  [nn.MaxPool2d(pool_dims, stride=1, return_indices=True)]\n",
    "        self.mxpool1 = nn.Sequential(*mxpool_1) # 14x14\n",
    "        \n",
    "\n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3) # 12x12\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 128\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) # 10x10\n",
    "        \n",
    "        # Maxpool 2\n",
    "        ###\n",
    "        mxpool_2 =  [nn.MaxPool2d(pool_dims, stride=1, return_indices=True)]\n",
    "        self.mxpool2 = nn.Sequential(*mxpool_2) # 5x5\n",
    "\n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 256\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) # 3x3\n",
    "        \n",
    "        # Conv 6\n",
    "        ###\n",
    "        conv6 = 512\n",
    "        ct6 = nn.Conv2d(conv5,conv6,f,stride = s)\n",
    "        cb6 = nn.BatchNorm2d(conv6)\n",
    "        ca6 = nw_activation_conv\n",
    "        cl6 = [ct6,cb6,ca6,dropout_node]\n",
    "        self.convl6 = nn.Sequential(*cl6) # 1x1\n",
    "        \n",
    "\n",
    "      \n",
    "        # Transconv layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Upconv layer 0\n",
    "        ###\n",
    "        up_conv0 = 256\n",
    "        t0 = nn.ConvTranspose2d(conv6,up_conv0,f,stride = s)\n",
    "        b0 = nn.BatchNorm2d(up_conv0)\n",
    "        a0 = nw_activation_conv\n",
    "        l0 = [t0,b0,a0,dropout_node]\n",
    "        self.upcl0 = nn.Sequential(*l0) # 3x3\n",
    "        \n",
    "        # Upconv layer 1\n",
    "        # concat layer\n",
    "        ###\n",
    "        up_conv1 = 128\n",
    "        t1 = nn.ConvTranspose2d(up_conv0 + conv5,up_conv1,f,stride = s)\n",
    "        b1 = nn.BatchNorm2d(up_conv1)\n",
    "        a1 = nw_activation_conv\n",
    "        l1 = [t1,b1,a1,dropout_node]\n",
    "        self.upcl1 = nn.Sequential(*l1) # 5x5\n",
    "        \n",
    "        # Maxunpool 2\n",
    "        ###\n",
    "        mxunpool_2 =  [nn.MaxUnpool2d(pool_dims, stride=1)]\n",
    "        self.mxunpool2 = nn.Sequential(*mxunpool_2) # 10x10\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Upconv layer 2\n",
    "        # concat layer\n",
    "        ###\n",
    "        up_conv2 = 64\n",
    "        t2 = nn.ConvTranspose2d(up_conv1 + conv4,up_conv2,f,stride = s)\n",
    "        b2 = nn.BatchNorm2d(up_conv2)\n",
    "        a2 = nw_activation_conv\n",
    "        l2 = [t2,b2,a2,dropout_node]\n",
    "        self.upcl2 = nn.Sequential(*l2) # 12x12\n",
    "        \n",
    "        # Upconv layer 3\n",
    "        # concat layer\n",
    "        ###\n",
    "        up_conv3 = 32\n",
    "        t3 = nn.ConvTranspose2d(up_conv2 + conv3,up_conv3,f,stride = s)\n",
    "        b3 = nn.BatchNorm2d(up_conv3)\n",
    "        a3 = nw_activation_conv\n",
    "        l3 = [t3,b3,a3,dropout_node]\n",
    "        self.upcl3 = nn.Sequential(*l3) # 14x14\n",
    "        \n",
    "        # Maxunpool 1\n",
    "        ###\n",
    "        mxunpool_1 =  [nn.MaxUnpool2d(pool_dims, stride=1)]\n",
    "        self.mxunpool1 = nn.Sequential(*mxunpool_1) # 28x28\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Upconv layer 4\n",
    "        # concat layer\n",
    "        ###\n",
    "        up_conv4 = 16\n",
    "        t4 = nn.ConvTranspose2d(up_conv3 + conv2,up_conv4,f,stride = s)\n",
    "        b4 = nn.BatchNorm2d(up_conv4)\n",
    "        a4 = nw_activation_conv\n",
    "        l4 = [t4,b4,a4,dropout_node]\n",
    "        self.upcl4 = nn.Sequential(*l4) # 30x30\n",
    "        \n",
    "        \n",
    "        # Upconv layer 5\n",
    "        # concat layer - FINAL LAYER\n",
    "        ###\n",
    "        up_conv5 = out_channels\n",
    "        t5 = nn.ConvTranspose2d(up_conv4 + conv1,up_conv5,f,stride = s)\n",
    "        a5 = nn.Sigmoid()\n",
    "        l5 = [t5,b5]\n",
    "        self.upcl5 = nn.Sequential(*l5) # 32x32\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Do over..\n",
    "        \n",
    "        \n",
    "        ''' WONT WORK SINCE MAXUNPOOL CANNOT UNPOOL OVER NEWLY CONCATENATED CHANNELS \n",
    "        -- JUST DO A STD ONE WITH CON LAYERS ONLY '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        return None\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_classifier(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, target_act):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is an UNET model\n",
    "        # ---------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (32,32) -- Insize\n",
    "        \n",
    "        # @conv1    -   (16,16)\n",
    "        # @conv2    -   (8,8)        \n",
    "        # @conv3    -   (4,4)\n",
    "        # @conv4    -   (2,2)        \n",
    "        # @conv5    -   (1,1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2), nn.Tanh(), nn.Softmax2d()\n",
    "        f = 2\n",
    "        s = 2\n",
    "        dropout_prob = 0.05\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        pool_dims = (2,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 32\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 64\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 128\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3)\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 256\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4)\n",
    "        \n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = out_channels\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        if target_act == 'sigmoid':\n",
    "            ca5 = nn.Sigmoid()\n",
    "        else:\n",
    "            ca5 = nn.Softmax2d()\n",
    "        cl5 = [ct5,ca5]\n",
    "        self.convl5 = nn.Sequential(*cl5)\n",
    "\n",
    "     \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # simple forward pass\n",
    "        # -------------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        \n",
    "        return c5_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_do_not_use(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is ONLY FOR TEST PURPOSE\n",
    "        # -----------------------------\n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        in_channels, out_channels, target_act = 3,10,'sigmoid'\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2), nn.Tanh(), nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        dropout_prob = 0.2\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        pool_dims = (2,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 16\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3)\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 128\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4)\n",
    "        \n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = out_channels\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = 1)\n",
    "        if target_act == 'sigmoid':\n",
    "            ca5 = nn.Sigmoid()\n",
    "        else:\n",
    "            ca5 = nn.Softmax2d()\n",
    "        cl5 = [ct5,ca5]\n",
    "        self.convl5 = nn.Sequential(*cl5)\n",
    "\n",
    "     \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # simple forward pass\n",
    "        # -------------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        \n",
    "        return c5_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_classifier_cifar10linear(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, target_act):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is an UNET model\n",
    "        # ---------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (32,32) -- Insize\n",
    "        \n",
    "        # @conv1    -   (30,30)\n",
    "        # @conv2    -   (28,28)\n",
    "        # @maxpool1 -   (14,14)\n",
    "        \n",
    "        # @conv3    -   (12,12)\n",
    "        # @conv4    -   (10,10)\n",
    "        # @maxpool2 -   (5,5)\n",
    "        \n",
    "        # @conv5    -   (3,3)\n",
    "        # @conv6    -   (1,1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2), nn.Tanh(), nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 1\n",
    "        dropout_prob = 0.05\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        pool_dims = (2,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 32\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1) # 30x30\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2) # 28x28\n",
    "        \n",
    "        # Maxpool 1\n",
    "        ###\n",
    "        mxpool_1 =  [nn.AvgPool2d(pool_dims, stride=2)]\n",
    "        self.mxpool1 = nn.Sequential(*mxpool_1) # 14x14\n",
    "\n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3) # 12x12\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 64\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) # 10x10\n",
    "        \n",
    "        # Maxpool 2\n",
    "        ###\n",
    "        mxpool_2 =  [nn.AvgPool2d(pool_dims, stride=2)]\n",
    "        self.mxpool2 = nn.Sequential(*mxpool_2) # 5x5\n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 64\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) # 3X3\n",
    "        \n",
    "        # Conv 6\n",
    "        ###\n",
    "        conv6 = 64\n",
    "        ct6 = nn.Conv2d(conv5,conv6,f,stride = s)\n",
    "        cb6 = nn.BatchNorm2d(conv6)\n",
    "        ca6 = nw_activation_conv\n",
    "        cl6 = [ct6,cb6,ca6,dropout_node]\n",
    "        self.convl6 = nn.Sequential(*cl6) # 1X1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # linear layer 1\n",
    "        ###\n",
    "        ld1 = nn.Linear(1*1*64,32)\n",
    "        lb1 = nn.BatchNorm1d(32)\n",
    "        la1 = nw_activation_conv\n",
    "        ln1 = [ld1,lb1,la1,dropout_node]\n",
    "        self.lin1 = nn.Sequential(*ln1)\n",
    "        \n",
    "        # linear layer 2\n",
    "        ###\n",
    "        ld2 = nn.Linear(32,10)\n",
    "        if target_act == 'sigmoid':\n",
    "            la2 = nn.Sigmoid()\n",
    "        else:\n",
    "            la2 = nn.Softmax2d()\n",
    "        ln2 = [ld2,la2]\n",
    "        self.lin2 = nn.Sequential(*ln2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # simple forward pass\n",
    "        # -------------------\n",
    "        c1_out = self.convl1(x) # 30X30\n",
    "        c2_out = self.convl2(c1_out) # 28x28\n",
    "        mxpool1_out = self.mxpool1(c2_out) # 14x14\n",
    "        \n",
    "        \n",
    "        c3_out = self.convl3(mxpool1_out) # 12x12\n",
    "        c4_out = self.convl4(c3_out) #10x10\n",
    "        mxpool2_out = self.mxpool2(c4_out) #5x5\n",
    "        \n",
    "        c5_out = self.convl5(mxpool2_out) #3x3\n",
    "        c6_out = self.convl6(c5_out) #1x1\n",
    "        \n",
    "        # linear pass\n",
    "        # -----------\n",
    "        l1_out = self.lin1(c6_out.view(c6_out.size()[0],-1))\n",
    "        l2_out = self.lin2(l1_out)\n",
    "        \n",
    "        return l2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_classifier_cifar10(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, target_act):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is an UNET model\n",
    "        # ---------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (32,32) -- Insize\n",
    "        \n",
    "        # @conv1    -   (30,30)\n",
    "        # @conv2    -   (28,28)\n",
    "        # @maxpool1 -   (14,14)\n",
    "        \n",
    "        # @conv3    -   (12,12)\n",
    "        # @conv4    -   (10,10)\n",
    "        # @maxpool2 -   (5,5)\n",
    "        \n",
    "        # @conv5    -   (3,3)\n",
    "        # @conv6    -   (1,1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2), nn.Tanh(), nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 1\n",
    "        dropout_prob = 0.1\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        pool_dims = (2,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 32\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1) # 30x30\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2) # 28x28\n",
    "        \n",
    "        # Maxpool 1\n",
    "        ###\n",
    "        mxpool_1 =  [nn.MaxPool2d(pool_dims, stride=2)]\n",
    "        self.mxpool1 = nn.Sequential(*mxpool_1) # 14x14\n",
    "\n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3) # 12x12\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 64\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) # 10x10\n",
    "        \n",
    "        # Maxpool 2\n",
    "        ###\n",
    "        mxpool_2 =  [nn.MaxPool2d(pool_dims, stride=2)]\n",
    "        self.mxpool2 = nn.Sequential(*mxpool_2) # 5x5\n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 128\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) # 3x3\n",
    "        \n",
    "        \n",
    "        # Conv 6\n",
    "        ###\n",
    "        ct6 = nn.Conv2d(conv5,out_channels,f,stride = s)\n",
    "        if target_act == 'sigmoid':\n",
    "            ca6 = nn.Sigmoid()\n",
    "        else:\n",
    "            ca6 = nn.Softmax2d()\n",
    "        cl6 = [ct6,ca6]\n",
    "        self.convl6 = nn.Sequential(*cl6) # 1x1\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # simple forward pass\n",
    "        # -------------------\n",
    "        c1_out = self.convl1(x) # 30X30\n",
    "        c2_out = self.convl2(c1_out) # 28x28\n",
    "        mxpool1_out = self.mxpool1(c2_out) # 14x14\n",
    "        \n",
    "        \n",
    "        c3_out = self.convl3(mxpool1_out) # 12x12\n",
    "        c4_out = self.convl4(c3_out) #10x10\n",
    "        mxpool2_out = self.mxpool2(c4_out) #5x5\n",
    "        \n",
    "        \n",
    "        c5_out = self.convl5(mxpool2_out) # 3x3\n",
    "        c6_out = self.convl6(c5_out) # 1x1\n",
    "        \n",
    "        return c6_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_classifier_cifar10_linlayers(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, target_act):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is an UNET model\n",
    "        # ---------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (32,32) -- Insize\n",
    "        \n",
    "        # @conv1    -   (30,30)\n",
    "        # @conv2    -   (28,28)\n",
    "        # @maxpool1 -   (14,14)\n",
    "        \n",
    "        # @conv3    -   (12,12)\n",
    "        # @conv4    -   (10,10)\n",
    "        # @maxpool2 -   (5,5)\n",
    "        \n",
    "        # @conv5    -   (3,3)\n",
    "        # @conv6    -   (1,1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.LeakyReLU(0.2) #nn.ReLU() #nn.LeakyReLU(0.2), nn.Tanh(), nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 1\n",
    "        dropout_prob = 0.05\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        pool_dims = (2,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 32\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1) # 30x30\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2) # 28x28\n",
    "        \n",
    "        # Maxpool 1\n",
    "        ###\n",
    "        mxpool_1 =  [nn.MaxPool2d(pool_dims, stride=2)]\n",
    "        self.mxpool1 = nn.Sequential(*mxpool_1) # 14x14\n",
    "\n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3) # 12x12\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 64\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) # 10x10\n",
    "        \n",
    "        # Maxpool 2\n",
    "        ###\n",
    "        mxpool_2 =  [nn.MaxPool2d(pool_dims, stride=2)]\n",
    "        self.mxpool2 = nn.Sequential(*mxpool_2) # 5x5\n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 128\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) # 3x3\n",
    "        \n",
    "        \n",
    "        # Conv 6\n",
    "        ###\n",
    "        conv6 = 128\n",
    "        ct6 = nn.Conv2d(conv5,conv6,f,stride = s)\n",
    "        cb6 = nn.BatchNorm2d(conv6)\n",
    "        ca6 = nw_activation_conv\n",
    "        cl6 = [ct6,cb6,ca6,dropout_node]\n",
    "        self.convl6 = nn.Sequential(*cl6) # 1x1\n",
    "        \n",
    "        \n",
    "        \n",
    "        #################### Linear 1x1 convs #########################\n",
    "        \n",
    "        # Conv ln 1\n",
    "        ###\n",
    "        convln1 = 64\n",
    "        ctln1 = nn.Conv2d(conv6,convln1,1,stride = 1)\n",
    "        cbln1 = nn.BatchNorm2d(convln1)\n",
    "        caln1 = nw_activation_conv\n",
    "        clln1 = [ctln1,cbln1,caln1,dropout_node]\n",
    "        self.convlln1 = nn.Sequential(*clln1) # 1x1\n",
    "        \n",
    "        # Conv ln 2\n",
    "        ###\n",
    "        convln2 = 32\n",
    "        ctln2 = nn.Conv2d(convln1,convln2,1,stride = 1)\n",
    "        cbln2 = nn.BatchNorm2d(convln2)\n",
    "        caln2 = nw_activation_conv\n",
    "        clln2 = [ctln2,cbln2,caln2,dropout_node]\n",
    "        self.convlln2 = nn.Sequential(*clln2) # 1x1\n",
    "        \n",
    "        # Conv ln 3\n",
    "        ###\n",
    "        #convln3 = 64\n",
    "        #ctln3 = nn.Conv2d(convln2,convln3,1,stride = 1)\n",
    "        #cbln3 = nn.BatchNorm2d(convln3)\n",
    "        #caln3 = nw_activation_conv\n",
    "        #clln3 = [ctln3,cbln3,caln3,dropout_node]\n",
    "        #self.convlln3 = nn.Sequential(*clln3) # 1x1\n",
    "        \n",
    "        # Conv ln 4\n",
    "        ###\n",
    "        #convln4 = 32\n",
    "        #ctln4 = nn.Conv2d(convln3,convln4,1,stride = 1)\n",
    "        #cbln4 = nn.BatchNorm2d(convln4)\n",
    "        #caln4 = nw_activation_conv\n",
    "        #clln4 = [ctln4,cbln4,caln4,dropout_node]\n",
    "        #self.convlln4 = nn.Sequential(*clln4) # 1x1\n",
    "        \n",
    "        \n",
    "        \n",
    "        #################### final output conv #########################\n",
    "        \n",
    "        \n",
    "        # Conv 6\n",
    "        ###\n",
    "        ct7 = nn.Conv2d(convln2,out_channels,1,stride = 1)\n",
    "        if target_act == 'sigmoid':\n",
    "            ca7 = nn.Sigmoid()\n",
    "        else:\n",
    "            ca7 = nn.Softmax2d()\n",
    "        cl7 = [ct7,ca7]\n",
    "        self.convl7 = nn.Sequential(*cl7) # 1x1\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # simple forward pass\n",
    "        # -------------------\n",
    "        c1_out = self.convl1(x) # 30X30\n",
    "        c2_out = self.convl2(c1_out) # 28x28\n",
    "        mxpool1_out = self.mxpool1(c2_out) # 14x14\n",
    "        \n",
    "        c3_out = self.convl3(mxpool1_out) # 12x12\n",
    "        c4_out = self.convl4(c3_out) #10x10\n",
    "        mxpool2_out = self.mxpool2(c4_out) #5x5\n",
    "        \n",
    "        c5_out = self.convl5(mxpool2_out) # 3x3\n",
    "        c6_out = self.convl6(c5_out) # 1x1\n",
    "        \n",
    "        # linear layer passes\n",
    "        # -------------------\n",
    "        cln1_out = self.convlln1(c6_out) # 1x1\n",
    "        cln2_out = self.convlln2(cln1_out) # 1x1\n",
    "        #cln3_out = self.convlln3(cln2_out) # 1x1\n",
    "        #cln4_out = self.convlln4(cln3_out) # 1x1\n",
    "        \n",
    "        # final out\n",
    "        # ---------\n",
    "        c7_out = self.convl7(cln2_out) # 1x1\n",
    "        \n",
    "        \n",
    "        return c7_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_classifier_cifar10_deeper(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, target_act):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is an UNET model\n",
    "        # ---------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (32,32) -- Insize\n",
    "        \n",
    "        # @conv1    -   (30,30)\n",
    "        # @conv2    -   (28,28)\n",
    "        # @conv3    -   (26,26)\n",
    "        # @conv4    -   (24,24)\n",
    "        # @conv5    -   (22,22)\n",
    "        # @maxpool1 -   (11,11)\n",
    "        \n",
    "\n",
    "        # @conv6    -   (9,9)\n",
    "        # @conv7    -   (7,7)\n",
    "        # @conv8    -   (5,5)\n",
    "        # @conv9    -   (3,3)\n",
    "        # @conv10    -   (1,1)\n",
    "        # @conv11    -   (1,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.LeakyReLU(0.2) #nn.LeakyReLU(0.2), nn.Tanh(), nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 1\n",
    "        dropout_prob = 0.0\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        pool_dims = (2,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 32\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1) \n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2) \n",
    "\n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3) \n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 64\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) \n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 128\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) \n",
    "        \n",
    "        \n",
    "        # Maxpool 1\n",
    "        ###\n",
    "        mxpool_1 =  [nn.MaxPool2d(pool_dims, stride=2)]\n",
    "        self.mxpool1 = nn.Sequential(*mxpool_1) # 14x14\n",
    "        \n",
    "        \n",
    "        # Conv 6\n",
    "        ###\n",
    "        conv6 = 128\n",
    "        ct6 = nn.Conv2d(conv5,conv6,f,stride = s)\n",
    "        cb6 = nn.BatchNorm2d(conv6)\n",
    "        ca6 = nw_activation_conv\n",
    "        cl6 = [ct6,cb6,ca6,dropout_node]\n",
    "        self.convl6 = nn.Sequential(*cl6) \n",
    "        \n",
    "        \n",
    "        # Conv 7\n",
    "        ###\n",
    "        conv7 = 256\n",
    "        ct7 = nn.Conv2d(conv6,conv7,f,stride = s)\n",
    "        cb7 = nn.BatchNorm2d(conv7)\n",
    "        ca7 = nw_activation_conv\n",
    "        cl7 = [ct7,cb7,ca7,dropout_node]\n",
    "        self.convl7 = nn.Sequential(*cl7) \n",
    "        \n",
    "        # Conv 8\n",
    "        ###\n",
    "        conv8 = 256\n",
    "        ct8 = nn.Conv2d(conv7,conv8,f,stride = s)\n",
    "        cb8 = nn.BatchNorm2d(conv8)\n",
    "        ca8 = nw_activation_conv\n",
    "        cl8 = [ct8,cb8,ca8,dropout_node]\n",
    "        self.convl8 = nn.Sequential(*cl8) \n",
    "        \n",
    "        # Conv 9\n",
    "        ###\n",
    "        conv9 = 512\n",
    "        ct9 = nn.Conv2d(conv8,conv9,f,stride = s)\n",
    "        cb9 = nn.BatchNorm2d(conv9)\n",
    "        ca9 = nw_activation_conv\n",
    "        cl9 = [ct9,cb9,ca9,dropout_node]\n",
    "        self.convl9 = nn.Sequential(*cl9) \n",
    "        \n",
    "        # Conv 10\n",
    "        ###\n",
    "        conv10 = 512\n",
    "        ct10 = nn.Conv2d(conv9,conv10,f,stride = s)\n",
    "        cb10 = nn.BatchNorm2d(conv10)\n",
    "        ca10 = nw_activation_conv\n",
    "        cl10 = [ct10,cb10,ca10,dropout_node]\n",
    "        self.convl10 = nn.Sequential(*cl10) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #################### Linear 1x1 convs #########################\n",
    "        \n",
    "        # Conv ln 1\n",
    "        ###\n",
    "        convln1 = 1024\n",
    "        ctln1 = nn.Conv2d(conv10,convln1,1,stride = 1)\n",
    "        cbln1 = nn.BatchNorm2d(convln1)\n",
    "        caln1 = nw_activation_conv\n",
    "        clln1 = [ctln1,cbln1,caln1,dropout_node]\n",
    "        self.convlln1 = nn.Sequential(*clln1) # 1x1\n",
    "        \n",
    "        # Conv ln 2\n",
    "        ###\n",
    "        convln2 = 512\n",
    "        ctln2 = nn.Conv2d(convln1,convln2,1,stride = 1)\n",
    "        cbln2 = nn.BatchNorm2d(convln2)\n",
    "        caln2 = nw_activation_conv\n",
    "        clln2 = [ctln2,cbln2,caln2,dropout_node]\n",
    "        self.convlln2 = nn.Sequential(*clln2) # 1x1\n",
    "        \n",
    "        # Conv ln 3\n",
    "        ###\n",
    "        convln3 = 256\n",
    "        ctln3 = nn.Conv2d(convln2,convln3,1,stride = 1)\n",
    "        cbln3 = nn.BatchNorm2d(convln3)\n",
    "        caln3 = nw_activation_conv\n",
    "        clln3 = [ctln3,cbln3,caln3,dropout_node]\n",
    "        self.convlln3 = nn.Sequential(*clln3) # 1x1\n",
    "        \n",
    "        # Conv ln 4\n",
    "        ###\n",
    "        convln4 = 128\n",
    "        ctln4 = nn.Conv2d(convln3,convln4,1,stride = 1)\n",
    "        cbln4 = nn.BatchNorm2d(convln4)\n",
    "        caln4 = nw_activation_conv\n",
    "        clln4 = [ctln4,cbln4,caln4,dropout_node]\n",
    "        self.convlln4 = nn.Sequential(*clln4) # 1x1\n",
    "        \n",
    "        \n",
    "        ##################### final out ########################\n",
    "        \n",
    "        # Conv 11\n",
    "        ###\n",
    "        ct11 = nn.Conv2d(convln4,out_channels,1,stride = 1)\n",
    "        if target_act == 'sigmoid':\n",
    "            ca11 = nn.Sigmoid()\n",
    "        else:\n",
    "            ca11 = nn.Softmax2d()\n",
    "        cl11 = [ct11,ca11]\n",
    "        self.convl11 = nn.Sequential(*cl11) # 1x1\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # simple forward pass\n",
    "        # -------------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out) \n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        \n",
    "        mxpool1_out = self.mxpool1(c5_out)\n",
    "        \n",
    "        c6_out = self.convl6(mxpool1_out)\n",
    "        c7_out = self.convl7(c6_out) \n",
    "        c8_out = self.convl8(c7_out)\n",
    "        c9_out = self.convl9(c8_out)\n",
    "        c10_out = self.convl10(c9_out)\n",
    "        \n",
    "        # linear layer passes\n",
    "        # -------------------\n",
    "        cln1_out = self.convlln1(c10_out) # 1x1\n",
    "        cln2_out = self.convlln2(cln1_out) # 1x1\n",
    "        cln3_out = self.convlln3(cln2_out) # 1x1\n",
    "        cln4_out = self.convlln4(cln3_out) # 1x1\n",
    "        \n",
    "        \n",
    "        c11_out = self.convl11(cln4_out)\n",
    "        \n",
    "        return c11_out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_classifier_cifar10_exo_deep(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, target_act):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is an UNET model\n",
    "        # ---------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (32,32) -- Insize\n",
    "        \n",
    "        # @conv1    -   (30,30)\n",
    "        # @conv2    -   (28,28)\n",
    "        # @conv3    -   (26,26)\n",
    "        # @conv4    -   (24,24)\n",
    "        # @conv5    -   (22,22)\n",
    "        \n",
    "        # avgppool - (1,1)\n",
    "        \n",
    "        # @conv6    -   (9,9)\n",
    "        # @conv7    -   (7,7)\n",
    "        # @conv8    -   (5,5)\n",
    "        # @conv9    -   (3,3)\n",
    "        # @conv10    -   (1,1)\n",
    "        # @conv11    -   (1,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.LeakyReLU(0.2) #nn.LeakyReLU(0.2), nn.Tanh(), nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 1\n",
    "        dropout_prob = 0.0\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        pool_dims = (2,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 32\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1) \n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2) \n",
    "\n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3) \n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 64\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) \n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 64\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) \n",
    "        \n",
    "        # Conv 6\n",
    "        ###\n",
    "        conv6 = 128\n",
    "        ct6 = nn.Conv2d(conv5,conv6,f,stride = s)\n",
    "        cb6 = nn.BatchNorm2d(conv6)\n",
    "        ca6 = nw_activation_conv\n",
    "        cl6 = [ct6,cb6,ca6,dropout_node]\n",
    "        self.convl6 = nn.Sequential(*cl6) \n",
    "        \n",
    "        \n",
    "        # Conv 7\n",
    "        ###\n",
    "        conv7 = 128\n",
    "        ct7 = nn.Conv2d(conv6,conv7,f,stride = s)\n",
    "        cb7 = nn.BatchNorm2d(conv7)\n",
    "        ca7 = nw_activation_conv\n",
    "        cl7 = [ct7,cb7,ca7,dropout_node]\n",
    "        self.convl7 = nn.Sequential(*cl7) \n",
    "        \n",
    "        # Conv 8\n",
    "        ###\n",
    "        conv8 = 256\n",
    "        ct8 = nn.Conv2d(conv7,conv8,f,stride = s)\n",
    "        cb8 = nn.BatchNorm2d(conv8)\n",
    "        ca8 = nw_activation_conv\n",
    "        cl8 = [ct8,cb8,ca8,dropout_node]\n",
    "        self.convl8 = nn.Sequential(*cl8) \n",
    "        \n",
    "        # Conv 9\n",
    "        ###\n",
    "        conv9 = 256\n",
    "        ct9 = nn.Conv2d(conv8,conv9,f,stride = s)\n",
    "        cb9 = nn.BatchNorm2d(conv9)\n",
    "        ca9 = nw_activation_conv\n",
    "        cl9 = [ct9,cb9,ca9,dropout_node]\n",
    "        self.convl9 = nn.Sequential(*cl9) \n",
    "        \n",
    "        # Conv 10\n",
    "        ###\n",
    "        conv10 = 512\n",
    "        ct10 = nn.Conv2d(conv9,conv10,14,stride = 1)\n",
    "        cb10 = nn.BatchNorm2d(conv10)\n",
    "        ca10 = nw_activation_conv\n",
    "        cl10 = [ct10,cb10,ca10,dropout_node]\n",
    "        self.convl10 = nn.Sequential(*cl10) \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Maxpool 1\n",
    "        ###\n",
    "        mxpool_1 =  [nn.AvgPool2d((12,12), stride=1)]\n",
    "        self.mxpool1 = nn.Sequential(*mxpool_1) # 14x14\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #################### Linear 1x1 convs #########################\n",
    "        \n",
    "        # Conv ln 1\n",
    "        ###\n",
    "        convln1 = 256\n",
    "        ctln1 = nn.Conv2d(conv10,convln1,1,stride = 1)\n",
    "        cbln1 = nn.BatchNorm2d(convln1)\n",
    "        caln1 = nw_activation_conv\n",
    "        clln1 = [ctln1,cbln1,caln1,dropout_node]\n",
    "        self.convlln1 = nn.Sequential(*clln1) # 1x1\n",
    "        \n",
    "        # Conv ln 2\n",
    "        ###\n",
    "        convln2 = 128\n",
    "        ctln2 = nn.Conv2d(convln1,convln2,1,stride = 1)\n",
    "        cbln2 = nn.BatchNorm2d(convln2)\n",
    "        caln2 = nw_activation_conv\n",
    "        clln2 = [ctln2,cbln2,caln2,dropout_node]\n",
    "        self.convlln2 = nn.Sequential(*clln2) # 1x1\n",
    "        \n",
    "        # Conv ln 3\n",
    "        ###\n",
    "        convln3 = 64\n",
    "        ctln3 = nn.Conv2d(convln2,convln3,1,stride = 1)\n",
    "        cbln3 = nn.BatchNorm2d(convln3)\n",
    "        caln3 = nw_activation_conv\n",
    "        clln3 = [ctln3,cbln3,caln3,dropout_node]\n",
    "        self.convlln3 = nn.Sequential(*clln3) # 1x1\n",
    "        \n",
    "        # Conv ln 4\n",
    "        ###\n",
    "        convln4 = 32\n",
    "        ctln4 = nn.Conv2d(convln3,convln4,1,stride = 1)\n",
    "        cbln4 = nn.BatchNorm2d(convln4)\n",
    "        caln4 = nw_activation_conv\n",
    "        clln4 = [ctln4,cbln4,caln4,dropout_node]\n",
    "        self.convlln4 = nn.Sequential(*clln4) # 1x1\n",
    "        \n",
    "        \n",
    "        ##################### final out ########################\n",
    "        \n",
    "        # Conv 11\n",
    "        ###\n",
    "        ct11 = nn.Conv2d(conv10,out_channels,1,stride = 1)\n",
    "        if target_act == 'sigmoid':\n",
    "            ca11 = nn.Sigmoid()\n",
    "        else:\n",
    "            ca11 = nn.Softmax2d()\n",
    "        cl11 = [ct11,ca11]\n",
    "        self.convl11 = nn.Sequential(*cl11) # 1x1\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # simple forward pass\n",
    "        # -------------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out) \n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        c6_out = self.convl6(c5_out)\n",
    "        c7_out = self.convl7(c6_out) \n",
    "        c8_out = self.convl8(c7_out)\n",
    "        c9_out = self.convl9(c8_out)\n",
    "        c10_out = self.convl10(c9_out)\n",
    "        \n",
    "        \n",
    "        #mxpool1_out = self.mxpool1(c10_out)\n",
    "        \n",
    "        \n",
    "        # linear layer passes\n",
    "        # -------------------\n",
    "        #cln1_out = self.convlln1(mxpool1_out) # 1x1\n",
    "        #cln2_out = self.convlln2(cln1_out) # 1x1\n",
    "        #cln3_out = self.convlln3(cln2_out) # 1x1\n",
    "        #cln4_out = self.convlln4(cln3_out) # 1x1\n",
    "        \n",
    "        \n",
    "        c11_out = self.convl11(c10_out)\n",
    "        \n",
    "        return c11_out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See All codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent function\n",
    "# ---------------\n",
    "\n",
    "\n",
    "def see_all(mode,model_in,xin,target_act):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Purpose of this function is to iterate over new kernel sizes and suggest dims changes\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # needed assertion\n",
    "    # ----------------\n",
    "    assert mode != 'all', 'This function works only for first layer weights resize, not all.'\n",
    "    \n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    _,in_h,in_w,_ = xin.shape\n",
    "    model_seq = []\n",
    "    d = {}\n",
    "    d[0] = {}\n",
    "    d[0]['f'] = []\n",
    "    d[0]['s_list'] = []\n",
    "    \n",
    "    \n",
    "    # 0.1 initial setups\n",
    "    # ------------------\n",
    "    model_1temp = copy.deepcopy(model_in)\n",
    "    for each in model_1temp.children():\n",
    "        model_seq += list(each)\n",
    "    d[0]['model'] = nn.Sequential(*model_seq)\n",
    "        \n",
    "    # 0.2 capturing kernel size & strides\n",
    "    # -----------------------------------\n",
    "    for each in model_seq:\n",
    "        if 'Conv2d' in str(type(each)):\n",
    "            d[0]['f'].append(each.kernel_size[0])\n",
    "            d[0]['s_list'].append(each.stride[0])\n",
    " \n",
    "    # 1. running master size ops on initial inputs\n",
    "    # --------------------------------------------\n",
    "    print('At increment step 1...', end=' ')\n",
    "    conv_h,conv_w = in_h,in_w\n",
    "    for i in range(len(d[0]['s_list'])):\n",
    "        conv_h,conv_w = outsize_conv(conv_h,conv_w,d[0]['f'][i],d[0]['s_list'][i],0)\n",
    "        \n",
    "    # CHECK DIMS\n",
    "    # ----------\n",
    "    if conv_h < 1 or conv_w < 1:\n",
    "        \n",
    "        assert 1 == 2, 'Input dimensions not large enough for original model. Resizing op not coded yet.'\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # rounding and preparing new dims\n",
    "        # -------------------------------\n",
    "        req_h = round(conv_h)\n",
    "        req_w = round(conv_w)\n",
    "        \n",
    "        # getting new dims\n",
    "        # ----------------\n",
    "        for i in range(len(d[0]['s_list'])):\n",
    "            \n",
    "            temporary_s = list(reversed(d[0]['s_list']))[i]\n",
    "            temporary_f = list(reversed(d[0]['f']))[i]\n",
    "            req_h,req_w = outsize_upconv(req_h,req_w,temporary_f,temporary_s,0)\n",
    "        \n",
    "        # saving new dims\n",
    "        # ---------------\n",
    "        d[0]['new_dims'] = (req_h,req_w)\n",
    "        print(' done.', end='\\n')\n",
    "        \n",
    "    \n",
    "    # 2. Iterating over new f's with incremental 1\n",
    "    # --------------------------------------------\n",
    "    recurr_f = d[0]['f'][0]\n",
    "    counter = 1\n",
    "    while True:\n",
    "        \n",
    "        # 2.0 sanity printing\n",
    "        # -------------------\n",
    "        print('At increment step ' + str(counter+1) + '...', end=' ')\n",
    "        \n",
    "        # 2.1 incrementing f\n",
    "        # -------------------\n",
    "        recurr_f += 1\n",
    "        \n",
    "        # 2.2 creating new model with new f\n",
    "        # ---------------------------------\n",
    "        temp_d = resize_model(mode,copy.deepcopy(model_in),recurr_f)\n",
    "        \n",
    "        # 2.3 running master size ops on recurring inputs\n",
    "        # -----------------------------------------------\n",
    "        conv_h,conv_w = in_h,in_w\n",
    "        for s in temp_d['s_list']:\n",
    "            temporary_f = s[0]\n",
    "            temporary_s = s[1]\n",
    "            conv_h,conv_w = outsize_conv(conv_h,conv_w,temporary_f,temporary_s,0)\n",
    "\n",
    "        # CHECK DIMS\n",
    "        # ----------\n",
    "        if conv_h < 2 or conv_w < 2:\n",
    "\n",
    "            print(' Stopped.', end='\\n')\n",
    "            break\n",
    "\n",
    "        else:\n",
    "\n",
    "            # rounding and preparing new dims\n",
    "            # -------------------------------\n",
    "            req_h = round(conv_h)\n",
    "            req_w = round(conv_w)\n",
    "\n",
    "            # getting new dims\n",
    "            # ----------------\n",
    "            for s in list(reversed(temp_d['s_list'])):\n",
    "                temporary_f = s[0]\n",
    "                temporary_s = s[1]\n",
    "                req_h,req_w = outsize_upconv(req_h,req_w,temporary_f,temporary_s,0)\n",
    "\n",
    "            # saving new dims\n",
    "            # ---------------\n",
    "            d[counter] = {}\n",
    "            d[counter]['model'] = temp_d['new_model']\n",
    "            d[counter]['new_dims'] = (req_h,req_w)\n",
    "            d[counter]['f'] = []\n",
    "            d[counter]['s_list'] = []\n",
    "            \n",
    "            # saving f,s\n",
    "            # ----------\n",
    "            for s in temp_d['s_list']:\n",
    "                d[counter]['f'].append(s[0])\n",
    "                d[counter]['s_list'].append(s[1])\n",
    "\n",
    "            # increment counter\n",
    "            # -----------------\n",
    "            counter += 1\n",
    "            print(' done.', end='\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "    # 3. doing all forward passes with this section\n",
    "    # ---------------------------------------------\n",
    "    output_dict = {}\n",
    "    print('\\nForward passing all...\\n--------------------')\n",
    "    for keys in d:\n",
    "        \n",
    "        # sanity print\n",
    "        # ------------\n",
    "        print('At new model ' + str(keys+1) + ' of ' + str(len(d)) + '..')\n",
    "        \n",
    "        # initialising tempd\n",
    "        # ------------------\n",
    "        temp_d = {}\n",
    "        temp_d[keys] = d[keys]\n",
    "        try:\n",
    "            recurr_sum += forward_pass_all(temp_d,xin,target_act)\n",
    "            #recurr_sum = torch.max(recurr_sum,forward_pass_all(temp_d,xin,target_act))\n",
    "            \n",
    "        except:\n",
    "            recurr_sum = forward_pass_all(temp_d,xin,target_act)\n",
    "        \n",
    "    print('done.')\n",
    "\n",
    "    \n",
    "    # a final normalisation to 0-1\n",
    "    # ----------------------------\n",
    "    for i in range(recurr_sum.shape[0]):\n",
    "        recurr_sum[i] = recurr_sum[i]/np.max(recurr_sum[i])\n",
    "    \n",
    "    # makes a bit more sense to normalise this way\n",
    "    ##\n",
    "    #recurr_sum[recurr_sum < np.mean(recurr_sum)] = 0\n",
    "    #recurr_sum = recurr_sum/np.max(recurr_sum)\n",
    "    \n",
    "    return recurr_sum, d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to actually see all\n",
    "# ----------------------------\n",
    "\n",
    "def forward_pass_all(d,xin,target_act):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1. takes as input dict with resized models and xin (m,h,w,c) numpy array\n",
    "    2. forward passes all images in the array and returns a (m,h,w,no_classes) output\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    m,h,w,c = xin.shape\n",
    "    \n",
    "    # 0. iterating\n",
    "    # ------------\n",
    "    for i in range(m):\n",
    "        \n",
    "        # 0.1 local initialisations\n",
    "        # -------------------------\n",
    "        x_curr = copy.deepcopy(xin[i])\n",
    "        \n",
    "        \n",
    "        # 1.1 iterating trough the model\n",
    "        # ------------------------------\n",
    "        for keys in d:\n",
    "            \n",
    "            # more local initialisations\n",
    "            # --------------------------\n",
    "            curr_model = d[keys]['model']\n",
    "            curr_h,curr_w = d[keys]['new_dims']\n",
    "            x_curr = cv2.resize(x_curr, (curr_w,curr_h))\n",
    "            x_curr = x_curr.reshape(1,curr_h,curr_w,3)\n",
    "            x_curr_trn = Variable(setup_image_tensor(x_curr)).float()\n",
    "            x_curr_trn = x_curr_trn/255\n",
    "            \n",
    "            # forward pass\n",
    "            # ------------\n",
    "            curr_out = curr_model(x_curr_trn)[0]\n",
    "            curr_out = curr_out.data.numpy()\n",
    "            \n",
    "            # axis swap prior to resize since resize required (h,w,c) shape\n",
    "            # -------------------------------------------------------------\n",
    "            curr_out = np.swapaxes(curr_out,0,2)\n",
    "            curr_out = np.swapaxes(curr_out,0,1)\n",
    "            no_channels = curr_out.shape[2]\n",
    "            \n",
    "            # we could include a target active code here if required\n",
    "            ###\n",
    "            curr_out[curr_out < 0.75] = 0.001\n",
    "            \n",
    "            #if target_act == 'sigmoid':\n",
    "                #curr_out[curr_out < 0.5] = 0\n",
    "                #curr_out[curr_out >= 0.5] = 1\n",
    "                #pass\n",
    "                \n",
    "            #else:\n",
    "                #curr_out[curr_out < 0.5] = 0\n",
    "                #curr_out[curr_out >= 0.5] = 1\n",
    "                #pass\n",
    "                \n",
    "\n",
    "            # resize\n",
    "            # ------\n",
    "            curr_out = cv2.resize(curr_out,(w,h))#, interpolation=cv2.INTER_NEAREST)\n",
    "            curr_out = curr_out.reshape(1,h,w,no_channels)\n",
    "            \n",
    "            \n",
    "            # adding the outputs of each see all model\n",
    "            # ----------------------------------------\n",
    "            try:\n",
    "                local_key_out += curr_out\n",
    "            except:\n",
    "                local_key_out = curr_out\n",
    "           \n",
    "                \n",
    "            \n",
    "        \n",
    "        # 1.2 outside see all model keys\n",
    "        # ------------------------------\n",
    "        try:\n",
    "            final_out = np.concatenate((final_out,local_key_out), axis = 0)\n",
    "        except:\n",
    "            final_out = local_key_out\n",
    "        \n",
    "        \n",
    "        # 1.3 clearning local_key_out to start fresh for other images\n",
    "        # -----------------------------------------------------------\n",
    "        del local_key_out\n",
    "        #print('Done with images ' + str(i+1) + ' of ' + str(m) + '..', end='\\r')\n",
    "\n",
    "            \n",
    "    # final return\n",
    "    # ------------\n",
    "    return final_out\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that resizes trained model\n",
    "# -----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def resize_model(mode,model_in,new_f):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1. easy formula for working out strides at each layer\n",
    "    2. using cv2.resize with nearest neighbour method to resize weights\n",
    "    3. assigns new weights and strides and returns a new model\n",
    "    4. also returns new input image size to be used based on required output size\n",
    "    \n",
    "    \n",
    "    A way to check weights \n",
    "    ----------------------\n",
    "    index = 8\n",
    "    out_channel = 24\n",
    "    in_channel = 21\n",
    "\n",
    "    oldmodel[index].weight[out_channel,in_channel,:,:]\n",
    "    newmodel[index].weight[out_channel,in_channel,:,:]\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    model_seq = []\n",
    "    old_model = []\n",
    "    strides_list = []\n",
    "    mode_counter = 0\n",
    "    mode_flag = 0\n",
    "    \n",
    "    \n",
    "    # 1. building a sequential model from input\n",
    "    # -----------------------------------------\n",
    "    model_1temp = copy.deepcopy(model_in)\n",
    "    model_2temp = copy.deepcopy(model_in)\n",
    "\n",
    "    for each in model_1temp.children():\n",
    "        model_seq += list(each)\n",
    "    \n",
    "    for each in model_2temp.children():\n",
    "        old_model += list(each)\n",
    "    \n",
    "    \n",
    "    # 2. iterating through each model and working out new stride values\n",
    "    # -----------------------------------------------------------------\n",
    "    for each in model_seq:\n",
    "        if 'Conv2d' in str(type(each)) or 'MaxPool2d' in str(type(each)) or 'AvgPool2d' in str(type(each)):\n",
    "            \n",
    "            # mode counter updation\n",
    "            # ---------------------\n",
    "            mode_counter += 1\n",
    "            \n",
    "            # setting mode to single automatically if max or avgpool found in NW \n",
    "            # ------------------------------------------------------------------\n",
    "            if 'MaxPool2d' in str(type(each)) or 'AvgPool2d' in str(type(each)):\n",
    "                mode = 'single'\n",
    "            \n",
    "            # checking mode\n",
    "            # -------------\n",
    "            if mode == 'all':\n",
    "                mode_flag = 1\n",
    "            else:\n",
    "                if mode_counter == 1:\n",
    "                    mode_flag = 1\n",
    "                else:\n",
    "                    mode_flag = 0\n",
    "            \n",
    "            if mode_flag == 1:\n",
    "            \n",
    "                # updating stride\n",
    "                # ---------------\n",
    "                curr_f = each.kernel_size[0]\n",
    "                curr_s = each.stride[0]\n",
    "\n",
    "                if mode == 'all':\n",
    "                \n",
    "                    # using math.floor here to floor down strides\n",
    "                    # -------------------------------------------\n",
    "                    new_s = math.floor((curr_s/curr_f)*new_f)\n",
    "                    \n",
    "                else:\n",
    "                    # using round to up strides\n",
    "                    # -------------------------------------------\n",
    "                    new_s = round((curr_s/curr_f)*new_f)\n",
    "                    #new_s = math.floor((curr_s/curr_f)*new_f)\n",
    "                    \n",
    "                \n",
    "\n",
    "                # resizing and updating weights\n",
    "                # -----------------------------\n",
    "                curr_weight = each.weight\n",
    "                for c in range(curr_weight.size()[0]):\n",
    "                    curr_weight_channel_orig = curr_weight[c].data.numpy()\n",
    "                    curr_weight_channel = copy.deepcopy(curr_weight_channel_orig)\n",
    "\n",
    "                    # axis swap prior to resize since resize required (h,w,c) shape\n",
    "                    # -------------------------------------------------------------\n",
    "                    curr_weight_channel = np.swapaxes(curr_weight_channel,0,2)\n",
    "                    curr_weight_channel = np.swapaxes(curr_weight_channel,0,1)\n",
    "\n",
    "                    # resize\n",
    "                    # ------\n",
    "                    updated_weight_channel = cv2.resize(curr_weight_channel,(new_f,new_f), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                    # axis swap after reshape - back to (c,h,w)\n",
    "                    # -----------------------------------------\n",
    "                    updated_weight_channel = np.swapaxes(updated_weight_channel,0,2)\n",
    "                    updated_weight_channel = np.swapaxes(updated_weight_channel,1,2)\n",
    "                    updated_weight_channel = updated_weight_channel.reshape(1,updated_weight_channel.shape[0],updated_weight_channel.shape[1],updated_weight_channel.shape[2])\n",
    "\n",
    "                    # concatenating\n",
    "                    # -------------\n",
    "                    try:\n",
    "                        new_weight = np.concatenate((new_weight,updated_weight_channel), axis = 0)\n",
    "                    except:\n",
    "                        new_weight = updated_weight_channel\n",
    "\n",
    "                # converting numpy weight to torch and setting it back to NW\n",
    "                # -----------------------------------------------------------\n",
    "                each.weight = torch.nn.Parameter(torch.from_numpy(new_weight))\n",
    "                each.kernel_size = (new_f,new_f)\n",
    "                each.stride = new_s\n",
    "                strides_list.append((new_f,new_s))\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # we are not updating kernel size or stride this is section\n",
    "                # ---------------------------------------------------------\n",
    "                curr_f = each.kernel_size[0]\n",
    "                try:\n",
    "                    curr_s = each.stride[0]\n",
    "                except:\n",
    "                    curr_s = each.stride\n",
    "                    \n",
    "                strides_list.append((curr_f,curr_s))\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    # final return\n",
    "    # ------------\n",
    "    d = {}\n",
    "    d['new_model'] = nn.Sequential(*model_seq)\n",
    "    d['old_model'] = nn.Sequential(*old_model)\n",
    "    d['s_list'] = strides_list\n",
    "    \n",
    "    \n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END OF CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 setting up model related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET UP CUDA OR NOT HERE + OTHER SET UPS\n",
    "##########################################\n",
    "\n",
    "dev_env = 'local' # 'gpu' or 'local'\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "# Setting CUDA\n",
    "# ------------\n",
    "if dev_env == 'gpu':\n",
    "    use_cuda = True\n",
    "else:\n",
    "    use_cuda = False\n",
    "if use_cuda == True:\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "# SET FILE SPECIFIC NAMES HERE\n",
    "# ----------------------------\n",
    "if dev_env == 'gpu':\n",
    "    save_path = '/home/venkateshmadhava/codes/pmate2_localgpuenv/models/'\n",
    "    parent_url = '/home/venkateshmadhava/datasets/images'\n",
    "else:\n",
    "    save_path = '/Users/venkateshmadhava/Documents/pmate2/pmate2_env/models/'\n",
    "    parent_url = '/Users/venkateshmadhava/Documents/projects/vision/object_detection/cifar_10_batches_py'\n",
    "\n",
    "\n",
    "# displaying save path\n",
    "# --------------------\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading classes\n",
    "# ---------------\n",
    "\n",
    "labels_head = unpickle(parent_url + '/' + 'batches.meta')\n",
    "labels_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading batches of data & concatenating them\n",
    "# --------------------------------------------\n",
    "\n",
    "# loading batchs\n",
    "# --------------\n",
    "btch1 = parent_url + '/data_batch_1'\n",
    "load_cifar10(btch1)\n",
    "x1 = copy.deepcopy(x)\n",
    "y1 = copy.deepcopy(y)\n",
    "\n",
    "##\n",
    "btch1 = parent_url + '/data_batch_2'\n",
    "load_cifar10(btch1)\n",
    "x2 = copy.deepcopy(x)\n",
    "y2 = copy.deepcopy(y)\n",
    "\n",
    "##\n",
    "btch1 = parent_url + '/data_batch_3'\n",
    "load_cifar10(btch1)\n",
    "x3 = copy.deepcopy(x)\n",
    "y3 = copy.deepcopy(y)\n",
    "\n",
    "##\n",
    "btch1 = parent_url + '/data_batch_4'\n",
    "load_cifar10(btch1)\n",
    "x4 = copy.deepcopy(x)\n",
    "y4 = copy.deepcopy(y)\n",
    "\n",
    "##\n",
    "btch1 = parent_url + '/data_batch_5'\n",
    "load_cifar10(btch1)\n",
    "x5 = copy.deepcopy(x)\n",
    "y5 = copy.deepcopy(y)\n",
    "\n",
    "\n",
    "##\n",
    "btch1 = parent_url + '/test_batch'\n",
    "load_cifar10(btch1)\n",
    "x_test = copy.deepcopy(x)\n",
    "y_test = copy.deepcopy(y)\n",
    "\n",
    "\n",
    "# concatenating\n",
    "# -------------\n",
    "x_train = np.concatenate((x1,x2,x3,x4,x5), axis = 0)\n",
    "y_train = np.concatenate((y1,y2,y3,y4,y5), axis = 0)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity viewing of images\n",
    "# ------------------------\n",
    "\n",
    "# random check\n",
    "# ------------\n",
    "randrange = list(np.random.randint(x_test.shape[0], size=(1, 2))[0,:])\n",
    "\n",
    "\n",
    "for i in randrange:\n",
    "    \n",
    "    print('TRAIN:')\n",
    "    curr_loc = np.argwhere(y_train[i]==1)[0,0]\n",
    "    curr_label = labels_head[b'label_names'][curr_loc]\n",
    "    print(curr_label)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.show()\n",
    "    print('------------------')\n",
    "    print('TEST:')\n",
    "    curr_loc = np.argwhere(y_test[i]==1)[0,0]\n",
    "    curr_label = labels_head[b'label_names'][curr_loc]\n",
    "    print(curr_label)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.show()\n",
    "    print('------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping y so that it is channel wise\n",
    "# --------------------------------------\n",
    "y_train = y_train.reshape(y_train.shape[0],1,1,y_train.shape[1])\n",
    "y_test = y_test.reshape(y_test.shape[0],1,1,y_test.shape[1])\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting set on local machine\n",
    "# ------------------------------\n",
    "\n",
    "if dev_env == 'local':\n",
    "    \n",
    "    # not doing that for now\n",
    "    # ----------------------\n",
    "    x_train = x_train[0:25]\n",
    "    y_train = y_train[0:25]\n",
    "    x_test = x_test[0:5]\n",
    "    y_test = y_test[0:5]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 2. setting up & training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snippet to work out filter sizes\n",
    "# --------------------------------\n",
    "f = 2\n",
    "s = 2\n",
    "pad = 0\n",
    "layers = 5\n",
    "\n",
    "h = 32 # 255\n",
    "w = 32 # 255\n",
    "print('Showing conv down sizes - ')\n",
    "print('--------------------------')\n",
    "# showing out sizes after conv\n",
    "# ----------------------------\n",
    "for _ in range(layers):   \n",
    "    h,w = outsize_conv(h,w,f,s,pad)\n",
    "    print((h,w))\n",
    "    \n",
    "h = 1\n",
    "w = 1\n",
    "print('\\nShowing conv up sizes - ')\n",
    "print('--------------------------')\n",
    "# showing out sizes after conv\n",
    "# ----------------------------\n",
    "dims = []\n",
    "for _ in range(layers):   \n",
    "    h,w = outsize_upconv(h,w,f,s,pad)\n",
    "    dims.append((h,w))\n",
    "    print((h,w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing images if required\n",
    "# ----------------------------\n",
    "#new_h,new_w = 191,319\n",
    "#resize_images = False\n",
    "\n",
    "#if resize_images == True:\n",
    "#    xtrn_src = resize_all(xtrn_src,new_h,new_w)\n",
    "#    xtst_src = resize_all(xtst_src,new_h,new_w)\n",
    "    \n",
    "\n",
    "# printing shapes for sanity\n",
    "# --------------------------\n",
    "#print(xtrn_src.shape)\n",
    "#print(xtrn_tgt.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up train data\n",
    "# -----------------\n",
    "xin_train = Variable(setup_image_tensor(x_train)).float()\n",
    "yout_train = Variable(setup_image_tensor(y_train)).float()\n",
    "\n",
    "print(xin_train.size())\n",
    "print(yout_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking class distribution in the training dataset\n",
    "# --------------------------------------------------\n",
    "\n",
    "d = return_class_distribution(yout_train.reshape(yout_train.shape[0],10).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 start of model setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using an FCN AE system\n",
    "# Training fcn_ae_6_layer_WNET to 65 expoch brings loss to 0.014\n",
    "# ----------------------\n",
    "\n",
    "\n",
    "# new model initialisations\n",
    "# -------------------------\n",
    "target_activation = 'sigmoid'\n",
    "no_classes_for_act = 10\n",
    "#model_classnet = fcn_classifier_cifar10_linlayers(3,no_classes_for_act,target_activation)\n",
    "#model_classnet.apply(weights_init)\n",
    "#model_classnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "# ------------------\n",
    "\n",
    "''' USE -1 AS EPOCHS TO LOAD SAVED MODEL WITHOUT TRAINING '''\n",
    "\n",
    "# model_train(xin,yin,xval,yval,load_mode,model,epochs,mbsize,loss_mode,flatten,use_cuda,save_state,path)\n",
    "\n",
    "cn_file_name = 'model_classnet_cifar10_sigmoid_simpler_softmaxinternal.tar'\n",
    "cn_save_path = save_path + cn_file_name\n",
    "print(cn_save_path)\n",
    "\n",
    "\n",
    "model_classnet = model_train(xin_train,yout_train*255,None,None,'from saved',None,-1,5,'mse',use_cuda,True,cn_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 checking accuracy & visualising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking accuracy\n",
    "# -----------------\n",
    "\n",
    "check_accuracy(x_test,y_test,model_classnet,target_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random check on test images\n",
    "# ---------------------------\n",
    "\n",
    "# setup test set\n",
    "# --------------\n",
    "randrange = list(np.random.randint(x_test.shape[0], size=(1, 2))[0,:])\n",
    "x_vis = x_test[randrange]\n",
    "\n",
    "vis_out = chunk_pass(setup_image_tensor(x_vis),model_classnet,False,use_cuda,1)\n",
    "vis_out = vis_out.view(vis_out.size()[0],-1).cpu().data.numpy()\n",
    "\n",
    "# correcting output\n",
    "# -----------------\n",
    "if target_activation == 'sigmoid':\n",
    "    vis_out[vis_out < 0.5] = 0\n",
    "else:\n",
    "    vis_out = softmax_to_onehot(vis_out)\n",
    "\n",
    "    \n",
    "# looping\n",
    "# --------\n",
    "for i in range(x_vis.shape[0]):\n",
    "    \n",
    "    try:\n",
    "        curr_loc = np.argwhere(vis_out[i]==1)[0,0]\n",
    "        curr_label = labels_head[b'label_names'][curr_loc]\n",
    "        print(curr_label)\n",
    "    except:\n",
    "        print('No confident prediction')\n",
    "    \n",
    "    plt.imshow(x_vis[i])\n",
    "    plt.show()\n",
    "    print('------------------')\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# reading images from local folder\n",
    "# --------------------------------\n",
    "\n",
    "if dev_env == 'local':\n",
    "    \n",
    "    in_folder = '/Users/venkateshmadhava/Desktop/test'\n",
    "    temp_h,temp_w = x_test.shape[1], x_test.shape[2]\n",
    "    x_vis = create_dataset_from_folder_all(in_folder,temp_h,temp_w)\n",
    "    print(x_vis.shape)\n",
    "    \n",
    "    vis_out = chunk_pass(setup_image_tensor(x_vis),model_classnet,False,use_cuda,1)\n",
    "    vis_out = vis_out.view(vis_out.size()[0],-1).cpu().data.numpy()\n",
    "    \n",
    "    # output corrcetion\n",
    "    # -----------------\n",
    "    if target_activation == 'sigmoid':\n",
    "        vis_out[vis_out >= 0.5] = 1\n",
    "        vis_out[vis_out < 0.5] = 0\n",
    "    else:\n",
    "        vis_out = softmax_to_onehot(vis_out)\n",
    "    \n",
    "    # looping\n",
    "    # -------\n",
    "    for i in range(x_vis.shape[0]):\n",
    "\n",
    "        try:\n",
    "            curr_loc = np.argwhere(vis_out[i]==1)[0,0]\n",
    "            curr_label = labels_head[b'label_names'][curr_loc]\n",
    "            print(curr_label)\n",
    "        except:\n",
    "            print('No confident prediction')\n",
    "\n",
    "        plt.imshow(x_vis[i])\n",
    "        plt.show()\n",
    "        print('------------------')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3. sell all code run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting in images from a folder -- images can be of arbbitary size\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "in_folder = '/Users/venkateshmadhava/Desktop/test'\n",
    "folder_h,folder_w = 500,500\n",
    "\n",
    "# using dict\n",
    "# ----------\n",
    "#imgs_dict = create_dataset_from_folder_to_dict(in_folder)\n",
    "\n",
    "# normalising to a single size\n",
    "# ----------------------------\n",
    "x_folder = create_dataset_from_folder_all(in_folder,folder_h,folder_w)\n",
    "print(x_folder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sanity showing of image\n",
    "# -----------------------\n",
    "\n",
    "plt.imshow(x_folder[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting up see_all models\n",
    "# -------------------------\n",
    "\n",
    "out,d = see_all('single',model_classnet.eval(),x_folder,target_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking dims for sanity\n",
    "# ------------------------\n",
    "\n",
    "#for keys in d:\n",
    "#    print(str(keys) + ' f,s: ' + str(d[keys]['f'][0]) + ',' + str(d[keys]['s_list'][0]) + ' | dims: ' + str(d[keys]['new_dims']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# displaying see all result\n",
    "# -------------------------\n",
    "\n",
    "for i in range(out.shape[0]):\n",
    "    \n",
    "    print('At image ' + str(i) + '..')\n",
    "    \n",
    "    for c in range(out.shape[3]):\n",
    "        print('label channel: ' + str(labels_head[b'label_names'][c]))\n",
    "        plt.imshow(x_folder[i])\n",
    "        plt.show()\n",
    "        plt.imshow(out[i,:,:,c])\n",
    "        plt.show()\n",
    "        \n",
    "    print('**************************************************')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END OF ALL CODES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rough but keep"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(d[0]['f'])\n",
    "print(d[0]['s_list'])\n",
    "print(d[0]['new_dims'])\n",
    "print(d[0]['model'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(d[1]['f'])\n",
    "print(d[1]['s_list'])\n",
    "print(d[1]['new_dims'])\n",
    "print(d[1]['model'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(d[2]['f'])\n",
    "print(d[2]['s_list'])\n",
    "print(d[2]['new_dims'])\n",
    "print(d[2]['model'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ind = 0\n",
    "\n",
    "\n",
    "conv_h,conv_w = d[ind]['new_dims'][0],d[ind]['new_dims'][1]\n",
    "print(conv_h,conv_w)\n",
    "\n",
    "for i in range(len(d[ind]['s_list'])):\n",
    "    f,s = d[ind]['f'][i],d[ind]['s_list'][i]\n",
    "    conv_h,conv_w = outsize_conv(conv_h,conv_w,f,s,0)\n",
    "    print((conv_h,conv_w))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# A way to check weights \n",
    "# ----------------------\n",
    "index = 4\n",
    "out_channel = 7\n",
    "in_channel = 0\n",
    "\n",
    "d[0]['model'][index].weight[out_channel,in_channel,:,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "d[4]['model'][index].weight[out_channel,in_channel,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmate2_env",
   "language": "python",
   "name": "pmate2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
